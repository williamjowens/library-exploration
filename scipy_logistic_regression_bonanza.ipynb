{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "U3HNo7mS-Vqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression as SklearnPenalizedLogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import statsmodels.api as sm\n",
        "from scipy.optimize import minimize, root, _numdiff\n",
        "from scipy.linalg import inv, svd, cholesky, LinAlgError\n",
        "from scipy.optimize import root\n",
        "from scipy.stats import norm"
      ],
      "metadata": {
        "id": "ZiIlC6IV-7Ma"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Data"
      ],
      "metadata": {
        "id": "zB6YuhOY_LCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "nAUWQf8e_MM6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sklearn LogReg"
      ],
      "metadata": {
        "id": "zWz1_fad_j2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with scikit-learn\n",
        "sklearn_model = SklearnLogisticRegression(solver='lbfgs')\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_model.predict(X_test)\n",
        "y_pred_proba_sklearn = sklearn_model.predict_proba(X_test)[:, 1]\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "log_loss_sklearn = log_loss(y_test, y_pred_proba_sklearn)\n",
        "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.4f}\")\n",
        "print(f\"Log Loss (scikit-learn): {log_loss_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCg3_5os_jNV",
        "outputId": "b380f0dc-6105-4f02-b27e-1cc6324596cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (scikit-learn): 0.8467\n",
            "Log Loss (scikit-learn): 0.3566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statsmodels LogReg"
      ],
      "metadata": {
        "id": "ZsT0tqWv_0L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with statsmodels\n",
        "X_train_sm = sm.add_constant(X_train)  # Adding intercept term\n",
        "X_test_sm = sm.add_constant(X_test)\n",
        "statsmodels_model = sm.Logit(y_train, X_train_sm).fit()\n",
        "y_pred_proba_sm = statsmodels_model.predict(X_test_sm)\n",
        "y_pred_sm = (y_pred_proba_sm >= 0.5).astype(int)\n",
        "accuracy_sm = accuracy_score(y_test, y_pred_sm)\n",
        "log_loss_sm = log_loss(y_test, y_pred_proba_sm)\n",
        "print(f\"\\nAccuracy (statsmodels): {accuracy_sm:.4f}\")\n",
        "print(f\"Log Loss (statsmodels): {log_loss_sm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mda2hQ9v_zkg",
        "outputId": "02233b88-2624-483e-bf97-81273db3b688"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.322850\n",
            "         Iterations 8\n",
            "\n",
            "Accuracy (statsmodels): 0.8400\n",
            "Log Loss (statsmodels): 0.3576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maximum Likelihood Estimation"
      ],
      "metadata": {
        "id": "DSS4mCZI_UTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionMLE:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation (MLE).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(solver='BFGS'):\n",
        "        Fits the logistic regression model using the specified solver.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"Computes the negative log-likelihood.\"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        return -np.sum(self.y * z - np.log(1 + np.exp(z)))\n",
        "\n",
        "    def fit(self, solver='BFGS'):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using the specified solver.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        solver : str, optional\n",
        "            The optimization solver to use (default is 'BFGS').\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = minimize(self.log_likelihood, initial_beta, method=solver)\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "yOnfVrU2-3s8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Likelihood Estimation\n",
        "mle_model = LogisticRegressionMLE(X_train, y_train)\n",
        "mle_model.fit(solver='BFGS')\n",
        "accuracy_mle, log_loss_mle = mle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (MLE): {accuracy_mle:.4f}\")\n",
        "print(f\"Log Loss (MLE): {log_loss_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhsZoD-G_Qxq",
        "outputId": "e636e4f8-baee-449a-b720-1f0ddabefc1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (MLE): 0.8400\n",
            "Log Loss (MLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laplace Approximation"
      ],
      "metadata": {
        "id": "QALMJiAK_eNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionLaplace:\n",
        "    \"\"\"\n",
        "    Logistic Regression with Laplace Approximation for Bayesian inference.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    sigma : float\n",
        "        The standard deviation of the prior.\n",
        "    beta_map : np.ndarray\n",
        "        The MAP estimate of coefficients after fitting the model.\n",
        "    posterior_covariance : np.ndarray\n",
        "        The covariance matrix of the posterior distribution.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(solver='BFGS'):\n",
        "        Fits the logistic regression model using Laplace Approximation.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    plot_posterior_distributions():\n",
        "        Plots the posterior distributions of the first two coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, sigma=1.0):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.sigma = sigma\n",
        "        self.beta_map = None\n",
        "        self.posterior_covariance = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"Computes the negative log-likelihood.\"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        return -np.sum(self.y * z - np.log(1 + np.exp(z)))\n",
        "\n",
        "    def log_prior(self, beta):\n",
        "        \"\"\"Computes the log-prior (Gaussian prior).\"\"\"\n",
        "        return -0.5 * np.sum(beta ** 2) / self.sigma ** 2\n",
        "\n",
        "    def negative_log_posterior(self, beta):\n",
        "        \"\"\"Computes the negative log-posterior.\"\"\"\n",
        "        return self.log_likelihood(beta) - self.log_prior(beta)\n",
        "\n",
        "    def jacobian(self, beta):\n",
        "        \"\"\"Computes the Jacobian (gradient) of the negative log-posterior.\"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        grad_log_likelihood = np.dot(self.X.T, (p - self.y))\n",
        "        grad_log_prior = beta / self.sigma ** 2\n",
        "        return grad_log_likelihood - grad_log_prior\n",
        "\n",
        "    def fit(self, solver='BFGS'):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Laplace Approximation.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        solver : str, optional\n",
        "            The optimization solver to use (default is 'BFGS').\n",
        "            Only solvers that provide the inverse Hessian are allowed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta_map : np.ndarray\n",
        "            The MAP estimate of coefficients.\n",
        "        posterior_covariance : np.ndarray\n",
        "            The covariance matrix of the posterior distribution.\n",
        "        \"\"\"\n",
        "        if solver not in ['BFGS', 'L-BFGS-B', 'Newton-CG']:\n",
        "            raise ValueError(f\"Solver '{solver}' does not provide an inverse Hessian or requires a Jacobian. Please choose 'BFGS', 'L-BFGS-B', or 'Newton-CG'.\")\n",
        "\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "\n",
        "        if solver in ['BFGS', 'L-BFGS-B']:\n",
        "            result = minimize(self.negative_log_posterior, initial_beta, method=solver)\n",
        "            hessian_matrix = result.hess_inv if solver == 'BFGS' else result.hess_inv.todense()\n",
        "        elif solver == 'Newton-CG':\n",
        "            result = minimize(self.negative_log_posterior, initial_beta, method=solver, jac=self.jacobian)\n",
        "            hessian_matrix = _numdiff.approx_derivative(self.jacobian, result.x, method='3-point')\n",
        "\n",
        "        # Regularization and SVD to stabilize Hessian inversion\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(hessian_matrix)\n",
        "        epsilon = max(1e-5, abs(np.min(eigenvalues)) + 1e-5)\n",
        "        hessian_matrix += np.eye(hessian_matrix.shape[0]) * epsilon\n",
        "\n",
        "        # Use SVD for stable inversion\n",
        "        U, s, Vt = svd(hessian_matrix)\n",
        "        s_inv = np.array([1 / si if si > 1e-10 else 0 for si in s])\n",
        "        hessian_inv = np.dot(Vt.T, np.dot(np.diag(s_inv), U.T))\n",
        "\n",
        "        self.posterior_covariance = hessian_inv\n",
        "        self.beta_map = result.x\n",
        "        return self.beta_map, self.posterior_covariance\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta_map)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val\n",
        "\n",
        "    def plot_posterior_distributions(self):\n",
        "        \"\"\"Plots the posterior distributions of the first two coefficients.\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        for i in range(2):  # Show first two coefficients for simplicity\n",
        "            mean = self.beta_map[i]\n",
        "            std = np.sqrt(self.posterior_covariance[i, i])\n",
        "            if np.isnan(std):  # Check if std is NaN\n",
        "                axes[i].text(0.5, 0.5, 'Invalid Covariance', horizontalalignment='center', verticalalignment='center', transform=axes[i].transAxes)\n",
        "            else:\n",
        "                x = np.linspace(mean - 3 * std, mean + 3 * std, 100)\n",
        "                y = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
        "                axes[i].plot(x, y)\n",
        "            axes[i].set_title(f'Posterior Distribution of Coefficient {i + 1}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "O4D992ez_B_y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace Approximation\n",
        "laplace_model = LogisticRegressionLaplace(X_train, y_train)\n",
        "laplace_model.fit(solver='Newton-CG')\n",
        "accuracy_laplace, log_loss_laplace = laplace_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Laplace): {accuracy_laplace:.4f}\")\n",
        "print(f\"Log Loss (Laplace): {log_loss_laplace:.4f}\\n\")\n",
        "\n",
        "# Visualize Posterior Distributions from Laplace Approximation\n",
        "laplace_model.plot_posterior_distributions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "Fhg-SzmY_B7n",
        "outputId": "049b86b8-af6c-4bc3-adfa-db41c9909e5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Laplace): 0.8400\n",
            "Log Loss (Laplace): 0.3562\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADGDElEQVR4nOzdeVxTV/4//lcSSMIaZF9ERUUBF0BUxL2K4lIrXa3TjtZa7Ti11drlV2Y62vXrZzpdbEen1nY62sXRaqe2tRaruFYprlgXVFAUVHYk7ASS+/sjJBoFIQjcLK/n45GHD8K5ue9g4J73ue9zjkQQBAFERERERERE1O6kYgdAREREREREZKuYdBMRERERERF1ECbdRERERERERB2ESTcRERERERFRB2HSTURERERERNRBmHQTERERERERdRAm3UREREREREQdhEk3ERERERERUQdh0k1ERERERETUQZh0k81Zu3YtJBIJLl26JHYoTXriiSfQo0ePTjlXjx498MQTTxi/Nvxsjhw50innHzt2LMaOHdsp52qrgoICPPTQQ/Dy8oJEIsGKFSvEDqlZzcWamZmJiRMnQqVSQSKRYMuWLW3+PejMzycRWS9ea2/gtbZlvNaa4rXW/jDpJuMfDMNDqVSiT58+WLhwIQoKCtr9fNXV1XjttdewZ8+edn/tzvbaa6+Z/OycnZ3RrVs3TJs2Df/5z39QV1fXLuc5c+YMXnvtNYvs3FhybK3x/PPPY/v27UhKSsKXX36JSZMm3bF9bW0tPvjgA8TGxkKlUpn8vpw/f16UWGfPno2TJ0/i7bffxpdffonBgwd3aBx3y9zPTF5eHl555RXcc889cHNzg0QisYm/H2RfeK1tO15rLTu21uC1tvOZ+5lJSUnBk08+iT59+sDZ2Rk9e/bEU089hby8vI4N1F4IZPf+85//CACEN954Q/jyyy+FTz/9VJg9e7YglUqFkJAQoaqqql3PV1RUJAAQli1b1q6va9DQ0CDU1NQIOp2uQ17/ZsuWLRMACB9//LHw5ZdfCp999pnw+uuvC8OHDxcACAMHDhRycnJMjtFoNEJtba1Z59m0aZMAQNi9e7dZx9XW1goajcb4teH/+vDhw2a9Tltjq6urE+rq6trtXB3Bz89PeOyxx1rVtqioSIiJiREACPfee6+wYsUK4bPPPhNeeuklITg4WHB0dOz0WKurqwUAwl//+leT59v6e9CWz6e5zP087969WwAghIaGCnFxcW36XSASG6+1bcdrLa+1vNaaz9zPc0xMjBASEiK8/PLLwqeffiokJSUJbm5ugp+fn5CXl9ehsdoDh85M8MmyTZ482Thq99RTT8HLywvvv/8+vv/+e8ycOVPk6FpWVVUFFxcXyGQyyGSydnvd6upqODs737HNQw89BG9vb+PXS5cuxddff41Zs2bh4Ycfxm+//Wb8nqOjY7vF1hRBEFBbWwsnJycoFIoOPVdL5HK5qOdvjcLCQnh4eLSq7RNPPIHjx49j8+bNePDBB02+9+abb+Kvf/1rB0R4Q1OxFhUVAcBtz7f196CjP59tERMTg5KSEnh6emLz5s14+OGHxQ6JqM14rW0ar7Vtx2tt+7LXa+3777+PkSNHQiq9UQg9adIkjBkzBitXrsRbb70lYnQ2QOysn8TX3Ijs1q1bBQDC22+/LQiCINTX1wtvvPGG0LNnT0Eulwvdu3cXkpKSbhupO3z4sDBx4kTBy8tLUCqVQo8ePYQ5c+YIgiAI2dnZAoDbHjePxGdkZAgPPvig0KVLF0GhUAgxMTHC999/32TMe/bsERYsWCD4+PgIHh4eJt/Lzs42OWbVqlVCRESEIJfLhYCAAOHPf/6zcP36dZM2Y8aMEfr16yccOXJEGDVqlODk5CQsWrSo2Z+dYfS9qKioye/Pnz9fACD88ssvxudmz54tdO/e3aTdf//7X2HQoEGCq6ur4ObmJvTv319YsWKFyfu59WEYuezevbswdepUITk5WYiJiREUCoXwwQcfGL83e/bs235ue/fuFebPny94enoKbm5uwh//+EehtLTUJKZb/18Mbn7NlmIbM2aMMGbMGJPjCwoKhCeffFLw9fUVFAqFMHDgQGHt2rUmbQyfk3/84x/CJ598YvzMDR48WDh06FCTP+tbXbhwQXjooYeELl26CE5OTkJsbKywdevW234Wtz6a89tvvwkAhHnz5rXq/IIgCCkpKcLIkSMFZ2dnQaVSCffdd59w5syZ29pduXJFmDNnjuDr6yvI5XIhIiJC+Pe//91irIbP380Pw2erud+Dbdu2CaNHjzZ+1gYPHix8/fXXxu839fnUarXCBx98IERERAgKhULw9fUV5s+ff9tnxvBZ3L9/vzBkyBBBoVAIISEhwrp161p8L60diW/rnSgisfFaewOvtTfwWmuK11rLuNbezNPTU3jggQfMPo5M8U43NevChQsAAC8vLwD6Efl169bhoYcewgsvvIC0tDQsX74cGRkZ+O677wDoRwcnTpwIHx8fvPLKK/Dw8MClS5fwv//9DwDg4+ODjz/+GAsWLMD999+PBx54AAAwcOBAAMDp06cxYsQIBAUF4ZVXXoGLiwu++eYbJCYm4ttvv8X9999vEuOf//xn+Pj4YOnSpaiqqmr2vbz22mt4/fXXER8fjwULFuDcuXP4+OOPcfjwYRw4cMBkxLGkpASTJ0/Go48+iscffxx+fn5t/hn+8Y9/xJo1a/DLL79gwoQJTbbZsWMHZs6cifHjx+Pvf/87ACAjIwMHDhzAokWLMHr0aDz33HP46KOP8Je//AXh4eEAYPwXAM6dO4eZM2fi6aefxrx589C3b987xrVw4UJ4eHjgtddeM/4sLl++jD179kAikbT6/bUmtpvV1NRg7NixyMrKwsKFCxESEoJNmzbhiSeeQFlZGRYtWmTSfv369aioqMDTTz8NiUSCd955Bw888AAuXrx4x1HigoICDB8+HNXV1Xjuuefg5eWFdevW4b777sPmzZtx//33Y/To0fjyyy/xxz/+ERMmTMCsWbPu+F5/+OEHAPr/09bYuXMnJk+ejJ49e+K1115DTU0N/vnPf2LEiBE4duyYcQGVgoICDBs2DBKJBAsXLoSPjw9+/vlnzJ07F+Xl5Vi8eHGzsQ4cOBAeHh54/vnnMXPmTEyZMgWurq7NxrR27Vo8+eST6NevH5KSkuDh4YHjx48jOTkZf/jDH5o97umnn8batWsxZ84cPPfcc8jOzsbKlStx/Pjx235/srKy8NBDD2Hu3LmYPXs2Pv/8czzxxBOIiYlBv379zP7MENk6Xmt5rW0Jr7XN47W2Y6+1lZWVqKysNKkwoTYSO+sn8RlGw3bu3CkUFRUJubm5woYNGwQvLy/ByclJuHLlipCeni4AEJ566imTY1988UUBgLBr1y5BEAThu+++a3Ee053mmY0fP14YMGCAyYi+TqcThg8fLoSGht4W88iRI4WGhoYm349h1LGwsFCQy+XCxIkTBa1Wa2y3cuVKAYDw+eefG58bM2aMAEBYvXp1yz84oeXR9+vXrwsAhPvvv9/43K2jm4sWLRLc3d1vex83u9Pdve7duwsAhOTk5Ca/19Toe0xMjMn8s3feeUcAYHKXo7n/o1tf806x3Tr6vmLFCgGA8NVXXxmf02g0QlxcnODq6iqUl5cLgnBj9N3Ly8tkhPf7778XAAg//vjjbee62eLFiwUAwv79+43PVVRUCCEhIUKPHj1MPgcAhGeeeeaOrycIgnD//fcLAG67Y9OcqKgowdfXVygpKTE+d+LECUEqlQqzZs0yPjd37lwhICBAKC4uNjn+0UcfFVQqlVBdXX3HWG++U3GzW38PysrKBDc3NyE2NlaoqakxaXvzXLRbP5/79+8XAJiM0AuCICQnJ9/2vOGzuG/fPuNzhYWFgkKhEF544QXjc3dzt5p3usla8VrLay2vtbzWGlj6tdbgzTffFAAIKSkpbX4N0uPq5WQUHx8PHx8fBAcH49FHH4Wrqyu+++47BAUFYdu2bQCAJUuWmBzzwgsvAAB++uknADfmumzduhX19fVmnb+0tBS7du3CI488goqKChQXF6O4uBglJSVISEhAZmYmrl69anLMvHnzWpxLs3PnTmg0GixevNhknsq8efPg7u5ujN1AoVBgzpw5ZsXeHMNIaEVFRbNtPDw8UFVVhR07drT5PCEhIUhISGh1+/nz55uMmC5YsAAODg7G/+eOsm3bNvj7+5vMW3R0dMRzzz2HyspK7N2716T9jBkz0KVLF+PXo0aNAgBcvHixxfMMHToUI0eOND7n6uqK+fPn49KlSzhz5ozZsZeXlwMA3NzcWmybl5eH9PR0PPHEE/D09DQ+P3DgQEyYMMH4cxYEAd9++y2mTZsGQRCMn/ni4mIkJCRArVbj2LFjZsfalB07dqCiogKvvPIKlEqlyffudMdl06ZNUKlUmDBhgkl8MTExcHV1xe7du03aR0REGP+fAP0dt759+7b4f0ZkL3it1eO1tuPwWstrbXvYt28fXn/9dTzyyCMYN25cu72uvWJ5ORmtWrUKffr0gYODA/z8/NC3b1/jhfPy5cuQSqXo3bu3yTH+/v7w8PDA5cuXAQBjxozBgw8+iNdffx0ffPABxo4di8TERPzhD39ocaGRrKwsCIKAv/3tb/jb3/7WZJvCwkIEBQUZvw4JCWnxfRliu7UMTC6Xo2fPnsbvGwQFBbXboiSVlZUA7nzx+POf/4xvvvkGkydPRlBQECZOnIhHHnmkxe00btaan8PNQkNDTb52dXVFQEBAh29FcvnyZYSGhpp0yIAb5U63/l9069bN5GtDp+D69estnic2Nva2528+T//+/c2K3d3dHYC+U9fSYjDNfeYMMWzfvh1VVVWoqqpCWVkZ1qxZgzVr1jT5WoWFhWbF2RxDCau57zszMxNqtRq+vr5Nfv/W+G79PwP0/28t/Z8R2Qtea/V4re04vNbyWnu3zp49i/vvvx/9+/fHZ5991i6vae+YdJPR0KFDW9xzsKU5SBKJBJs3b8Zvv/2GH3/8Edu3b8eTTz6J9957D7/99tsd58DodDoAwIsvvtjsSPKtHREnJ6c7xtMW7fmap06dAnB73Dfz9fVFeno6tm/fjp9//hk///wz/vOf/2DWrFlYt25dq87TET+H5mi12k47V3N3VgRB6LQYDMLCwgAAJ0+eNBldvhuGz/zjjz+O2bNnN9nGMAdTLDqdDr6+vvj666+b/L6Pj4/J15b0f0ZkiXitbf/X5LX27ljS321ea8W/1ubm5mLixIlQqVTYtm1bq6oOqGVMuqlVunfvDp1Oh8zMTJNFGAoKClBWVobu3bubtB82bBiGDRuGt99+G+vXr8djjz2GDRs24Kmnnmq2M9GzZ08A+hKo+Pj4do0d0C+AYjgHAGg0GmRnZ7fruW715ZdfAkCL5WhyuRzTpk3DtGnToNPp8Oc//xmffPIJ/va3v6F3795mLbjSGpmZmbjnnnuMX1dWViIvLw9TpkwxPtelSxeUlZWZHKfRaJCXl2fynDmxde/eHb///jt0Op3JCPzZs2eN328P3bt3x7lz5257/m7OM23aNCxfvhxfffVVix2Bmz9zTcXg7e0NFxcXKJVKuLm5QavVdujnEAB69eoFQN85vVPHtKnjdu7ciREjRrRbh7O9P89EtoLX2rbhtdYUr7W81gJtu9aWlJRg4sSJqKurQ0pKCgICAtolFgI4p5taxXCBWLFihcnz77//PgBg6tSpAPSlSLeOskVFRQEA6urqAMC4D+etFxlfX1+MHTsWn3zyyW0XG+DGHonmio+Ph1wux0cffWQS27///W+o1Wpj7O1t/fr1+OyzzxAXF4fx48c3266kpMTka6lUahxxNfzMXFxcANz+M2urNWvWmMwD/Pjjj9HQ0IDJkycbn+vVqxf27dt323G3jr6bE9uUKVOQn5+PjRs3Gp9raGjAP//5T7i6umLMmDFteTtNnufQoUNITU01PldVVYU1a9agR48eiIiIMPs14+LiMGnSJHz22WfYsmXLbd/XaDR48cUXAQABAQGIiorCunXrTH4up06dwi+//GL8fZLJZHjwwQfx7bffGu/U3Kytn/mmTJw4EW5ubli+fDlqa2tNvnenkfFHHnkEWq0Wb7755m3fa2hoaNNnsr0/z0S2gtda8/Faeztea3mtBcz/PFdVVWHKlCm4evUqtm3bdtv0CLo7vNNNrRIZGYnZs2djzZo1KCsrw5gxY3Do0CGsW7cOiYmJxpHcdevW4V//+hfuv/9+9OrVCxUVFfj000/h7u5u/OPn5OSEiIgIbNy4EX369IGnpyf69++P/v37Y9WqVRg5ciQGDBiAefPmoWfPnigoKEBqaiquXLmCEydOmB27j48PkpKS8Prrr2PSpEm47777cO7cOfzrX//CkCFD8Pjjj9/1z2fz5s1wdXWFRqPB1atXsX37dhw4cACRkZHYtGnTHY996qmnUFpainHjxqFr1664fPky/vnPfyIqKsp4pyMqKgoymQx///vfoVaroVAoMG7cuGbn/rREo9Fg/PjxeOSRR4w/i5EjR+K+++4zietPf/oTHnzwQUyYMAEnTpzA9u3bb9s2wpzY5s+fj08++QRPPPEEjh49ih49emDz5s04cOAAVqxY0W4lTK+88gr++9//YvLkyXjuuefg6emJdevWITs7G99+++1t89xa64svvsDEiRPxwAMPYNq0aRg/fjxcXFyQmZmJDRs2IC8vD++++y4A4B//+AcmT56MuLg4zJ0717iNiUqlwmuvvWZ8zf/7v//D7t27ERsbi3nz5iEiIgKlpaU4duwYdu7cidLS0vb4kcDd3R0ffPABnnrqKQwZMgR/+MMf0KVLF5w4cQLV1dXNlleOGTMGTz/9NJYvX4709HRMnDgRjo6OyMzMxKZNm/Dhhx/ioYceMiuWtnye33rrLQD6rY4A/Z2tX3/9FQDw6quvmnV+IkvFa+2d8VrLay2vta1n7uf5sccew6FDh/Dkk08iIyMDGRkZxu+5uroiMTHRrPPTLTp/wXSyNIbtDu609YggCEJ9fb3w+uuvCyEhIYKjo6MQHBwsJCUlmWw5cuzYMWHmzJlCt27dBIVCIfj6+gr33nuvcOTIEZPXOnjwoBATEyPI5fLbtsu4cOGCMGvWLMHf319wdHQUgoKChHvvvVfYvHlzq2K+dfsGg5UrVwphYWGCo6Oj4OfnJyxYsOC2LSnGjBkj9OvXr4Wf2A2GbUwMD6VSKXTt2lW49957hc8//9zkZ2Nw6zYRmzdvFiZOnCj4+voKcrlc6Natm/D0008LeXl5Jsd9+umnQs+ePQWZTGayBUT37t2FqVOnNhlfc9uY7N27V5g/f77QpUsXwdXVVXjsscdMttsQBEHQarXC//f//X+Ct7e34OzsLCQkJAhZWVm3veadYrt1GxNBEISCggJhzpw5gre3tyCXy4UBAwYI//nPf0zaNLc1hyA0v73KrS5cuCA89NBDgoeHh6BUKoWhQ4cKW7dubfL1WrONiUF1dbXw7rvvCkOGDBFcXV0FuVwuhIaGCs8++6yQlZVl0nbnzp3CiBEjBCcnJ8Hd3V2YNm2acObMmdtes6CgQHjmmWeE4OBgwdHRUfD39xfGjx8vrFmzpsVYW7uNicEPP/wgDB8+3BjT0KFDhf/+97/G79/6+TRYs2aNEBMTIzg5OQlubm7CgAEDhJdfflm4du2asU1zn8WmPgfNfWaac/Pv2a0PImvAa+0NvNbewGtt03it7fxrrWErsqYeTcVK5pEIAle3ISIiIiIiIuoInNNNRERERERE1EGYdBMRERERERF1ECbdRERERERERB2ESTcRERERERFRB2HSTURERERERNRBmHQTERERERERdRAHsQOwJDqdDteuXYObmxskEonY4RARETVJEARUVFQgMDAQUql1jZ/zWktERNagPa+1TLpvcu3aNQQHB4sdBhERUavk5uaia9euYodhFl5riYjImrTHtZZJ903c3NwA6H+w7u7uIkdDRETUtPLycgQHBxuvW9aE11oiIrIG7XmtZdJ9E0OZm7u7OzsCRERk8ayxPJvXWiIisibtca21rolgRERERERERFaESTcRERERERFRB2lT0r1q1Sr06NEDSqUSsbGxOHTo0B3bb9q0CWFhYVAqlRgwYAC2bdtm8n1BELB06VIEBATAyckJ8fHxyMzMNGnz9ttvY/jw4XB2doaHh0ez51q7di0GDhwIpVIJX19fPPPMM215i0RERERERER3zeyke+PGjViyZAmWLVuGY8eOITIyEgkJCSgsLGyy/cGDBzFz5kzMnTsXx48fR2JiIhITE3Hq1Cljm3feeQcfffQRVq9ejbS0NLi4uCAhIQG1tbXGNhqNBg8//DAWLFjQbGzvv/8+/vrXv+KVV17B6dOnsXPnTiQkJJj7FomIiIiIiIjahUQQBMGcA2JjYzFkyBCsXLkSgH6/zeDgYDz77LN45ZVXbms/Y8YMVFVVYevWrcbnhg0bhqioKKxevRqCICAwMBAvvPACXnzxRQCAWq2Gn58f1q5di0cffdTk9dauXYvFixejrKzM5Pnr168jKCgIP/74I8aPH2/OWzIqLy+HSqWCWq3m4i5ERGSx2uN69fHHH+Pjjz/GpUuXAAD9+vXD0qVLMXny5Cbbr127FnPmzDF5TqFQmAyQd1bsREREHa09r1dm3enWaDQ4evQo4uPjb7yAVIr4+HikpqY2eUxqaqpJewBISEgwts/OzkZ+fr5JG5VKhdjY2GZfsyk7duyATqfD1atXER4ejq5du+KRRx5Bbm5us8fU1dWhvLzc5EFERGQPunbtiv/7v//D0aNHceTIEYwbNw7Tp0/H6dOnmz3G3d0deXl5xsfly5c7MWIiIiLrZFbSXVxcDK1WCz8/P5Pn/fz8kJ+f3+Qx+fn5d2xv+Nec12zKxYsXodPp8P/+3//DihUrsHnzZpSWlmLChAnQaDRNHrN8+XKoVCrjIzg4uNXnIyIismbTpk3DlClTEBoaij59+uDtt9+Gq6srfvvtt2aPkUgk8Pf3Nz5uvXYTERHR7Wxm9XKdTof6+np89NFHSEhIwLBhw/Df//4XmZmZ2L17d5PHJCUlQa1WGx93uitORERkq7RaLTZs2ICqqirExcU1266yshLdu3dHcHBwi3fFDVhVRkRE9s6spNvb2xsymQwFBQUmzxcUFMDf37/JY/z9/e/Y3vCvOa/ZlICAAABARESE8TkfHx94e3sjJyenyWMUCgXc3d1NHkRERPbi5MmTcHV1hUKhwJ/+9Cd89913JtfRm/Xt2xeff/45vv/+e3z11VfQ6XQYPnw4rly5csdzsKqMiIjsnVlJt1wuR0xMDFJSUozP6XQ6pKSkNDsyHhcXZ9Ie0M+/NrQPCQmBv7+/SZvy8nKkpaXdcbT9ViNGjAAAnDt3zvhcaWkpiouL0b1791a/DhERkb3o27cv0tPTkZaWhgULFmD27Nk4c+ZMk23j4uIwa9YsREVFYcyYMfjf//4HHx8ffPLJJ3c8B6vKiIjI3jmYe8CSJUswe/ZsDB48GEOHDsWKFStQVVVlXNF01qxZCAoKwvLlywEAixYtwpgxY/Dee+9h6tSp2LBhA44cOYI1a9YA0M8PW7x4Md566y2EhoYiJCQEf/vb3xAYGIjExETjeXNyclBaWoqcnBxotVqkp6cDAHr37g1XV1f06dMH06dPx6JFi7BmzRq4u7sjKSkJYWFhuOeee+7yx0RERGR75HI5evfuDQCIiYnB4cOH8eGHH7aYSAOAo6MjoqOjkZWVdcd2CoUCCoWiXeIlIiKyRmYn3TNmzEBRURGWLl2K/Px8REVFITk52biYSk5ODqTSGzfQhw8fjvXr1+PVV1/FX/7yF4SGhmLLli3o37+/sc3LL7+MqqoqzJ8/H2VlZRg5ciSSk5OhVCqNbZYuXYp169YZv46OjgYA7N69G2PHjgUAfPHFF3j++ecxdepUSKVSjBkzBsnJyXB0dDT3bRIREdkdnU6Hurq6VrXVarU4efIkpkyZ0sFRERERWTez9+m2Zdw7lIiIrEF7XK+SkpIwefJkdOvWDRUVFVi/fj3+/ve/Y/v27ZgwYcJtlWtvvPEGhg0bht69e6OsrAz/+Mc/sGXLFhw9erTZeeAdFTsREVFHa8/rldl3uomIiMj6FRYWYtasWcjLy4NKpcLAgQONCTdwe+Xa9evXMW/ePOTn56NLly6IiYnBwYMHzUq4iYiI7BHvdN+Eo+9ERGQNrPl6Zc2xExGR/eCdbiKyaA1aHQ5cKEFWYaXJ8wEqJcaF+ULpKBMpMiIiItugrqlHSkYBrlfXG5+TABjYVYWY7l0gkUjEC46ITDDpJqJ2IQgCTl8rx3fHr+L79Gsormx6MSZXhQMm9/fH/YOCMCzEC1IpOwVEREStUa/VYe+5Inx3/Cp2ZBRA06Brsl03T2ckRgfh/ugghHi7dHKURHQrlpffhCVvRG2Tp67BC9+cwMELJcbnPF3kiOvpBQeZPqkWBOBYznVcuV5jbNPHzxUrZkQjIpC/b0TmsObrlTXHTiSmA1nFeHHTCeSpa43P9fFzRXjAjd+j2notfs0sRpVGa3xuWmQg3krsD5UTd/MhMgfLy4nIYiSfysf/9+3vUNfUQ+4gxYRwP9wfHYQxfX3gKJOatNXpBBy5fB3fHb+Crb/n4XxBJRJXHcArk8MwZ0QPlsIRERHdol6rw3u/nMcn+y5AEABvVwUSowJx/6AgRAS433btrNFo8cuZfHx3/Cr2nS/Cjyeu4djl6/hoZhRiunuK9C6I7BvvdN+Eo+9ErVdbr8WbW8/g67QcAMCAIBU+mhnd6jK20ioNXt58AjszCgEA48J88Y+HBsLLVdFhMRPZCmu+Xllz7ESd7XJJFZ7773GcuKIGAMwc2g1L742Ak7x1a6Ok55bhuf8eR05pNWRSCRaND8Uz9/SGjFO7iFrUntcractNiIhMVWsa8IdPfzMm3E+P7olvFww3a96Yp4scn84ajDem94PcQYpdZwtx38oDyFPXtHwwERGRjTt9TY1p//wVJ66ooXJyxOrHB2H5AwNanXADQFSwB356biTujw6CVifg/R3n8ex/j0Gr4z03os7EpJuIzFLXoMXTXx7FsZwyqJwc8cWTQ5E0JRxyB/P/nEgkEsyK64HvnxmB7l7OuFpWgz/++xBKqzQdEDkREZF1yC6uwuzPD6G8tgGRwR74edEoTOof0KbXclM64oMZUXj/kUjIZVJsO5mPv353Eix2Jeo8TLqJqNW0OgHPb0zH/sxiOMtl+M+cIRjdx+euXzc8wB1fPxWLAJUSWYWVeOI/h1BRW9/ygURERDYmT12Dxz9LQ3GlBhEB7vjiyaEI9HC669d9YFBXfDQzClIJsOFwLv7v57NMvIk6CZNuImoVQRDwl/+dxLaT+ZDLpFjzx8EY1K1Lu71+1y7O+HJuLDxd5Pj9ihrzvjiC2nptywcSERHZiNIqDf7470O4WlaDnt4u+GLu0HZddXxS/wD83wMDAQCf7LuIj/deaLfXJqLmMekmolb5YGcmNh7JhVQCfDQzCiNDvdv9HL19XbFuzlC4Khzw28VSPL8xnaPwRERkF+oatJiz9jCyCisRoFLii7lD4d0Bi4s+MiQYr04NBwC8k3wOm49eafdzEJEpJt1E1KK0iyX4565MAMDyBwa0eV5ZawzoqsKnswZDLpPi51P5+O+h3A47FxERkaV4f8d5nMgtg4ezI76cG4uuXZw77FxPjeqJP4/tBQD425ZTyC6u6rBzERGTbiJqgbqmHku+OQFBAB6O6YoZQ7p1+DnjennhxYQ+AIA3t57BxaLKDj8nERGRWFIvlGDNvosAgL8/OBC9fV07/JwvTOyLYT09UVOvxeKN6ajX6jr8nET2ikk3Ed3R0u9P4WpZDbp7OWPZff067bxPjeyJ4b282BkgIiKbpq6ux5Jv0iEIwKNDgpHQz79TziuTSvD+I1FwVzrgRG4Z/pmS2SnnJbJHTLqJqFnfp1/F9+nXIJNKsGJGFFwVDp12bqlUgvceiYTKyRG/X1Fjxc7znXZuIiKiziAIAv665STy1LXo4eWMv90b0annD/Rwwtv3DwAArNydhSOXSjv1/ET2gkk3ETXpyvVqvPrdKQDAc+NCEd2OK5W3VoDKCcsf0HcG/rXnAg5lszNARES247vjV7H19zz94Paj0XDpxMFtg2mRgXggOgg6AVi8MZ1bdhJ1ACbdRNSkV7ecQkVdA2K6d8Ez9/QSLY4pAwLwUExXCALw8uYTqGvgNmJERGT9SirrsOyH0wCAxeNDERXsIVosr0/vh65dnHDleg3e3X5OtDiIbBWTbiK6ze6zhdhzrgiOMgnefTgSDjJx/1QsmxYBHzcFLpVUY93BS6LGQkRE1B7e23EeFbUN6BfojgVjxRvcBgA3pSPeeVC/f/dXaTk4X1AhajxEtoZJNxGZqNfq8OZPZwAAc0aEIMTbReSI9J2BlxL6AgD+mZKF4so6kSMiIiJqu4y8cmw4lAMAWDatn+iD2wAwvLc3Evr5QasT8ObWMxAEQeyQiGyG+L/hRGRRvki9jItFVfB2lWPhuN5ih2P00KCuGBCkQkVdA977haVvRERknQRBwBs/noFOAKYOCMDQEE+xQzL665QIyGVS7M8sxq6zhWKHQ2QzmHQTkVFplQYfNq4S/uLEvnBXOooc0Q1SqQTLpulXdd1wOBenrqpFjoiIiMh8208XIPViCeQOUrwyOUzscEx083LGkyNDAABv/ZQBTQO36yRqD0y6icjo/R3nUF7bgIgAdzw8OFjscG4zuIcnpkUGQhCAN1j6RkREVqauQYv/ty0DADB/VE8EezqLHNHtFo7rDW9XBbKLq7iOClE7YdJNRACAs/nlWJ9mmF8WAZlUInJETXtlchiUjlIcyi7Fz6fyxQ6HiIio1T7/9RJySqvh564QffG05rgqHPDyJP06Kh+lZHIdFaJ2wKSbiAAA724/B50ATBngj9ieXmKH06wgDyfMH63vqPxj+zlodbzbTURElk9dU49/7ckCAPx/k8JE2ZO7tW5eR+Vfuy+IHQ6R1WPSTUQ4fU2NnRmFkEqAFyb2FTucFs0f3RMezo7ILq7C1t+viR0OERFRi9YdvISK2gb08XNFYlSQ2OHckVQqMe4asv7QZd7tJrpLTLqJCCt36Ufe7x0YiF4+riJH0zJXhQPmjtAv9LJyVxZ0vNtNREQWrLKuAZ8fyAYALBwXCqmFTuG62ahQb0QGe6C2XodP918UOxwiq8akm8jOnS+oMM6NtqQtwloye0QPuCkdkFlYieTTnNtNRESW68vUyyirrkdPHxdMHRAgdjitIpFI8Fxjv+DL1Mu4XqUROSIi68Wkm8jOGe5yT+7vjz5+biJH03ruSkfMGd4DAPDPXVlcyZyIiCxStaYBnzXeKX5mbG+LXai0KePCfNEv0B3VGq3xTj0RmY9JN5Edu1hUaZwTbU13uQ2eHBkCF7kMGXnl2JlRKHY4REREt1mfloOSKg26eTpjelSg2OGYRSKR4NnG/sHaA5egrqkXOSIi68Skm8iOrdp9AToBiA/3Rb9AldjhmM3DWY5ZxrvdmbzbTUREFqW2XotP9unvcv95bC84yKyv6z0xwh99/dxQUdfAfbuJ2sj6fvOJqF3klFRjS/pVAMCz40JFjqbtnhoZAidHGX6/osae80Vih0NERGS08XAuiirqEOThhAcGdRU7nDaRSiV4pvFu979/zUZFLe92E5mLSTeRnfrs14vQ6gSM7uODyGAPscNpMy9XBR6L7QYAWL2He4kSEZFl0OoErGm8y/2nMT0hd7DebvfUAQHo6eMCdU09Nh7OFTscIqtjvb/9RNRm5bX12Hz0CgDgT6N7ihzN3Zs7KgQyqQRp2aXIyCsXOxwiIiLszCjA1bIadHF2xMODg8UO567IpBLMG6XvL3yRehlabtVJZBYm3UR2aNORK6jWaNHHzxVxvbzEDueuBaicMKm/PwBwvhkREVmEtQcuAQBmDu0GpaNM3GDaQWJUEFROjsgprcbus1y8lMgcTLqJ7IxWJxgT0yeGh0AisZ6tS+7kicYF1b47fpV7iRIRkajO5pcj9WIJZFIJHh/WXexw2oWTXIZHh+rv2K/lADeRWZh0E9mZPecKkVNaDZWTIxKjrWvrkjsZ3L0L+gW6o65Bhw2cb0ZERCIyDG5P6uePQA8ncYNpR38c1h1SCfBrVjEyCyrEDofIajDpJrIzhtHpGUOC4Sx3EDeYdiSRSIx3u7/67TIatDpxAyIiIrt0vUqD747rdweZ3XhdshVduzhjQoQfAN7tJjIHk24iO5JZUIH9mcWQSvSj1bZmWmQgPF3kuFpWg50ZBWKHQ0REdmjjkVzU1usQEeCOIT26iB1Ou3tieAgA4H/HrkJdze3DiFqjTUn3qlWr0KNHDyiVSsTGxuLQoUN3bL9p0yaEhYVBqVRiwIAB2LZtm8n3BUHA0qVLERAQACcnJ8THxyMzM9Okzdtvv43hw4fD2dkZHh4edzxfSUkJunbtColEgrKysra8RSKbtC71EgAgPtwPwZ7O4gbTAZSOMsxsnG/2n8YFbIiIiDpLg1aHL1MvAwCeGNHDZtZNudmwnp4I83dDTb0W3xzhdC6i1jA76d64cSOWLFmCZcuW4dixY4iMjERCQgIKC5texfDgwYOYOXMm5s6di+PHjyMxMRGJiYk4deqUsc0777yDjz76CKtXr0ZaWhpcXFyQkJCA2tpaYxuNRoOHH34YCxYsaDHGuXPnYuDAgea+NSKbpq6px7dH9eVuT4zoIW4wHejxYd25fRgREYnCsE2Yp4sc90XazropN7t5Ote61EvcPoyoFcxOut9//33MmzcPc+bMQUREBFavXg1nZ2d8/vnnTbb/8MMPMWnSJLz00ksIDw/Hm2++iUGDBmHlypUA9He5V6xYgVdffRXTp0/HwIED8cUXX+DatWvYsmWL8XVef/11PP/88xgwYMAd4/v4449RVlaGF1980dy3RmTTvj16BTX1WvT1c0NcT+vfJqw5N28f9kXjnX0iIqLO8EXjXe6ZQ4NtYpuw5kyPCoKHsyOuXK/h9mFErWBW0q3RaHD06FHEx8ffeAGpFPHx8UhNTW3ymNTUVJP2AJCQkGBsn52djfz8fJM2KpUKsbGxzb5mc86cOYM33ngDX3zxBaTSlt9aXV0dysvLTR5EtkgQBGMJ2OPDutlkudvNHo/Vz1f/8UQeqjUNIkdDRET2IKekGgcvlEAi0e/Nbcuc5DI8NKgrALDEnKgVzEq6i4uLodVq4efnZ/K8n58f8vPzmzwmPz//ju0N/5rzmk2pq6vDzJkz8Y9//APdurXuD93y5cuhUqmMj+Dg4Fafj8ia/H5FjbP5FVA4SHFfVJDY4XS4YT090d3LGZV1Ddh2svV/R4iIiNpq01F98jmytze6drG9dVNu9cgQfb9519lCFFXUiRwNkWWzmdXLk5KSEB4ejscff9ysY9RqtfGRm8uROrJNGxtHoSf394fKyVHkaDqeRCLBI4P1nYFvuGc3ERF1MK1OwOajVwDot+S0B3383BAV7IEGnYD/HbsidjhEFs2spNvb2xsymQwFBaZb8RQUFMDf37/JY/z9/e/Y3vCvOa/ZlF27dmHTpk1wcHCAg4MDxo8fb4x52bJlTR6jUCjg7u5u8iCyNTUaLX5MvwYAxkTUHjw4qCukEuDQpVJcLKoUOxwiIrJh+zKLkKeuhYezo3Efa3tgGGDYeCQXgsAF1YiaY1bSLZfLERMTg5SUFONzOp0OKSkpiIuLa/KYuLg4k/YAsGPHDmP7kJAQ+Pv7m7QpLy9HWlpas6/ZlG+//RYnTpxAeno60tPT8dlnnwEA9u/fj2eeeabVr0Nka34+lYeKugYEezphmA0voHYrf5USY/r4AAA2HeUIPBERdZxNjRVliVFBUDjY7gJqt7p3YACcHGW4WFSFYznXxQ6HyGI5mHvAkiVLMHv2bAwePBhDhw7FihUrUFVVhTlz5gAAZs2ahaCgICxfvhwAsGjRIowZMwbvvfcepk6dig0bNuDIkSNYs2YNAH0Z6OLFi/HWW28hNDQUISEh+Nvf/obAwEAkJiYaz5uTk4PS0lLk5ORAq9UiPT0dANC7d2+4urqiV69eJnEWFxcDAMLDw1vc15vIlm1sLK9+JCYYUqltL6B2qxlDgrH7XBG+PXoFL0zoAweZzcyoISIiC1FSWYcdZ/QVm/ZSWm7gpnTE1IEB2Hz0CjYezkVMd0+xQyKySGb3QGfMmIF3330XS5cuRVRUFNLT05GcnGxcCC0nJwd5eXnG9sOHD8f69euxZs0aREZGYvPmzdiyZQv69+9vbPPyyy/j2Wefxfz58zFkyBBUVlYiOTkZSqXS2Gbp0qWIjo7GsmXLUFlZiejoaERHR+PIkSN38/6JbNql4iqkZZdCKgEeGtxV7HA63bgwP3i5yFFYUYc954rEDofIonz88ccYOHCgcXpVXFwcfv755zses2nTJoSFhUGpVGLAgAHYtm1bJ0VLZLm+O34V9VoBA7uqEB5gf1MVDVPXtv6eh8o67hhC1BSJwAkYRuXl5VCpVFCr1ZzfTTbhneSz+NeeCxjb1wdr5wwVOxxRvP3TGXy6PxsTIvzw6azBYodD1C7a43r1448/QiaTITQ0FIIgYN26dfjHP/6B48ePo1+/fre1P3jwIEaPHo3ly5fj3nvvxfr16/H3v/8dx44dMxlI74zYiSyFIAhIWLEP5wsq8VZifzw+rLvYIXU6QRAw/r29uFhchb8/OAAzhtj2dmlkP9rzesVaSyIb1aDVGVdStacF1G5leO+7zhaisKJW5GiILMe0adMwZcoUhIaGok+fPnj77bfh6uqK3377rcn2H374ISZNmoSXXnoJ4eHhePPNNzFo0CCsXLmykyMnshzpuWU4X1AJhYMU0yIDxQ5HFBKJBA83Xms3cscQoiYx6SayUfszi1FYUQdPFzniw+1nJdVbhfq5IbqbB7Q6Ad8duyp2OEQWSavVYsOGDaiqqmp2EdPU1FTEx8ebPJeQkIDU1NQ7vnZdXR3Ky8tNHkS2wrBQ55QBAXaxJWdzHhwUBJlUgmM5Zcgq5I4hRLdi0k1ko747rk8w74sMhNzBvn/VH47Rj8Bvadw6jYj0Tp48CVdXVygUCvzpT3/Cd999h4iIiCbb5ufnG9dvMfDz80N+fv4dz7F8+XKoVCrjIzjYfitvyLZoGnT46Xf9OkYPxdjfuik383W/sWPI9+kc4Ca6lX33xIlsVFVdg3El1fujg0SORnxTBvjDUSZBRl45zhdUiB0OkcXo27cv0tPTkZaWhgULFmD27Nk4c+ZMu54jKSkJarXa+MjNZfkp2YY95wqhrqmHr5vCrrbkbM70KH15/ffp17hnN9EtmHQT2aAdZwpQU69FDy9nDOyqEjsc0Xk4yzGmjy8AjsAT3Uwul6N3796IiYnB8uXLERkZiQ8//LDJtv7+/igoKDB5rqCgAP7+/nc8h0KhMK6QbngQ2YLvT+irp+6LDITMzrbkbMqECD84y2XIKa3G8dwyscMhsihMuols0JbGxHJ6VBAkEnYEACAxmiPwRC3R6XSoq6tr8ntxcXFISUkxeW7Hjh3NzgEnsmUVtfXY2VhRlsiKMgCAs9wBEyP0U1C+P84BbqKbMekmsjEllXXYn1kM4EapFwHjw/zgIpfhyvUaHL18XexwiESXlJSEffv24dKlSzh58iSSkpKwZ88ePPbYYwCAWbNmISkpydh+0aJFSE5OxnvvvYezZ8/itddew5EjR7Bw4UKx3gKRaH45XYC6Bh16+rigXyCrNwymR+kHILb+nocGrU7kaIgsB5NuIhvz08k8aHUCBnZVoaePq9jhWAwnuQwJ/fVlsN9zQTUiFBYWYtasWejbty/Gjx+Pw4cPY/v27ZgwYQIAICcnB3l5ecb2w4cPx/r167FmzRpERkZi8+bN2LJli1l7dBPZCkNFWSIrykyMDPWGp4scJVUa/JpVLHY4RBbDQewAiKh9GRJKw2gz3ZAYFYT/HbuKn07mYem0CDjKOO5I9uvf//73Hb+/Z8+e2557+OGH8fDDD3dQRETWoaiiDgcaE8r77HRv7uY4yqS4d2AAvki9jB/Sr2FsX1+xQyKyCOxxEtmQ3NJqHL18HVIJMG1ggNjhWJzhvbzg7SpHaZUG+zOLxA6HiIis0Nbfr0EnAFHBHujh7SJ2OBbHMLVt++l81Gi0IkdDZBmYdBPZEMPK3MN7ecPXXSlyNJbHQSbFvQNvLKhGRERkri3GijLe5W7KoG5dEOzphCqNFjsyClo+gMgOMOkmshGCILAj0AqGVWZ/OV2AqroGkaMhIiJrcqm4CidyyyCVwDiIS6YkEgmmR+qvtT9wm04iAEy6iWxGRl4FsgorIXeQGhcMo9tFdlWhu5czauq12HGGI/BERNR6PzTuzT2itzd83BQiR2O5DIP/e84V4XqVRuRoiMTHpJvIRvx0Ut8RGNfXF+5KR5GjsVwSicS48M1PJ/NaaE1ERHTDT7/rrxtcQO3OQv3cEObvhgadwAFuIjDpJrIJgiBg28l8AMAULqDWoikD9D+jveeLUMkScyIiaoWswkqcK6iAo0yCiRGsKGvJ1MZr7bZTHOAmYtJNZAPO5lcgu7gKCgcpxoVxe46WhPm7oae3CzQNOqRwkRciImqFnxuro0b09obKmRVlLTHcBDiQVQx1db3I0RCJi0k3kQ0wdATG9PGBq8JB5Ggsn0QiweQB+rsUPzdWCBAREd3JtlONFWX9WVHWGr18XNHXzw31WoGrmJPdY9JNZOUEQTDOTTaUTVPLJjd2mnafK+Qq5kREdEfZxVXIyCuHg1SCif38xA7Hahj6Jdu4hgrZOSbdRFYus7ASF4qqIJdJMT6cpeWt1S/QHd29nFHXoMPuc4Vih0NERBbMkDTG9fKCh7Nc5Gisx5TGqrL9mUUor2WJOdkvJt1EVs6wkuroPt5w46rlrSaRSIx3u1liTkREd2JIuqeyoswsoX5uCPV1Rb1WwE6uYk52jEk3kZX7+RRLy9vK0HnadbYQNRqtyNEQEZElulxShdPXyiGTSjCxH1ctN9dkY4k5B7jJfjHpJrJiWYUVOF9QCUeZBOPDOcfMXP2D3NG1ixNq6rXYwxJzIiJqgiFZjOvpBU8XlpabyzDAvS+zCBUsMSc7xaSbyIoZOgIje3tD5cTScnNJJJIbi7yc4gg8ERHdzlBRZtj1gszTx88VPX3023TuOssBbrJPTLqJrNg2rlp+1ww/u5SMAtTWs8SciIhuyC2txu9X1JBKgASWlreJRCIx3u02rENDZG+YdBNZqYtFlTibXwEHqQQTIlha3laRXVUI8nBCtUaLveeLxA6HiIgsiOEud2yIF7xdFSJHY70MC5fuOV/EbTrJLjHpJrJSvzSuAsrtS+6ORCIx3r345TRXViUiohsM1wWWlt+d8AA3dPdyhqZBh30c4CY7xKSbyEr9clo/B5krqd69if30lQIpZwvQoNWJHA0REVmCooo6HM25DgCI52Kld0UikWBiY1XeL9w6jOwQk24iK1RYXovjuWUAgAnsCNy1wd27oIuzI8qq63H40nWxwyEiIguQklEAQQAGdlUh0MNJ7HCsnuEmQUpGAeo5wE12hkk3kRXamVEIQQAigz3gr1KKHY7Vc5BJjVuu/XKGq5gTEdGNO7ITuW5KuxjUrQu8XOQor23AoexSscMh6lRMuoms0I7GxJAdgfZjLHs7XQBBEESOhoiIxFRV14Bfs4oBcBpXe5FJJcYy/R0sMSc7w6SbyMpU1jXgQFYJACbd7WlUqA+UjlJcLavBmbxyscMhIiIR7TtfBE2DDt29nBHq6yp2ODZjgnGAO58D3GRXmHQTWZm954qg0eoQ4u2C3uwItBsnuQyjQn0AcASeiMje3VxaLpFIRI7GdowM9YaTowzX1LU4fY0D3GQ/mHQTWZlfbiotZ0egfd1cYk5ERPapXqtDSkZj0s3S8naldJRhTB/9ALdhFxYie8Ckm8iK1Gt12HW2EMCNba6o/YwP94NUApzJK0duabXY4RARkQgOZ5eivLYBXi5yDOrWRexwbI6h/8Ktw8ieMOkmsiJpF0tRUdsAb1c5ooLZEWhvni5yDO7hCYAl5kRE9sqQDI4P94VMyoqy9jYuTP9zPZtfgZwSDnCTfWDSTWRFDKXl8eF+7Ah0EGOJObcOIyKyO4IgGMueJ0awtLwjeDjLMbRxgJvXWrIXTLqJrIS+I2CYY8bS8o5i6GQdyi7F9SqNyNEQEVFnOn2tHNfUtXBylGFkqLfY4dgslpiTvWHSTWQlTl0tR355LZzlMgzvxY5AR+nm5YwwfzfoBBjnzxMRkX0wJIGj+3hD6SgTORrbZdg67MilUpRygJvsAJNuIiuRclbfERgVyo5AR4sP13cGmHQTEdmXXY3XWsN1gDpG1y43Brj3nue1lmxfm5LuVatWoUePHlAqlYiNjcWhQ4fu2H7Tpk0ICwuDUqnEgAEDsG3bNpPvC4KApUuXIiAgAE5OToiPj0dmZqZJm7fffhvDhw+Hs7MzPDw8bjvHiRMnMHPmTAQHB8PJyQnh4eH48MMP2/L2iCySIQEcH8aOQEcbF+4LANh3vgiaBp3I0RARUWfIV9fi1NVySCTAPWG+Yodj88Y3XmtTMph0k+0zO+neuHEjlixZgmXLluHYsWOIjIxEQkICCgub/oU5ePAgZs6ciblz5+L48eNITExEYmIiTp06ZWzzzjvv4KOPPsLq1auRlpYGFxcXJCQkoLa21thGo9Hg4YcfxoIFC5o8z9GjR+Hr64uvvvoKp0+fxl//+lckJSVh5cqV5r5FIotTWF6L36+oAbAj0BmiunrAy0WOiroGHLlUKnY4RETUCXaf0/dlo4I94O2qEDka2ze+sZpg7/ki1Gs5wE22zeyk+/3338e8efMwZ84cREREYPXq1XB2dsbnn3/eZPsPP/wQkyZNwksvvYTw8HC8+eabGDRokDEZFgQBK1aswKuvvorp06dj4MCB+OKLL3Dt2jVs2bLF+Dqvv/46nn/+eQwYMKDJ8zz55JP48MMPMWbMGPTs2ROPP/445syZg//973/mvkUii2O4yx0Z7AEfN3YEOppUKjEObqSwxJyIyC6kZDRuFcbB7U4RaRjgrm3AYQ5wk40zK+nWaDQ4evQo4uPjb7yAVIr4+HikpqY2eUxqaqpJewBISEgwts/OzkZ+fr5JG5VKhdjY2GZfs7XUajU8PT2b/X5dXR3Ky8tNHkSWKMVYWs6OQGeJN5a9FUAQBJGjISKijlRbr8WvWcUAgHGcxtUpZFIJxvbVX2t3scScbJxZSXdxcTG0Wi38/Ez/GPn5+SE/v+l99vLz8+/Y3vCvOa/ZGgcPHsTGjRsxf/78ZtssX74cKpXK+AgODm7z+Yg6Sm29Fr9mGjoCTLo7y8hQHzjKJLhUUo2LxVVih0NERB0o9UIJaut1CFQpER7gJnY4dsMwr5sLl5Kts8nVy0+dOoXp06dj2bJlmDhxYrPtkpKSoFarjY/c3NxOjJKodVIvlqCmXgt/dyX6BbqLHY7dcFU4YFhPLwAcgScisnU7G0vLx4X7QiKRiByN/RgV6g1HmQQXi6twsahS7HCIOoxZSbe3tzdkMhkKCkw3si8oKIC/v3+Tx/j7+9+xveFfc17zTs6cOYPx48dj/vz5ePXVV+/YVqFQwN3d3eRBZGkMCR87Ap1vvHFed0ELLYmIyFoJgsAdQkTipnREbEjjADfvdpMNMyvplsvliImJQUpKivE5nU6HlJQUxMXFNXlMXFycSXsA2LFjh7F9SEgI/P39TdqUl5cjLS2t2ddszunTp3HPPfdg9uzZePvtt806lsgSmXYEWFre2Qzz+g5fug51Tb3I0RARUUfIyKtAnroWSkcp4np5iR2O3TFMnePWYWTLzC4vX7JkCT799FOsW7cOGRkZWLBgAaqqqjBnzhwAwKxZs5CUlGRsv2jRIiQnJ+O9997D2bNn8dprr+HIkSNYuHAhAEAikWDx4sV466238MMPP+DkyZOYNWsWAgMDkZiYaHydnJwcpKenIycnB1qtFunp6UhPT0dlpb4U5dSpU7jnnnswceJELFmyBPn5+cjPz0dRUdHd/HyIRHU2vwJXy2qgdJRiRG9vscOxO928nBHq6wqtTsDe8/xbQkRkiwyrlo/s7QOlo0zkaOyPYV734UulHOAmm+Vg7gEzZsxAUVERli5divz8fERFRSE5Odm4EFpOTg6k0hu5/PDhw7F+/Xq8+uqr+Mtf/oLQ0FBs2bIF/fv3N7Z5+eWXUVVVhfnz56OsrAwjR45EcnIylEqlsc3SpUuxbt0649fR0dEAgN27d2Ps2LHYvHkzioqK8NVXX+Grr74ytuvevTsuXbpk7tsksgiGu9wjenmzIyCS8eF+yCysxK6MAtwXGSh2OERE1M6MO4SEs6JMDN29XNDb1xVZhZXYd74I03itJRskEbgXjlF5eTlUKhXUajXnd5NFeOBfB3Aspwxv398fj8V2Fzscu3T4UikeXp0KD2dHHPlrPBxkNrn+JFkZa75eWXPsZHuKKuow9P/thCAAvyWNh79K2fJB1O6Wb8vAJ/su4v7oIHwwI0rscIgAtO/1ir1HIgtVUlmH47llALhVmJiigz3g4eyIsup6HMspEzscIiJqR3vOFUIQgP5B7ky4RWTo5+w+VwitjvcDyfYw6SayUPsyiyAIQHiAOwJUTmKHY7ccZFKM6eMDQN85IyIi27HnnH69jnF9ObgtppjuXeCudEBZdT3SG284ENkSJt1EFmr3WX1H4J6+PiJHQvf0NYzAczE1IiJb0aDVYV+m/u/6WFaUicpBJsUoDnCTDWPSTWSBtDrB2BG4hx0B0Y3u4wOJBMjIK0e+ulbscIiIqB0cyylDRW0Dujg7IrKrh9jh2D3DAPceDnCTDWLSTWSB0nPLUFZdD3elA6KDPcQOx+55usiNHbK95zkCT7Zh+fLlGDJkCNzc3ODr64vExEScO3fujsesXbsWEonE5HHzTiNE1mR34x3VMX18IJNKRI6GDFO5Tl5Vo7CCA9xkW5h0E1kgQ2nV6D4+XC3bQhhLzM9yBJ5sw969e/HMM8/gt99+w44dO1BfX4+JEyeiqqrqjse5u7sjLy/P+Lh8+XInRUzUvnY3bhXGijLL4OOmwIAgFQBgL+92k40xe59uIup4htH3sVzYxWLcE+aDD3aex69ZxdA06CB34GAIWbfk5GSTr9euXQtfX18cPXoUo0ePbvY4iUQCf3//jg6PqEPlqWtwNr8CEgkwKpRrp1iKe/r64ORVNfacL8LDg4PFDoeo3bDXSGRhCitqcepqOYAbpVYkvv6BKni7ylFZ14Cjl6+LHQ5Ru1Or1QAAT0/PO7arrKxE9+7dERwcjOnTp+P06dOdER5RuzLcSY0K9oCni1zkaMjAsKDdvvNFaNDqRI6GqP0w6SayMIaOwMCuKvi4KUSOhgykUglGc2VVslE6nQ6LFy/GiBEj0L9//2bb9e3bF59//jm+//57fPXVV9DpdBg+fDiuXLnS7DF1dXUoLy83eRCJzVBRdg8ryixKZFcPdHF2REVtA47llIkdDlG7YdJNZGEMq3aytNzy3Ng6jEk32ZZnnnkGp06dwoYNG+7YLi4uDrNmzUJUVBTGjBmD//3vf/Dx8cEnn3zS7DHLly+HSqUyPoKDWTJK4tI06PBrZjEAJt2WRsYBbrJRTLqJLIjJnqHcn9vijA71gVQCnC+oxNWyGrHDIWoXCxcuxNatW7F792507drVrGMdHR0RHR2NrKysZtskJSVBrVYbH7m5uXcbMtFdOXKpFFUaLbxd5egX6C52OHSLGwPcXEyNbAeTbiILwj1DLZvK2RGDunUBwBF4sn6CIGDhwoX47rvvsGvXLoSEhJj9GlqtFidPnkRAQECzbRQKBdzd3U0eRGLac16fzI3p4wsptwqzOKP7+EAiATLyypGv5tZhZBuYdBNZEO4ZavkMW8tw6zCyds888wy++uorrF+/Hm5ubsjPz0d+fj5qam5UccyaNQtJSUnGr9944w388ssvuHjxIo4dO4bHH38cly9fxlNPPSXGWyBqkxtbhbGizBJ5usiNNx72nucAN9kGJt1EFoR7hlo+Q9n/gaxi1DVoRY6GqO0+/vhjqNVqjB07FgEBAcbHxo0bjW1ycnKQl5dn/Pr69euYN28ewsPDMWXKFJSXl+PgwYOIiIgQ4y0QmS23tBqZhZWQSSUY1ZtJt6UylphzgJtsBPfpJrIQ+epa7hlqBSIC3OHrpkBhRR0OZZfy/4qsliAILbbZs2ePydcffPABPvjggw6KiKjj7W0sLR/UzQMqZ0eRo6HmjO3rgw92nsevWcWo1+rgKON9QrJu/AQTWYh9jR2ByK7cM9SSSSQS4/7phv8zIiKyDoakmzuEWLYBQSp4ushRWdeAY5evix0O0V1j0k1kIfYaF3bhnVNLN6axxHwvk24iIquhadDhYJZ+qzBeay2bVCrB6FBvALzWkm1g0k1kARq0Ouxv3CpsDLcKs3gje3sbtw67xq3DiIiswtHL11Gl0cLLRY6IAK6ib+k4wE22hEk3kQU4cUWN8toGqJy4VZg18HCWIyrYAwBLzImIrMW+xsHt0X18uFWYFTCsmXL6WjkKK7h1GFk3Jt1EFsAwijsq1JtbhVmJMX308wE5Ak9EZB32nuM0Lmvi7arAgCAVAGD/+WKRoyG6O0y6iSwA53NbH0PZ26+Z+pVViYjIchWW1+JMXnnjDiHeYodDrWToF3GAm6wdk24ikZVWafD7lTIA+pI3sg4DglTwcHZERV0D0nPLxA6HiIjuYF+m/k7pgCAVvFwVIkdDrWXoF+3PLIJW1/I2h0SWikk3kcj2ZxZBEIAwfzf4uSvFDodaSSaVGOebGUoWiYjIMrGizDpFd/OAm8IB16vrcfKqWuxwiNqMSTeRyIwdAa5abnVY9kZEZPm0OuHGDiFMuq2Ko0yKEb0btw7jADdZMSbdRCLS6QTsO889Q62VYQ/Rk1fVKK6sEzkaIiJqyu9XylBWXQ83pYNx5wmyHje2DisUORKitmPSTSSiM3nlKK6sg7NchsHdPcUOh8zk66407vVquItCRESWxVCNNLK3Nxxk7PpaG8O87vTcMqir60WOhqht+JeHSESGjsDwXt6QO/DX0RoZRuD3cTsTIiKLxPnc1i3Iwwmhvq7QCcCvWbzWknViL59IRJzPbf0Mnbh954ug48qqREQWpaxagxONO0xwhxDrdWMNFZaYk3Vi0k0kkoraehy7fB0AMCaUHQFrFdO9C1wVDiip0uD0tXKxwyEiopv8mlUMnQCE+roi0MNJ7HCojW7M6y6CIHCAm6wPk24ikaReKEGDTkCItwu6eTmLHQ61kaNMirheXgCAfZzXTURkUfaxtNwmDOnhCaWjFAXldThfUCl2OERmY9JNJBJDgjaqcQVssl6juXUYEZHFEYQbO4SwtNy6KR1liA1pHODmtZasEJNuIpEYOwIsLbd6hq3Djl2+joparqxKRGQJMgsrkV9eC4WDFENDuEOItTPcpGBVGVkjJt1EIrhUXIWc0mo4yiTG0mSyXt29XNDdyxkNOgG/XSwVOxwiIsKNO6JDQzyhdJSJHA3dLcMUgbTsUtTWa0WOhsg8TLqJRGAYpY3p3gUuCgeRo6H2YKhYYNkbEZFl2JepryjjfG7b0NvXFQEqJTQNOqRlc4CbrAuTbiIRGErLR7G03GYY5guy7I2ISHy19VqkXSwBwPnctkIikXCAm6wWk26iTqZp0CH1AkffbU1cLy84SCW4XFKNyyVVYodDRGTXDmWXoq5BB393JUJ9XcUOh9qJcYCbSTdZGSbdRJ3sWM51VGm08HKRIyLAXexwqJ24KhwQ070LAHYGiIjEZvg7PLqPNyQSicjRUHsZ0dsLUol+kbxrZTVih0PUaky6iTqZoSMwKtQbUik7ArbkRol5sciREBHZt/2ZnMZlizyc5RjY1QMA8CuvtWRFmHQTdTLDnF/OMbM9hrlmqRdKUK/ViRwNEZF9ylfX4lxBBSQSYGRvb7HDoXZm6D/t5RoqZEXalHSvWrUKPXr0gFKpRGxsLA4dOnTH9ps2bUJYWBiUSiUGDBiAbdu2mXxfEAQsXboUAQEBcHJyQnx8PDIzM03avP322xg+fDicnZ3h4eHR5HlycnIwdepUODs7w9fXFy+99BIaGhra8haJOkRxZR1OXS0HAIwMZUfA1vQLdIeXixyVdQ04dvm62OEQEdklw+D2wK4e6OIiFzkaam9j+uj7T79mFkOrE0SOhqh1zE66N27ciCVLlmDZsmU4duwYIiMjkZCQgMLCwibbHzx4EDNnzsTcuXNx/PhxJCYmIjExEadOnTK2eeedd/DRRx9h9erVSEtLg4uLCxISElBbW2tso9Fo8PDDD2PBggVNnker1WLq1KnQaDQ4ePAg1q1bh7Vr12Lp0qXmvkWiDmMohQoPcIevm1LkaKi9SaUS42AKVzEnIhKHYRrXGA5u26TIrh5wUzpAXVOP36+UiR0OUauYnXS///77mDdvHubMmYOIiAisXr0azs7O+Pzzz5ts/+GHH2LSpEl46aWXEB4ejjfffBODBg3CypUrAejvcq9YsQKvvvoqpk+fjoEDB+KLL77AtWvXsGXLFuPrvP7663j++ecxYMCAJs/zyy+/4MyZM/jqq68QFRWFyZMn480338SqVaug0WjMfZtEHeLmhV3INt3YzoRzzYiIOptWJ+DXLP3fX07jsk0OMqlx2gCvtWQtzEq6NRoNjh49ivj4+BsvIJUiPj4eqampTR6Tmppq0h4AEhISjO2zs7ORn59v0kalUiE2NrbZ12zuPAMGDICfn5/JecrLy3H69Okmj6mrq0N5ebnJg6ijCIKA/YaOABd2sVmjGgdUTl1To6SyTuRoiIjsy6mrapRV18NN4YCoYA+xw6EOcmPhUlaVkXUwK+kuLi6GVqs1SWwBwM/PD/n5+U0ek5+ff8f2hn/NeU1zznPzOW61fPlyqFQq4yM4OLjV5yMy19n8ChRV1EHpKMXgHl3EDoc6iK+bEmH+bhAE4MCFErHDISKyK/sbk7Dhvb3gION6wbbKcKc7PbcM5bX1IkdD1DK7/muUlJQEtVptfOTm5oodEtkwQ2n5sJ5eUDjIRI6GOpJxBJ77dRMRdSpDuTG3CrNtwZ7O6OntAq1OwMEsDnCT5TMr6fb29oZMJkNBQYHJ8wUFBfD392/yGH9//zu2N/xrzmuac56bz3ErhUIBd3d3kwdRRzHsGcrScttn+D/en1kEQeDKqkREnaGith7HcvQ7R4zhfG6bZxjg3s8Sc7ICZiXdcrkcMTExSElJMT6n0+mQkpKCuLi4Jo+Ji4szaQ8AO3bsMLYPCQmBv7+/SZvy8nKkpaU1+5rNnefkyZMmq6jv2LED7u7uiIiIaPXrEHWEGo0Why6VAuAiavZgcI8uUDhIUVBeh8zCSrHDISKyC79dLEWDTkAPL2cEezqLHQ51sFGNq9MbbmoQWTKzy8uXLFmCTz/9FOvWrUNGRgYWLFiAqqoqzJkzBwAwa9YsJCUlGdsvWrQIycnJeO+993D27Fm89tprOHLkCBYuXAgAkEgkWLx4Md566y388MMPOHnyJGbNmoXAwEAkJiYaXycnJwfp6enIycmBVqtFeno60tPTUVmp79BOnDgRERER+OMf/4gTJ05g+/btePXVV/HMM89AoVDczc+I6K4dulQKTYMOASolevm4ih0OdTClowyxPb0AsMSciKizGO54srTcPgzr6QVHmQQ5pdW4XFIldjhEd+Rg7gEzZsxAUVERli5divz8fERFRSE5Odm4aFlOTg6k0hu5/PDhw7F+/Xq8+uqr+Mtf/oLQ0FBs2bIF/fv3N7Z5+eWXUVVVhfnz56OsrAwjR45EcnIylMob+xgvXboU69atM34dHR0NANi9ezfGjh0LmUyGrVu3YsGCBYiLi4OLiwtmz56NN954w/yfClE7M24VFuoDiUQicjTUGUaHemPf+SLsyyzGU6N6ih0OEZHNu7EtJ5Nue+CicEBM9y747WIp9p0vwh/jXMQOiahZEoETDo3Ky8uhUqmgVqs5v5va1cQP9uJ8QSVW/iEa9w4MFDsc6gTnCyow8YN9UDhIcWLZRCgduXgetR9rvl5Zc+xkuXJKqjH6H7vhIJXg+NIJcFM6ih0SdYJVu7Pwj+3nMCHCD5/OGix2OGRj2vN6ZderlxN1hnx1Lc4XVEIiAUb04nxuexHq6wo/dwXqGnQ4cum62OEQEdm0/Vn6u9yDunVhwm1HDAuXpl4oQb1WJ3I0RM1j0k3UwfY1zjEbGKRCFxe5yNFQZ5FIJMZ5hfu4sioRUYcylJYbFtci+9Av0B2eLnJU1jXgeE6Z2OEQNYtJN1EHM24Vxjlmdof7dRMRdbwGrc64VzOvtfZFKpVgZG/DKua81pLlYtJN1IF0OgG/cjVVuzWytzckEuBsfgUKy2vFDoeIyCaduFKGiroGeDg7on+QSuxwqJMZqhv2ceswsmBMuok60Olr5bheXQ8XuQzR3TzEDoc6maeLHP0D9R3AX7PYGSAi6gj7zuv/vo7o7Q2ZlDuE2BvDTY3fr5ShrFojcjRETWPSTdSBDHN543p5w1HGXzd7ZByBZ4k5EVGHMFxrR3M+t13yVynRx88VgsABbrJczAKIOpAh0RrThx0Be2WYX/hrVjF0Ou7QSETUntTV9TiRWwaA07jsmWEVcw5wk6Vi0k3UQSrrGnAsR79V1Eh2BOzWoG5d4CyXobhSg4z8crHDISKyKQcvFEMnAL18XBDo4SR2OCSSUYYB7sxiCAIHuMnyMOkm6iC/XShBvVZAsKcTeng5ix0OiUTuIEVcTy8AN+YdEhFR+zCWlnPVcrs2tIcn5A5SXFPX4kJRpdjhEN2GSTdRB9lvnGPmA4mEC7vYM0NnkNuZEBG1H0EQjIOZTLrtm5NchtgQTwAc4CbLxKSbqIMY9ufmHDMyLKZ25NJ1VGsaRI6GiMg2ZBdX4WpZDeQyqTHhIvtluNZygJssEZNuog6QW1qNi8VVkEklGN7bS+xwSGQh3i4I8nCCRqtDWnap2OEQAQCWL1+OIUOGwM3NDb6+vkhMTMS5c+daPG7Tpk0ICwuDUqnEgAEDsG3btk6Iluh2hsHtwT26wFnuIHI0JDbDTY7fLpairkErcjREpph0E3UAQ0cgOtgD7kpHkaMhsUkkEozuw63DyLLs3bsXzzzzDH777Tfs2LED9fX1mDhxIqqqqpo95uDBg5g5cybmzp2L48ePIzExEYmJiTh16lQnRk6kZ/h7yooyAoAwfzf4uClQU6/F0UvXxQ6HyASTbqIOYChtYkeADAzbmRgGZIjElpycjCeeeAL9+vVDZGQk1q5di5ycHBw9erTZYz788ENMmjQJL730EsLDw/Hmm29i0KBBWLlyZSdGTgRoGnRIvVgCAMZBTbJvEonEWGK+j9dasjBMuonaWYNWhwNZjfO52RGgRsN7eUMqAbIKK3GtrEbscIhuo1arAQCens3PjU1NTUV8fLzJcwkJCUhNTe3Q2IhudSznOqo1Wni7yhHu7y52OGQhbgxws6qMLAuTbqJ29vtVNcprG+CudEBkVw+xwyELoXJ2RGSwBwD9PqJElkSn02Hx4sUYMWIE+vfv32y7/Px8+Pn5mTzn5+eH/Pz8Zo+pq6tDeXm5yYPobhmSqpG9vSGVcocQ0hvRW3+z4/S1chRX1okcDdENTLqJ2plhjtnIUG/I2BGgmximG+zlCDxZmGeeeQanTp3Chg0b2v21ly9fDpVKZXwEBwe3+znI/hi2heI0LrqZj5sCEQH6ygcOcJMlYdJN1M64VRg1Z0zjdIMDWcXQ6gSRoyHSW7hwIbZu3Yrdu3eja9eud2zr7++PgoICk+cKCgrg7+/f7DFJSUlQq9XGR25ubrvETfarpLIOp67pp0MY5vASGRj2bN/HAW6yIEy6idqRuqYe6bllANgRoNtFdvWAm8IBZdX1OHVVLXY4ZOcEQcDChQvx3XffYdeuXQgJCWnxmLi4OKSkpJg8t2PHDsTFxTV7jEKhgLu7u8mD6G4cuFACQdCvVu3rrhQ7HLIwo437dRdDEDjATZaBSTdRO0q9UAKtTkBPHxd07eIsdjhkYRxkUuO+7VzkhcT2zDPP4KuvvsL69evh5uaG/Px85Ofno6bmxkJ/s2bNQlJSkvHrRYsWITk5Ge+99x7Onj2L1157DUeOHMHChQvFeAtkp/Y3TuMy3NEkullMjy5QOkpRVFGHcwUVYodDBIBJN1G7MpQyjWZpOTXDMO3AMB+RSCwff/wx1Go1xo4di4CAAONj48aNxjY5OTnIy8szfj18+HCsX78ea9asQWRkJDZv3owtW7bccfE1ovYkCMJN07hYUUa3UzjIMKynfoDbsM4OkdgcxA6AyFYIgmD8486OADVnTOOdmWM511FRWw83paPIEZG9ak3Z5Z49e2577uGHH8bDDz/cARERtex8QSXyy2uhcJBiSI/mt7cj+zY61Ad7zhVh3/lizB/dS+xwiHinm6i9XCqpxpXrNXCUSYwjrES3CvZ0Rg8vZzToBKReKBE7HCIiq2KYmhPb0wtKR5nI0ZClGt24cOmhS6Wo0WhFjoaISTdRuzHc5R7c3RMuChaRUPO4sioRUdvsNcznZkUZ3UEvH1cEqpTQNOiQls0BbhIfk26idrKPC7tQK43mvG4iIrPV1mtxKLsUwI2pOkRNkUgkNwa4ea0lC8Ckm6gdaBp0SL2oH0k1lDQRNSeulxccZRLklFbjUnGV2OEQEVmFtOxS1DXoEKBSorevq9jhkIVjVRlZEibdRO3gyOVSVGu08HaVI9yfe9DSnbkoHDCoWxcA7AwQEbXWzYuVSiQSkaMhSzeilzekEiCrsBLXympaPoCoAzHpJmoHN7Yv8YFUyo4AtYxlb0RE5jEsosZpXNQaKmdHRAZ7ALjx2SESC5NuonZwYz43S8updQzzEVMvFEPToBM5GiIiy5anrsH5gkpIJcDI3rzWUutwDRWyFEy6ie5SUUUdTl8rBwCM7M3Rd2qdiAB3eLnIUaXR4ljOdbHDISKyaPsbk6aBXT3g4SwXORqyFoaqiF+ziqHVCSJHQ/aMSTfRXfo1S3+XOyLAHT5uCpGjIWshlUowsnHLG0OlBBERNW1vJrcKI/NFdlXBTekAdU09TlwpEzscsmNMuonukmH0nXPMyFyGsjfDmgBERHQ7rU7AgSxea8l8DjKpcTrCfpaYk4iYdBPdBZ1OwL5MQ0eAo+9knlGNn5lT19QoqawTORoiIst08qoaZdX1cFM6IKpxYSyi1uLWYWQJmHQT3YWM/HIUV9bBWS7D4O6eYodDVsbXTYnwAHcIgn6+GRER3c4wBWdEL284yNh1JfMYku703DKoa+pFjobsFf9yEd0Fw2qYcT29IHfgrxOZz1AhsZfzuomImnRjhxCWlpP5gjyc0MvHBVqdgIMc4CaRMEsguguGjsAoLuxCbXTzdiY6rqxKRGRCXVOP47llAHitpbYb1Xit5QA3iYVJN1EbVdU14MjlUgDAmL6+IkdD1mpwjy5wcpShuLIOGfnlYodDRGRRDjZu9dTTxwXBns5ih0NWakxfwwB3EQSBA9zU+Zh0E7VR6oUS1GsFdPN0Rg8vdgSobRQOMgzv5QXgxnQFIiLSMyx+NYal5XQXhoXopwFeU9fiQlGl2OGQHWLSTdRGhhKlMX18IJFIRI6GrJlhBH7v+UKRIyEishyCIGDvOSbddPec5DLEhugXvN1zjiXm1PnalHSvWrUKPXr0gFKpRGxsLA4dOnTH9ps2bUJYWBiUSiUGDBiAbdu2mXxfEAQsXboUAQEBcHJyQnx8PDIzM03alJaW4rHHHoO7uzs8PDwwd+5cVFaajlRt374dw4YNg5ubG3x8fPDggw/i0qVLbXmLRC3ay4VdqJ0Y5nUfuXQdlXUNIkdDRGQZsgorcU1dC4WDFMN6eokdDlk5w8AN53WTGMxOujdu3IglS5Zg2bJlOHbsGCIjI5GQkIDCwqbv0Bw8eBAzZ87E3Llzcfz4cSQmJiIxMRGnTp0ytnnnnXfw0UcfYfXq1UhLS4OLiwsSEhJQW1trbPPYY4/h9OnT2LFjB7Zu3Yp9+/Zh/vz5xu9nZ2dj+vTpGDduHNLT07F9+3YUFxfjgQceMPctErXoUnEVckqr4SiTIK4XOwJ0d3p4u6C7lzMauLIqEZGRITmK7ekFpaNM5GjI2o1trCpLyy5FjUYrcjRkb8xOut9//33MmzcPc+bMQUREBFavXg1nZ2d8/vnnTbb/8MMPMWnSJLz00ksIDw/Hm2++iUGDBmHlypUA9He5V6xYgVdffRXTp0/HwIED8cUXX+DatWvYsmULACAjIwPJycn47LPPEBsbi5EjR+Kf//wnNmzYgGvXrgEAjh49Cq1Wi7feegu9evXCoEGD8OKLLyI9PR319dyTj9qXoSMwuLsnXBUOIkdDtsAwAm+Yv0hEZO+MFWVctZzaQS8fVwSqlNA06JCWXSJ2OGRnzEq6NRoNjh49ivj4+BsvIJUiPj4eqampTR6Tmppq0h4AEhISjO2zs7ORn59v0kalUiE2NtbYJjU1FR4eHhg8eLCxTXx8PKRSKdLS0gAAMTExkEql+M9//gOtVgu1Wo0vv/wS8fHxcHR0bDK2uro6lJeXmzyIWsM4n7svS8upfRiS7j3nuLIqEVGNRou0bP0OIWN5raV2IJFIblpDhQPc1LnMSrqLi4uh1Wrh5+dn8ryfnx/y8/ObPCY/P/+O7Q3/ttTG19d0SyYHBwd4enoa24SEhOCXX37BX/7yFygUCnh4eODKlSv45ptvmn0/y5cvh0qlMj6Cg4Nb+hEQoa5Bi9QL+hFSw1xcors1rKcXHGUSXLleg+ziKrHDISIS1W/ZJdA06BDk4YRePq5ih0M2gvO6SSw2s3p5fn4+5s2bh9mzZ+Pw4cPYu3cv5HI5HnrooWbvGiUlJUGtVhsfubm5nRw1WaMjl66jpl4LHzcFwgPcxA6HbISLwgFDeuhXVmVngIjsnWHV8tHcIYTa0fDe3pBJJbhYVIXc0mqxwyE7YlbS7e3tDZlMhoKCApPnCwoK4O/v3+Qx/v7+d2xv+LelNrcu1NbQ0IDS0lJjm1WrVkGlUuGdd95BdHQ0Ro8eja+++gopKSnGEvRbKRQKuLu7mzyIWsKtwqijcASeiEhv33luFUbtz13piJhuXQDwWkudy6ykWy6XIyYmBikpKcbndDodUlJSEBcX1+QxcXFxJu0BYMeOHcb2ISEh8Pf3N2lTXl6OtLQ0Y5u4uDiUlZXh6NGjxja7du2CTqdDbGwsAKC6uhpSqenbkclkxhiJ2gv3DKWOYphr9tvFEtTWc2VVIrJPuaXVuFhcBZlUguG9uUMIta/RffQL8+1j0k2dyOzy8iVLluDTTz/FunXrkJGRgQULFqCqqgpz5swBAMyaNQtJSUnG9osWLUJycjLee+89nD17Fq+99hqOHDmChQsXAtAvarB48WK89dZb+OGHH3Dy5EnMmjULgYGBSExMBACEh4dj0qRJmDdvHg4dOoQDBw5g4cKFePTRRxEYGAgAmDp1Kg4fPow33ngDmZmZOHbsGObMmYPu3bsjOjr6bn9ORACAfHUtzhVUQCIBRvbmaqrUvvr6ucHPXYHaeh0OXyoVOxwiIlEY7kDGdOsCd2XTi+EStdWYPvp1og5e0K8bQNQZzE66Z8yYgXfffRdLly5FVFQU0tPTkZycbFwILScnB3l5ecb2w4cPx/r167FmzRpERkZi8+bN2LJlC/r3729s8/LLL+PZZ5/F/PnzMWTIEFRWViI5ORlKpdLY5uuvv0ZYWBjGjx+PKVOmYOTIkVizZo3x++PGjcP69euxZcsWREdHY9KkSVAoFEhOToaTk1ObfjhEtzKMikZ29UAXF7nI0ZCtkUgkxsX5DBUVRET2hjuEUEfqF+gOLxc5KusacCznutjhkJ2QCNybxqi8vBwqlQpqtZrzu6lJf/76KLadzMdz40OxZEIfscMhG7T192tYuP44evu6YueSMWKHQxbKmq9X1hw7dTxNgw7Rb/yCKo0WPy4ciQFdVWKHRDbo+Y3p+O74VfxpTC+8MjlM7HDIQrXn9cpmVi8n6mgNWh32ZxYDAO7h6Dt1kFG9fSCVAFmFlbhynSurEpF9OXKpFFUaLbxd5egXyEEZ6hiGdXn2nCtsoSVR+2DSTdRKx3LKUFHbgC7OjhjY1UPscMhGqZwdMahxZdU9LDEnIjuzx7hquS+kUu4QQh1DvxUdcDa/AvnqWrHDITvApJuolQyjoaP7+EDGjgB1oLF9DSPwTLqJyL4YrrVjWVFGHcjTRY7Ixhsoe8/zbjd1PCbdRK20uzEBYkeAOtrYvoaVVYtR18Ctw4jIPlwtq8H5gkpIJcCoUO4QQh3L0J/bfZYD3NTxmHQTtUJBeS0y8sohkcC4ujRRR4kIcIePmwLVGi2OXOLKqkRkHwx3uaO7dYGHM3cIoY5lGOA+kFWMei23DqOOxaSbqBUM2zcN7OoBL1eFyNGQrZNKJcZFXnafZdkbEdkHw5SasX04uE0db2CQCp4uclTUNeDoZQ5wU8di0k3UCnsa5/uwI0CdxTiv+zzL3ojI9tU1aHEwq3GHkDBfkaMhe3DzADfXUKGOxqSbqAX1Wh32n9d3BDifmzoLtw4jInty5NL1xq3CFIgI4FZh1DluLFzKqjLqWEy6iVpw7PJ1VNQ1wNNFzq3CqNOonB0R051bhxGRfTAkPWP6+HCrMOo0o0JvbB2Wp64ROxyyYUy6iVpgKO8dHerNrcKoUxkWeWHSTUS2bg93CCERmGwdxmstdSAm3UQtuNER4Bwz6lyGuWbcOoyIbNmV69XILORWYSSOGyXmTLqp4zDpJrqDfPVNW4VxETXqZP0Cb2wddjibK6sSkW0yJDvcKozEYLip8mtWMTQN3DqMOgaTbqI72Nu4anlkVw94urAjQJ1LIpEYV8znIi9EZKsMSfc9LC0nEQwMUsHLRY5Kbh1GHYhJN9Ed7GrcI5lzzEgshhH4XUy6icgG1dZrcSDLsEMIp3FR55NKJcZqxt281lIHYdJN1Iy6Bi1+zdR3BMZxz1ASyag+3nCQSnCxqAqXS6rEDoeIqF2lZZeipl4LXzcF+gVyqzASh2FveMPNFqL2xqSbqBmHs2/sGdo/UCV2OGSn3JWOGNxDv3UYOwNEZGt2N/5du6evLyQS7hBC4hgT6gOZVIKswkrkllaLHQ7ZICbdRM3YZewIcM9QEtf4MD8ATLqJyLYIgmD8uzYunBVlJB6VsyNiunOAmzoOk26iZhjm9YxnR4BEZih7S7tYiqq6BpGjISJqHxeKqpBTWg25TIqRvblVGIlrHEvMqQMx6SZqwsWiSmQXV8FRJsHIUC6iRuLq5eOCbp7O0Gh1+LVxwSGi9rBv3z5MmzYNgYGBkEgk2LJlyx3b79mzBxKJ5LZHfn5+5wRMNsVQWh7b0xMuCgeRoyF7Z0i6Uy+WoFrDAW5qX0y6iZpgGOUcGuIJV3YESGQSicTYGdjNEXhqR1VVVYiMjMSqVavMOu7cuXPIy8szPnx9WRFE5ks5WwBAP5+bSGyhvq4I8nCCpkGHA1klYodDNobZBFETDKXl7AiQpRgX5ou1By9h97lCCILABYeoXUyePBmTJ082+zhfX194eHi0f0BkN8pr63Hkkn5PZE7jIksgkUgwPtwXX6Rexq6zhZgQ4Sd2SGRDeKeb6BaVdQ04lF0KgFuFkeWI7ekJZ7kMBeV1OH2tXOxwyM5FRUUhICAAEyZMwIEDB8QOh6zQ/vPFaNAJ6Onjgu5eLmKHQwTgxhoqexoHuInaC5Nuolv8mlmEeq2AHl7O6OnjKnY4RAAAhYMMIxoXGuIiLySWgIAArF69Gt9++y2+/fZbBAcHY+zYsTh27Fizx9TV1aG8vNzkQWRctZwVZWRB4np6QekoRZ66Fhl5FWKHQzaESTfRLYwdgTCWFZFl4cqqJLa+ffvi6aefRkxMDIYPH47PP/8cw4cPxwcffNDsMcuXL4dKpTI+goODOzFiskQ6nYC95w3XWibdZDmUjjKM6KUf4DZMNSRqD0y6iW6i0wnYfa4IADsCZHkMawycuFKGkso6kaMh0hs6dCiysrKa/X5SUhLUarXxkZub24nRkSX6/aoaxZUauCkcMLiHp9jhEJkw7BnPAW5qT0y6iW5y+lo5iirq4CKXYWgIOwJkWfxVSvQLdIcgAHsaB4eIxJaeno6AgIBmv69QKODu7m7yIPtmSGZG9fGG3IFdUbIshgHu4znXUVqlETkashX8S0d0E8P2JSND2REgy2SowDB8VonuRmVlJdLT05Geng4AyM7ORnp6OnJycgDo71LPmjXL2H7FihX4/vvvkZWVhVOnTmHx4sXYtWsXnnnmGTHCJyu1i1uFkQUL9HBCmL8bdIJ+QTWi9sCsgugmOzP0HYHx4ZzPTZbJ8Nncd74YmgadyNGQtTty5Aiio6MRHR0NAFiyZAmio6OxdOlSAEBeXp4xAQcAjUaDF154AQMGDMCYMWNw4sQJ7Ny5E+PHjxclfrI+eeoanLpaDonkxkrRRJYmvvFam5LBpJvaB/fpJmp0c0eA87nJUg0MUsHHTYGiijqkZZdgVKiP2CGRFRs7duwdt8VZu3atydcvv/wyXn755Q6OimyZIYkZ1K0LvF0VIkdD1LT4CD+s3J2FveeLoGnQsfqR7ho/QUSN2BEgayCVShDfuMjLzjMsMSci62KoKItnRRlZMMMAd2VdA9KyS8QOh2wAk26iRjdKy3mXmyzb+Mbt7HZmFN7xLiURkSWpqmvAwSx9AhPPay1ZMKlUgvFhHOCm9sOkmwimHYEJHH0nCzeitzeUjlJcLavB2fwKscMhImqV/ZnF0Gh16O7ljN6+rmKHQ3RHhmoMDnBTe2DSTQRgf2YROwJkNZzkMozsrZ/LzRF4IrIWN5eWSyQSkaMhujMOcFN7YtJNBP0oJsCOAFmPCRGNZW8ZTLqJyPJpdYJxf27O5yZrwAFuak9Musnu3dwR4HxushaGrXZOXFGjsLxW5GiIiO4sPfc6Sqs0cFc6YHCPLmKHQ9QqxoVLz3LrMLo7TLrJ7h3PudERGNLDU+xwiFrF102JqGAPAEAKOwNEZOF2nNH/nbonzBeOMnY/yTqMa0y6T+SWcYCb7gr/6pHdM5SWsyNA1mZCROMiLyx7IyILx63CyBpxgJvaCzMMsns3tgpjR4Csi2E6xK9ZxajRaEWOhoioaZeKq5BVWAkHqQRj+vqIHQ6RWQwl5ilcQ4XuApNusmvZN3cE+rAjQNalr58bunZxQl2DDr9mFYsdDhFRkwyD27E9PeGudBQ5GiLzxDdWle3P5AA3tV2bku5Vq1ahR48eUCqViI2NxaFDh+7YftOmTQgLC4NSqcSAAQOwbds2k+8LgoClS5ciICAATk5OiI+PR2Zmpkmb0tJSPPbYY3B3d4eHhwfmzp2LysrK217n3XffRZ8+faBQKBAUFIS33367LW+R7IShLHdoiCdUTuwIkHWRSCTGUs0dZ/JFjoaIqGk7Gq+148NYUUbW5+YB7v2ZRWKHQ1bK7KR748aNWLJkCZYtW4Zjx44hMjISCQkJKCxsep7DwYMHMXPmTMydOxfHjx9HYmIiEhMTcerUKWObd955Bx999BFWr16NtLQ0uLi4ICEhAbW1NxYseOyxx3D69Gns2LEDW7duxb59+zB//nyTcy1atAifffYZ3n33XZw9exY//PADhg4dau5bJDuy/bQ+UZkYwY4AWSfDZ3dnRiEatDqRoyEiMlVSWYfDl0oB3FiHgsiaSCQS42d3+2mWmFPbSARBEMw5IDY2FkOGDMHKlSsBADqdDsHBwXj22Wfxyiuv3NZ+xowZqKqqwtatW43PDRs2DFFRUVi9ejUEQUBgYCBeeOEFvPjiiwAAtVoNPz8/rF27Fo8++igyMjIQERGBw4cPY/DgwQCA5ORkTJkyBVeuXEFgYCAyMjIwcOBAnDp1Cn379m3TD6O8vBwqlQpqtRru7u5teg2yHkUVdRj6/3ZCEICDr4xDoIeT2CERma1Bq8Pgt3eirLoeG+cPQ2xPL7FDok5gzdcra46dzPfN4Vy8/O3v6Bfojp+eGyV2OERt8tvFEjy65jd4ODviyF/j4cCFd+1Ce16vzPrEaDQaHD16FPHx8TdeQCpFfHw8UlNTmzwmNTXVpD0AJCQkGNtnZ2cjPz/fpI1KpUJsbKyxTWpqKjw8PIwJNwDEx8dDKpUiLS0NAPDjjz+iZ8+e2Lp1K0JCQtCjRw889dRTKC0tbfb91NXVoby83ORB9mNnRgEEARjYVcWEm6yWg0xqLNnkCDwRWZpfGqe+JPTzFzkSorYb3L0LPF3kKKuux6FLzecWRM0xK+kuLi6GVquFn59peZCfnx/y85ueT5ifn3/H9oZ/W2rj6+tr8n0HBwd4enoa21y8eBGXL1/Gpk2b8MUXX2Dt2rU4evQoHnrooWbfz/Lly6FSqYyP4ODgln4EZENYWk62YmI/Q9KdDzOLl4iIOkxVXQP2ZeoXeTT8nSKyRvoBbn0u8gsHuKkNbKY2QqfToa6uDl988QVGjRqFsWPH4t///jd2796Nc+fONXlMUlIS1Gq18ZGbm9vJUZNYKmrrcTCrBABH38n6jQ71gdJRiqtlNTiTx4odIrIMe88XQdOgQ3cvZ/T1cxM7HKK7Yugv/sIBbmoDs5Jub29vyGQyFBSYjvAUFBTA37/pxMXf3/+O7Q3/ttTm1oXaGhoaUFpaamwTEBAABwcH9OnTx9gmPDwcAJCTk9NkbAqFAu7u7iYPsg97zhVBo9Whp7cLevu6ih0O0V1xksuMW96xxJyILIWhoiyhnz8kEonI0RDdnZGh3nCWy3BNXYuTV9Vih0NWxqykWy6XIyYmBikpKcbndDodUlJSEBcX1+QxcXFxJu0BYMeOHcb2ISEh8Pf3N2lTXl6OtLQ0Y5u4uDiUlZXh6NGjxja7du2CTqdDbGwsAGDEiBFoaGjAhQsXjG3Onz8PAOjevbs5b5PsgLG0nB0BshE3j8ATEYlN06DDrrP6GyYJLC0nG6B0lGFsX/0AN0vMyVxml5cvWbIEn376KdatW4eMjAwsWLAAVVVVmDNnDgBg1qxZSEpKMrZftGgRkpOT8d577+Hs2bN47bXXcOTIESxcuBCAfhn+xYsX46233sIPP/yAkydPYtasWQgMDERiYiIA/R3rSZMmYd68eTh06BAOHDiAhQsX4tFHH0VgYCAA/cJqgwYNwpNPPonjx4/j6NGjePrppzFhwgSTu99EdQ1a7Dmn32eRc8zIVowL84VMKsHZ/ApcLqkSOxwisnO/XSxBRW0DvF0ViA7uInY4RO1iYoR+gHs7B7jJTGYn3TNmzMC7776LpUuXIioqCunp6UhOTjYuhJaTk4O8vDxj++HDh2P9+vVYs2YNIiMjsXnzZmzZsgX9+/c3tnn55Zfx7LPPYv78+RgyZAgqKyuRnJwMpVJpbPP1118jLCwM48ePx5QpUzBy5EisWbPmxhuRSvHjjz/C29sbo0ePxtSpUxEeHo4NGza06QdDtuvghRJU1jXA102BqK4eYodD1C48nOUY1tMTAEfgiUh8hqRkQoQfpFJWlJFtuCfMFw5SCTILK3GxqFLscMiKmL1Pty3j3qH2Iel/v+O/h3Lx+LBueCtxgNjhELWbL1IvYen3pzG4exdsXjBc7HCoA1nz9cqaY6fW0ekEDFuegsKKOqydMwRj+/q2fBCRlfjjv9OwP7MY/9+kMCwY20vscKgDibZPN5G10+oE7DijvwtoKBEishUTGre/O5pzHUUVdSJHQ0T26nhuGQor6uCqcEBcLy+xwyFqVxP7scSczMekm+zKsZzrKK7UwE3pgGE92REg2xKgckJkVxUEAcbBJSKizmZY0PGeMF8oHGQiR0PUviY2DnCn55YhX10rcjRkLZh0k135+aS+IzA+zBdyB378yfYk9NePwP98Kq+FlkRE7U8QBPx8yrBVGBcrJdvj567EoG4eAHi3m1qPWQfZDZ1OMCYiUwYEiBwNUceY3F//2T54oQTXqzQiR0NE9ub0tXLklFZD6SjFPZzLTTbK0I/cdpID3NQ6TLrJbqRfKUOeuhYuchlG9/EROxyiDhHi7YLwAHeT9QuIiDqLIQkZ28cXLgoHkaMh6hiTGqvKDl0q5Roq1CpMuslu/NzYERgf7gelI+eYke2aOkDfGdjGEnMi6kSCIBiT7ikDWVFGtqtrF2dEBntAEIBklphTKzDpJrug7wjo/yhOGcBVy8m2TW4sezuQVQx1db3I0RCRvTibX4FLJdWQO0gxLoyl5WTbphjWUGGJObUCk26yC79fUeNqWQ2c5TLuF0o2r5ePK/r6uaFeK2BHBkvMiahzGO5yj+njA1eWlpONM8zr/u1iCUoqWWJOd8akm+yCocz2njBflpaTXTB0BjgCT0SdQRAE/NT492YqFyslOxDs6YwBQSroBOAXrqFCLWDSTTZPEATjVmFT+rMjQPbBMI1if2YxymtZYk5EHSuzsBIXi6ogl0kxLpwVZWQfJhvWUOEAN7WASTfZPJPtS8K4ajnZh1A/N/T2dYVGq0MKS8yJqIP99Ls+6RgV6g13paPI0RB1Dm7TSa3FpJtsnmH08Z6+vnCWc44Z2Q/DIi+GRQSJiDrKz43TuCaztJzsCLfppNZi0k02zWT7EnYEyM4YtuzZe74IlXUNIkdDRLYqq7AC5wsq4SiTYEK4n9jhEHUqbtNJrcGkm2xaRp5++xKFgxT3cPsSsjN9/dzQ09sFmgaWmBNRxzFU04zo7Q2VM0vLyb4Yqjt+zSxGWTVLzKlpTLrJpv34+zUAwNi+3L6E7I9EIsHUxrvdP57gCDwRtT9BEPDDCf21lhVlZI96+bgizN8NDToB209zOhc1jUk32SxBEPBjY0fgvsggkaMhEse0yEAAwN7zhVBXcxVzImpfZ/MrkFVYCblMioR+/mKHQyQKw7XWMABFdCsm3WSzjuWU4cr1GrjIZRjH0nKyU3383BDm74Z6rYDk07zbTUTtyzC4PbavD1ROLC0n+3RfY9KdeqEEhRW1IkdDlohJN9ksQ0dgQoQfnOQykaMhEo9hBJ4l5kTUngRBME7jMvydIbJHwZ7OiAr2gE4Atv3Oay3djkk32SStTsBPjauW3xfFjgDZt2kD9b8DBy8UcwSeiNpNem4Zcktr4CyXIZ6rlpOdu48l5nQHTLrJJqVdLEFRRR1UTo4Y2dtH7HCIRNXNyxmRjSPwP3PPbiJqJz+woozI6N6BAZBI9NMbc0urxQ6HLAyTbrJJN1ZS9YfcgR9zIo7AE1F70uoE/NRYRmuopiGyZ77uSgwL8QIAbGWJOd2C2QjZHE2DDj+f0t/NY0eASM8wAn/08nVcuc4ReCK6O2nZJSisqIO70gGj+7CijAi4MaXxRw5w0y2YdJPN2Z9ZBHVNPXzcFIjt6SV2OEQWwc9didgQTwAcgSeiu2dYmHFy/wBWlBE1mtTPHw5SCc7klSOrsFLscMiC8K8k2RxD+ezUAQGQSSUiR0NkOYz7iKZzBJ6I2k5fUcbFSolu1cVFbqz84HQuuhmTbrIpNRotdpwpAMCOANGtJvcP4Ag8Ed21A1nFKKuuh7erAsNYUUZk4r7IGyXmgiCIHA1ZCibdZFN+OZOPao0WXbs4ITrYQ+xwiCyKp4sco0K9AQDfp18VORqyBPv27cO0adMQGBgIiUSCLVu2tHjMnj17MGjQICgUCvTu3Rtr167t8DjJsnx3XP/3496BrCgjulV8hB+UjlJkF1fh9ytqscMhC8Gkm2yKoSPwQHQQJBJ2BIhudf+grgD0vys6HUfg7V1VVRUiIyOxatWqVrXPzs7G1KlTcc899yA9PR2LFy/GU089he3bt3dwpGQpKmrr8csZ/WKl90cHiRwNkeVxVThgYoQ/gBv9UiIHsQMgai+FFbXYd74IwI3EgohMTYzwg6vCAVeu1+DwpVIuNmjnJk+ejMmTJ7e6/erVqxESEoL33nsPABAeHo5ff/0VH3zwARISEjoqTLIgP5/KR229Dr18XDCwq0rscIgs0gODgvDDiWv44cQ1/GVKOBcbJN7pJtvxQ/o16AQgupsHQrxdxA6HyCIpHWWYMoAj8NQ2qampiI+PN3kuISEBqampIkVEne1/x64AAB4Y1JUVZUTNGNnbG96uCpRWabC38YYQ2Tcm3WQz/nfsRmk5ETXv/mh9JchPv+ehtl4rcjRkTfLz8+Hn52fynJ+fH8rLy1FTU9PkMXV1dSgvLzd5kHW6cr0av10sBQAk8lpL1CwHmRSJjQv6fnf8isjRkCVg0k024Wx+Oc7klcNRJsG9A7lqOdGdxIZ4IsjDCRV1DdiZUSB2OGTjli9fDpVKZXwEBweLHRK10feN2w0O66n/G0JEzbt/kH5gaueZQqir60WOhsTGpJtswneNd7nv6euLLi5ykaMhsmxSqQSJ0frBKUOFCFFr+Pv7o6DAdKCmoKAA7u7ucHJqOglLSkqCWq02PnJzczsjVGpngiCYlJYT0Z1FBLgjzN8NGq0OP53MEzscEhmTbrJ6Wp2ALY3bH7EjQNQ6hhLzveeLUFxZJ3I0ZC3i4uKQkpJi8tyOHTsQFxfX7DEKhQLu7u4mD7I+v19R40JRFRQOUkzu7y92OEQWTyKRGFf4NwxYkf1i0k1W7+CFYhSU10Hl5Ih7wnzEDofIKvT2dUVkVxW0OgE/NJaMkv2prKxEeno60tPTAei3BEtPT0dOTg4A/V3qWbNmGdv/6U9/wsWLF/Hyyy/j7Nmz+Ne//oVvvvkGzz//vBjhUycyLLyY0M8fbkpHkaMhsg6J0UGQSoAjl6/jckmV2OGQiJh0k9UzlJZPiwyAwkEmcjRE1uOBm/bsJvt05MgRREdHIzo6GgCwZMkSREdHY+nSpQCAvLw8YwIOACEhIfjpp5+wY8cOREZG4r333sNnn33G7cJsXL1Whx9O6AfnDPNUiahlfu5KjOjtDYDXWnvHfbrJqlXVNeDnU/kAbpTLElHrTIsMxJtbz+DkVTXOF1Sgj5+b2CFRJxs7diwEQWj2+2vXrm3ymOPHj3dgVGRp9pwrQmmVBt6uCoxqTCCIqHXujw7C/sxifHf8KhaND+VWe3aKd7rJqv30ex5q6rUI8XbBoG4eYodDZFU8XeS4J8wXALDpCBe3IqKmfdP49yExKhAOMnYdicwxqb8/XOQyXC6pRlp2qdjhkEj4l5Os2obD+rLHRwYHc+SQqA1mDNZv3/TtsavQNOhEjoaILE1heS12nS0EAMwYwu3eiMzlLHfAtEj9jiEbD3OA214x6SarlVVYgWM5ZZBJJXgwhnPMiNpibF8f+LopUFqlQQr37CaiW3x77Cq0OgHR3TwQyikoRG3ySOOA1baTeVDXcM9ue9SmpHvVqlXo0aMHlEolYmNjcejQoTu237RpE8LCwqBUKjFgwABs27bN5PuCIGDp0qUICAiAk5MT4uPjkZmZadKmtLQUjz32GNzd3eHh4YG5c+eisrKyyfNlZWXBzc0NHh4ebXl7ZCUMo4X39PWFr5tS5GiIrJODTIoHY/TrIWxkiTkR3UQQBOPUk0d5l5uozaKDPRDq64q6hhuLEpJ9MTvp3rhxI5YsWYJly5bh2LFjiIyMREJCAgoLC5tsf/DgQcycORNz587F8ePHkZiYiMTERJw6dcrY5p133sFHH32E1atXIy0tDS4uLkhISEBtba2xzWOPPYbTp09jx44d2Lp1K/bt24f58+ffdr76+nrMnDkTo0aNMvetkRXRNOjwv8ZVy9kRILo7jzSWmO87X4Q8dY3I0RCRpTh86TouFlfBWS7D1IGBYodDZLUkEolxesY3LDG3S2Yn3e+//z7mzZuHOXPmICIiAqtXr4azszM+//zzJtt/+OGHmDRpEl566SWEh4fjzTffxKBBg7By5UoA+lHUFStW4NVXX8X06dMxcOBAfPHFF7h27Rq2bNkCAMjIyEBycjI+++wzxMbGYuTIkfjnP/+JDRs24No109GiV199FWFhYXjkkUfMfWtkRXadLUBJlQa+bgqM7cu9uYnuRoi3C2JDPKETgM1HrogdDhFZCENF2b0DA+Cq4IY3RHfjgUFd4SiT4ORVNU5fU4sdDnUys5JujUaDo0ePIj4+/sYLSKWIj49Hampqk8ekpqaatAeAhIQEY/vs7Gzk5+ebtFGpVIiNjTW2SU1NhYeHBwYPHmxsEx8fD6lUirS0NONzu3btwqZNm7Bq1apWvZ+6ujqUl5ebPMg6bGjsCDwY05UrqRK1A8MI/MYjudDpmt9CiojsQ3ltPX46qb+xMWNIN5GjIbJ+ni5yTIjwA8C73fbIrGyluLgYWq0Wfn5+Js/7+fkhPz+/yWPy8/Pv2N7wb0ttfH19Tb7v4OAAT09PY5uSkhI88cQTWLt2Ldzd3Vv1fpYvXw6VSmV8BAezTNka5KlrsO98EYAbZbFEdHcm9w+Am8IBV67XIPViidjhEJHIfjxxDbX1OvT2deWWnETtxNBv3ZJ+DbX1WpGjoc5kM7cI582bhz/84Q8YPXp0q49JSkqCWq02PnJzOepkDTYfuQKdAMSGeCLE20XscIhsgpNchunR3NKEiPQMd+JmcEtOonYzKtQHgSol1DX12H666RuWZJvMSrq9vb0hk8lQUGC6rUxBQQH8/f2bPMbf3/+O7Q3/ttTm1oXaGhoaUFpaamyza9cuvPvuu3BwcICDgwPmzp0LtVoNBweHZuebKxQKuLu7mzzIsul0Ar452tgR4AJqRO1qxmB9CWny6XyUVWtEjoaIxJKRV44TV9RwlElw/yBuyUnUXmRSCR5qvNvNAW77YlbSLZfLERMTg5SUFONzOp0OKSkpiIuLa/KYuLg4k/YAsGPHDmP7kJAQ+Pv7m7QpLy9HWlqasU1cXBzKyspw9OhRY5tdu3ZBp9MhNjYWgH7ed3p6uvHxxhtvwM3NDenp6bj//vvNeZtkwfZnFSO3tAZuSgdM7h8gdjhENqV/kDsiAtyhadBh81EuqEZkr9an5QAAJkT4wdtVIXI0RLbl4ZiukEiAgxdKcLGo6e2PyfaYXV6+ZMkSfPrpp1i3bh0yMjKwYMECVFVVYc6cOQCAWbNmISkpydh+0aJFSE5OxnvvvYezZ8/itddew5EjR7Bw4UIA+iX0Fy9ejLfeegs//PADTp48iVmzZiEwMBCJiYkAgPDwcEyaNAnz5s3DoUOHcODAASxcuBCPPvooAgMDjW369+9vfAQFBUEqlaJ///7o0qXL3f6cyEJ8mXoZAPBQTFc4yWUiR0NkWyQSCR4f1h0A8HVaDhdUI7JDlXUN+N8x/aDb47HdRY6GyPYEezrjnr76taq+bhzgIttndtI9Y8YMvPvuu1i6dCmioqKQnp6O5ORk40JoOTk5yMvLM7YfPnw41q9fjzVr1iAyMhKbN2/Gli1b0L9/f2Obl19+Gc8++yzmz5+PIUOGoLKyEsnJyVAqlcY2X3/9NcLCwjB+/HhMmTIFI0eOxJo1a+7mvZOVuXK9GrvO6qchGBIDImpf06MC4aZwQHZxFQ5cKBY7HCLqZN8dv4oqjRY9fVwQ18tL7HCIbNIfG/uxm47kokbDBdXsgUQQBN7KaFReXg6VSgW1Ws353RboH9vPYtXuCxjR2wtfPzVM7HCIbNZrP5zG2oOXMDHCD2tmDW75AOp01ny9subYbZ0gCJi0Yj/OFVRg2bQIzBkRInZIRDZJqxMw9t3dyC2twTsPDsQjXKfIIrXn9cpmVi8n21bXoDUuOPFH3uUm6lCPD9MvqLYzowDXympEjoaIOsvhS9dxrqACTo4yPDCoq9jhENksmVSCxxqnb3z522WRo6HOwKSbrELyqXwUV2rg565AfLhfywcQUZv19nVDXE8v6ATgv4c434zIXnzV2PmfHhUIlZOjyNEQ2bZHBgdD7iDFyatqnMgtEzsc6mBMuskqGDoCM4d2g4OMH1uijvbHOP0I/H8P5ULToBM5GiLqaEUVdfj5lH5NHq6bQtTxPF3kuHeAfice3u22fcxeyOJl5JXj8KXrkEklmDm0m9jhENmFCRF+8HVToLiyDttP54sdDhF1sG+O5KJeKyC6mwf6B6nEDofILjzeOMD944lruF6lETka6khMusniGe5yJ/Tzg5+7soXWRNQeHGVS4yAXR+CJbJtWJ+Drxt9zrptC1Hmigz3QL9AddQ06bD56RexwqAMx6SaLpq6px3fHrwJguRtRZ5s5tBtkUgkOZZciI69c7HCIqIPszCjANXUtujg7YkpjuSsRdTyJRGIc6Pryt8vQ6riplK1i0k0WbcOhHFRrtOjj54q4ntwvlKgz+auUmNTfHwDw71+zRY6GiDrKv/frf78fHdoNSkeZyNEQ2ZfpUUHwcHZETmk1dpwpEDsc6iBMusli1Wt1WHfwEgBg7sgQSCQScQMiskNzR+r36f0h/RoKK2pFjoaI2tvvV8pw6FIpHKQSzI7rIXY4RHbHSS7DHxqnc33OAW6bxaSbLNbPp/JxTV0LLxc5pkcFiR0OkV0a1K0LBnXzgEarw1epnNtNZGsMVSzTIgPhr+K6KURimD28BxxlEhy6VIrfr5SJHQ51ACbdZJEEQcC/918EoN+6iOVuROJ5alRPAMBXaTmordeKHA0RtZc8dQ1++l2/TZihqoWIOp+fuxLTBgYC4HQuW8WkmyzS0cvXceKKGnIHKRdQIxLZxAg/BHk4obRKY1zYkIis37qDl9GgExAb4sltwohE9mTjwNdPv+chT10jcjTU3ph0k0UyjPLdHxUEb1eFyNEQ2TcHmRRzRvQAoP/dFASurkpk7arqGrA+TT9lxFDNQkTi6R+kwrCenmjQCVh3kNO5bA2TbrI4uaXV2H46H8CNUT8iEteMIcFwVTggq7ASe88XiR0OEd2lb49dQXltA3p4OWN8mK/Y4RARgLkj9QNg69Muo6quQeRoqD0x6SaL858Dl6ATgFGh3ujr7yZ2OEQEwE3piBlDggFwvhmRtdPpBOMqyU+ODIFUyt1BiCzB+DBf9PByRnltA749dkXscKgdMekmi1JWrcHGwzkAuKgLkaV5YngPSCXA/sxinL6mFjscImqjX87k41JJNdyVDnhwUFexwyGiRlKpxFjl+en+i2jQ6kSOiNoLk26yKGsPXkKVRoswfzeM6eMjdjhEdJNgT2fc27i66r/2XBA5GiJqC0EQsGq3/vd39vAecFE4iBwREd3s4ZhgeLrIkVtag62NuwuQ9WPSTRajsq4B/zlwCQDwzD29IZGw3I3I0iwY2wsAsO1kHi4WVYocDRGZa///396dh8d07nEA/84kmSwkmSyySqyR2BNCJFW0UmtbylXUVnXpYr2lLbdKe29bqrpRFL1FW5WiiqLUWkqERGSThAiSIAkik32ZzHv/CFNjzUQmZyb5fp5nHo9z3sz8fpPMvOd3znve99x1xF1WwdrCDOOf4IgyImNjrTDTjvZcfigFGg0nL60LWHST0dgQkQZVcTmaOTfAgPbuUodDRPfR2t0Ooa1dIASwgle7iUzO1wdTAAAju3rDsYFC4miI6H5Gd2sCW0tznM0qwN7ELKnDoRrAopuMQkl5BVYfSQUAvN6zBcw4qQuR0XrjqZYAgF+jL+NyLtcSJTIVJy/m4MSFHFiYyTCpB5cJIzJW9tYWGBvSBACw/GAKl+qsA1h0k1H45VQGsvNL4WFvhcEBnlKHQ0QP0cnbASEtnKDWCKw+nCp1OERURctvXeX+R+fGcLO3kjgaInqY8U80g5WFHDEZKhxNuSF1OPSYWHST5NQVGnzzZ+Uw1Uk9mkNhzj9LImM3+dbV7g0n0nAtv1TiaIjoUeIvq3Aw+RrkMuDVHi2kDoeIHsG5oSVGdPEGAHx98JzE0dDjYnVDkvst9grSc4rh1ECB4be+XIjIuIW0cEJHLyVK1Rp8d5TrdhMZu9tzMDzbwQNNnRtIHA0RVcWkHs1hLpfheGoOoi7lSB0OPQYW3SSpCo3A1wcqh7u90r0ZrBVmEkdERFUhk8kw+dZM5j+EX8LNwjKJIyKiBzmXlY9d8ZVLD91egYCIjJ+H0hpDOlXedrlkf4rE0dDjYNFNktp2+jLOXyuEvbUFxgQ3kTocItJDaGtXtHa3Q0GpGit5bzeR0fpi31kIAfRpU/mZJSLT8UavljCTy/Dn2WuIvMir3aaKRTdJprxCgy/3Vd6j8lrPFrCzspA4IiLSh1wuw8xnWgEA1h67gOz8EokjIqK7xV9WYVdcJmQyYGYfX6nDISI9NXVugGGdGwMAFv+RzJnMTRSLbpLMpsgMpOUUwbmhJcaF8Co3kSnq3doFHb2UKCnXYPlBrttNZGw+33sWAPB8Rw/4utlKHA0RVcfU3j5QmMlxPDUHx85zJnNTxKKbJFFSXoGlByqvck9+qgVsFOYSR0RE1SGTyfDWratnP0Wk4QrX7SYyGlGXbuJAUjbM5DLMCG0ldThEVE2eSmu8FFQ52TCvdpsmFt0kiZ8i0nBVVQJ3eyuM7MoZy4lM2RMtnRDUzBFlFRrtyTQikt5nfyQDAP7RqTGaccZyIpP2xlMtYGUhR3RaLg4kZUsdDumJRTfVuqIyNZYfqpyBcerTPrCy4IzlRKZMJpNhVt/Kq90bIzNw8XqhxBER0bGU6zh2/gYszGSY2rul1OEQ0WNysbXCuJCmAIDP/jgLjYZXu00Ji26qdWuPXcT1gjJ4O9pgWGBjqcMhohrQpakjerZqhAqNwFf7ebWbSEpCCHx2617ukV290djBRuKIiKgmvNajBRpamuPM1Tz8Hp8pdTikBxbdVKtuFJRixa3JlmaE+sDCjH+CRHXFzD6V94xuPX0Z8ZdVEkdDVH/tSchC1KWbsLKQY8pTvMpNVFc4NFBgQvdmAIBP9yShTK2ROCKqKlY8VKu+2n8O+aVqtPWww2B/T6nDIaIa1KGxEs939IAQwEc7EznRC5EEytQaLPg9EQAw6cnmcLGzkjgiIqpJE3s0h3NDS1y8UYQfjl+SOhyqIhbdVGtSsvOxPiINAPDuwNaQy2USR0RENe3tfr5QmMsRnnoD+xI50YspWLZsGZo2bQorKysEBQXhxIkTD2y7du1ayGQynYeVFYs6Y/J9+EVculGERraWeLVnC6nDIaIa1tDSHLNujSxbsv8ccovKJI6IqoJFN9Waj3cloUIjENraFSEtnKUOh4gMoLGDjXbo28e7Ejn0zcj9/PPPePPNNzF//nycOnUKHTt2RN++fZGd/eATJnZ2drh69ar2cekSr7QYi5uFZVhya06FWX1aoYEll+MkqouGBXrBz80WquJyzqNiIlh0U604cu4aDiRlw1wuw5wBflKHQ0QG9EavFnBqoMCF64VYH8GCzJh9/vnnmDhxIsaPH482bdrgm2++gY2NDb777rsH/oxMJoObm5v24erqWosR08N8tf8c8krU8HOzxT86e0kdDhEZiJlchrkD2wAAfgi/hNRrBRJHRI/CopsMrkIj8NHOyvvLRndrghaNGkocEREZkq2VBd68NfTty30c+masysrKEBUVhdDQUO02uVyO0NBQhIeHP/DnCgoK0KRJE3h5eWHQoEFISEh46OuUlpYiLy9P50E17/y1Avx46/7OuQPbwIy3cBHVad19nPG0nwvUGoEFvydJHQ49AotuMrhNkelIysyHnZU5pvf2kTocIqoFwwO90Mq1IVTF5ViyP0XqcOg+rl+/joqKinuuVLu6uiIz8/5L0fj6+uK7777Dtm3b8OOPP0Kj0SAkJAQZGRkPfJ0FCxbA3t5e+/Dy4hVYQ1iwKwlqjUBvPxd09+EtXET1wb8H+MFMLsPeM1k4dv661OHQQ7DoJoO6WViGT3ZXnn2b1tsHDg0UEkdERLXB3EyOd28NfVsXfhFJmby6WRcEBwdj7Nix8Pf3R8+ePbFlyxY0atQIK1eufODPzJkzByqVSvtIT0+vxYjrhwNJWdiXmAUzuQxzBrSWOhwiqiUtXWzxUldvAMD8bQmcR8WIsegmg/pkdxJuFpXD19UW40KaSh0OEdWinq0aoW9bV1RoBOb+Gg+NhkuIGRNnZ2eYmZkhKytLZ3tWVhbc3Nyq9BwWFhYICAhASsqDRzNYWlrCzs5O50E1p7isAvO2VQ7x/2f3Zmjpwlu4iOqTmX1awamBAueyC/C/vy5IHQ49QLWKbn2WFwGATZs2wc/PD1ZWVmjfvj127dqls18IgXnz5sHd3R3W1tYIDQ3FuXO6M/Hl5ORg1KhRsLOzg1KpxIQJE1BQ8PekAYcOHcKgQYPg7u6OBg0awN/fH+vXr69OelRDoi7lIOxk5RWND19oBwsznuMhqm/mP9cWNgozRF66ic1RDx6CTLVPoVCgc+fO2L9/v3abRqPB/v37ERwcXKXnqKioQFxcHNzd3Q0VJj3C1wfPIeNmMTzsrTCNt3AR1TtKG4V2hMtX+88iPadI4ojofvSugvRdXuTYsWMYOXIkJkyYgOjoaAwePBiDBw9GfHy8ts2iRYuwZMkSfPPNN4iIiECDBg3Qt29flJSUaNuMGjUKCQkJ2Lt3L3bs2IHDhw9j0qRJOq/ToUMH/PLLL4iNjcX48eMxduxY7NixQ98UqQaoKzR499fK3/Gwzo3RpamjxBERkRQ8lNaYEVpZCCz4PRE3CzmpmjF58803sXr1aqxbtw6JiYl4/fXXUVhYiPHjxwMAxo4dizlz5mjb/+c//8Eff/yB1NRUnDp1CqNHj8alS5fwz3/+U6oU6rWU7HysOpwKAJj/fFsuEUZUTw3t5ImuzRxRUq7BB789fHJLkoZMCKHXeL+goCB06dIFX3/9NYDKs+JeXl6YOnUqZs+efU/74cOHo7CwUKf47datG/z9/fHNN99ACAEPDw/MnDkTs2bNAgCoVCq4urpi7dq1GDFiBBITE9GmTRucPHkSgYGBAIDdu3djwIAByMjIgIeHx31jHThwIFxdXR+69Mmd8vLyYG9vD5VKxeFvj+nbI6n4cGcilDYWODCzFxx5LzdRvVVeocFzS/9CUmY+hgd64ZN/dJA6JJNXk/3V119/jU8//RSZmZnw9/fHkiVLEBQUBADo1asXmjZtirVr1wIA/vWvf2HLli3IzMyEg4MDOnfujA8//BABAQGSxF6fCSEwcvVxHE/NQW8/F3w7LhAyGWcsJ6qvzmXlo/9XR6DWCKwa0xl92lbtNiF6sJrsr/S60l2d5UXCw8N12gNA3759te0vXLiAzMxMnTb29vYICgrStgkPD4dSqdQW3AAQGhoKuVyOiIiIB8arUqng6PjgK6xcxsQwruQW4/O9ZwEAc/r7seAmqucszOT4cHA7AMDPkemIvJgjcUR0pylTpuDSpUsoLS1FRESEtuAGKm/dul1wA8AXX3yhbZuZmYmdO3fqVXBTzfk1+jKOp+bAykKO959vy4KbqJ7zcbXFxB7NAQDvb09AYala4ojoTnoV3dVZXiQzM/Oh7W//+6g2Li4uOvvNzc3h6Oj4wNfduHEjTp48qR0idz9cxqTmCSEwb1s8isoq0LmJA4Z15ntKREBgU0cMD6z8Ppi9JQ4l5RUSR0Rkuq4XlOLDnYkAgKlP+8DL0UbiiIjIGEx72geeSmtcUZVg8R/JUodDd6iTM1sdPHgQ48ePx+rVq9G2bdsHtuMyJjXvl1OXsS8xGwozOT5+oT3kcp55J6JKs/v7wbmhJVKyC/DFvrNSh0NkkoSoXA0gp7AMfm62mPhkc6lDIiIjYa0ww4cvVI4sW3vsIiJSb0gcEd2mV9FdneVF3NzcHtr+9r+PanP3RG1qtRo5OTn3vO6ff/6J5557Dl988QXGjh370Hy4jEnNuqoq1k7eMOMZH/i62UocEREZE4cGCiwY0h4AsPpwKqIu3ZQ4IiLTsz3mCnYnZMJcLsNnL3aEwrxOXj8homp6ytcFwwO9IATw1uZYDjM3Enp9U1dneZHg4GCd9gCwd+9ebftmzZrBzc1Np01eXh4iIiK0bYKDg5Gbm4uoqChtmwMHDkCj0dxz79nAgQPxySef6MxsToYnhMDbm2ORX6KGv5cSk3jmnYju45k2rhjSyRMaAczaFIPiMg4zJ6qq7LwS7ZrcU5/2QVsPe4kjIiJjNPfZ1vBUWiMtpwgLf0+SOhxCNYaX67u8yPTp07F792589tlnSEpKwvvvv4/IyEhMmTIFACCTyTBjxgx8+OGH2L59O+Li4jB27Fh4eHhg8ODBAIDWrVujX79+mDhxIk6cOIGjR49iypQpGDFihHbm8oMHD2LgwIGYNm0ahg4diszMTGRmZiInhxP21Iawk+k4cu46LM3lWDysI8y5JjcRPcD859rC1c4SF64X4tM9vOeMqCqEEPj3r3FQFZejnacd3niqhdQhEZGRsrWywCdDK1cK+eH4JRxNuS5xRKR3ZTR8+HAsXrwY8+bNg7+/P06fPo3du3drJ0JLS0vD1atXte1DQkLw008/YdWqVejYsSM2b96MrVu3ol27dto2b7/9NqZOnYpJkyahS5cuKCgowO7du2FlZaVts379evj5+aF3794YMGAAunfvjlWrVmn3r1u3DkVFRViwYAHc3d21jyFDhlTrjaGqS88pwoc7zgAA3urri5YuDSWOiIiMmb313wcDa45dwHHec0b0SHfOmfLZMH9Y8OQ2ET1Edx9njO7mDQC3RqOWSxxR/ab3Ot11GdcO1V95hQYjVh1H1KWbCGzigJ9fDYYZJ08joiqY/Usswk6mw1NpjZ3TukNpw+UFq8qU+ytTjl0qF68X4tmlf6GgVI23+/nijV4tpQ6JiExAYaka/b86grScIgzy98CXw/25vKAeJFunm+hun+89i6hLN2FraY7PX/RnwU1EVTb32TZo6mSDy7nFmLUpFjwHTHSvUnUFJv90CgWlanRt6sg5U4ioyhpYmuOL4R1hJpdh2+kr2BjJlZqkwqKbqu3Ps9ew4tB5AMDCoR3g7cR1Qomo6hpamuPrlzpBYSbHvsQsrDl6UeqQiIzOxzsTkXAlDw42FlgyMoBzphCRXjo3ccSsPr4AgPnbE5CcmS9xRPUTv7mpWrLySvDmz6cBAKO7eWNgB3dpAyIik9TO0x5zn20NAFjweyJiM3KlDYjIiOyOv4p14ZcAAJ8P94ebvdUjfoKI6F6v9miOnq0aoaRcg8k/nUJRGZcRq20suklvFRqB6WHRuFFYhtbudpg7sI3UIRGRCRvTrQn6tXVDeYXAlJ+ikcfJXoiQnlOEtzbHAgBe7dkcT/m6SBwREZkquVyGz1/sCFc7S6RkF2iXHqTaw6Kb9Lb4j2QcT82BjcIMy14KgJWFmdQhEZEJk8lk+OQfHdDYoXJN0ZkbY6DR8P5uqr+KyyrwxvpTyC9RI8BbqR0aSkRUXU4NLfHViADIZcDmqAxsOJEmdUj1Cotu0svW6Mva+7gXDGmP5o24PBgRPT57a4vK+7vN5dh7Jguf7z0rdUhEkhBC4O1fYhF3WQUHGwssHRnA5cGIqEZ0a+6EN59pBQCYty0eJy7kSBxR/cFvcaqymPRcvP1L5VC313u1wCB/T4kjIqK6xN9LiYVD2gMAvj6Ygu0xVySOiKj2LT90Hr/FXIG5XIYVozujsQMnKSWimjP5qZYY2N4d5RUCr/0YhYybRVKHVC+w6KYqycorwcTvI1Gm1iC0tQve4lA3IjKAIZ0a49UelUsivbUphhOrUb3yR0ImPt2TDAD4YFBbdGvuJHFERFTXyGQyLB7WEW097JBTWIZ/rotEYSknVjM0Ft30SCXlFZj0fSSy80vRyrUhvhwRADnX4yYiA3m7nx+e9nNBqVqDid9HIiuvROqQiAwuKTMP/7q1KsjY4CYYFdRE2oCIqM6yVphh9dhAODe0RFJmPt7ceJpzqRgYi256KHWFBlM3RCMmQwWljQW+HdsFDS3NpQ6LiOowM7kMX43wR0uXhsjKK8XLa05yRnOq0y7nFmP8mpMoLKtASAsnvPcsVwUhIsPyUFpj5ZjOUJjJsSchC//deQZCsPA2FBbd9EBCCPz71zjsPZMFhbkcK0d3hrcT7y0jIsOztbLA/8ZVnoVPvJqHf66LREl5hdRhEdW4nMIyjPlfBK6qStDSpSGWj+rEidOIqFZ0buKAT4d1AACsOXoRy29Nlkw1j9/q9ECf7E7GxsgMyGXA1yMDEMR7y4ioFjVxaoC147vA1tIcJy7kYNqGaKgrNFKHRVRjCkvVGL/mBFKvFcLD3grfv9IVShuF1GERUT0yyN8T826Nrvl0TzKXEjMQFt10X6sPp+KbPyvPdi0c0gF92rpJHBER1UftPO2xelwgFOZy/HEmC+/+Gs/hb1QnlKor8NqPUYjJqFwa7PsJQfBQWksdFhHVQ690b4bJT7UAALz7axx2x1+VOKK6h0U33eOH45fw0a5EAMDs/n54sYuXxBERUX3WrbkTlo4MgFwG/ByZjg9+431nZNrK1BpM/SkaR85dh43CDGvGd0VLl4ZSh0VE9disPr4Y2dULGgFM23Aa+xOzpA6pTmHRTTq+++sC3tsaDwB4tUdz7dI9RERS6tvWDQuHVt53tvbYRby3LZ4zrZJJKimvvML9x+35UsZ0hr+XUuqwiKiek8lk+HBwewxo74ayCg1e+zEKexIypQ6rzmDRTVqrDp/Hf3acAQC81rMFZvf3g0zGpcGIyDi8GOiFRf/oAJkM+PF4GuZsiWPhTSalpLwCE7+PxIGkbFiay/G/cYF40qeR1GEREQG4vXpIAJ7t4I7yCoHJ609hZyyHmtcEFt0EAFh2MAUf70oCAEx7uiXe6efLgpuIjM6LgV74/MWO2qHmszbHoIKFN5mAojI1Xll7EkfOXYe1hRnWjO/CgpuIjI6FmRxfDvfHCwGeUGsEpm44ha3Rl6UOy+RxweV6rkIj8OHOM1hz9CIA4M1nWmFabx9pgyIieogXAhrDXC7HjJ9PY8upy8grVmPJSH/YKNilkXG6ll+Kf647iZgMFRrcuoe7azNHqcMiIrovczM5Fg/rCHO5DJuiMvCvjadxvaAUE7o340W5auKV7nqsuKwCb6yP0hbc/x7gx4KbiEzCcx09sHxUJ1iay7EvMQsjVh1Hdn6J1GER3SMluwBDVhxFTIYKShsL/PDPIBbcRGT0zOQyfDK0A8YGN4EQwIc7E/HBb2c4uqyaWHTXU9cLSjFy9XHsSciCwkyOpSMDMKlHC6nDIiKqsr5t3fDTxCA42FggNkOFF5YdQ0p2vtRhEWlFpN7A0BXHkJ5TjCZONtjyegg6eTtIHRYRUZXI5TJ88Hxb/HuAH4DKiUxf/SEKRWVqiSMzPSy666EzV/LwwvKjOJ2eC6WNBdZPDMJzHT2kDouISG+dmzhiyxtPoKmTDS7nFmPI8mM4mJwtdVhE2BiZjjH/OwFVcTkCvJXY8noImjfismBEZFpkMhkm9WiBZS91guLW6LLhK4/jcm6x1KGZFBbd9cymyHS8sPwo0nOK4e1og19eD0GXphzmRkSmq5lzA2x54wl0buKAvJLKyaq+2HuWQ+BIEiXlFXhncyze3hyLsgoN+rV1w4aJ3eDU0FLq0IiIqm1gB3dsuDW6LO6yCs8uOYI/z16TOiyTwaK7nigpr8CcLbF4a3MsStUa9PJthG2Tn0ALnnUnojrAsYECP00MwqggbwgBfLX/HMavPYmcwjKpQ6N6JO1GEYauOIafI9MhkwGz+rTC8lGdYGVhJnVoRESPrXMTR2yf0h3tPe1xs6gcL685gS/3neXynVXAorseOJeVj398cwwbTlQeBLz5TCt8N64LHBoopA6NiKjGWJqb4aMX2uPzFzvCykKOw2ev4dklRxB+/obUoVE9sCP2Cp5degQJV/Lg2ECBH14JwpSnfSCXc6ZfIqo7vBxtsOm1YLx06yT3l/vOYdyaE8hUcTLTh5EJIXhq4pa8vDzY29tDpVLBzs5O6nAeW4VGYM3RC1i0Jxllag0cbCywZGQA1wUlojovKTMPr/94CheuFwIAxj/RFO/086szVxxNub8y5djv52ZhGeZtT8BvMVcAAAHeSix7qRM8lNYSR0ZEZFi/RGXg3a1xKCnXwM7KHP8Z1A6D/D3qzLJiNdlfsei+Q106EEi7UYRZm2Jw4mIOAKCXbyN8MrQDXO2sJI6MiKh2FJSq8dHORGw4kQYAaN6oAT4b1hEBdWD2aFPur0w59rsdSMrCO7/E4Vp+KczkMkx+qiWmPNUSCnMOJCSi+iElOx8zN8YgJkMFAOjfzg0fDm5XJ+axYNFtIHXhQKBUXYFvj1zA0gPnUFKuQQOFGd57tg2Gd/GqM2ediIj0cTA5G+9sjkV2finkMmB0tyaY+Ywv7G0spA6t2ky5vzLl2G+7qirGhzsTsTP2KgCgRaMG+PxFf3T0UkobGBGRBNQVGiw/dB5L9p+DWiOgtLHAO/38MDzQy6RvsWHRbSCmfiDw59lreH97gnY4ZXBzJyz6Rwd4OdpIHBkRkbRyi8rwwW9n8Gv0ZQCAUwMFZvf3w9BOjU3ygMCU+ytTjr1MrcF3Ry9gyf5zKCqrgFwGvPJEM8zq61tnbl0gIqqu+MsqzNoUg6TMfABARy8l/juoLTo0VkobWDWx6DYQUz0QOJuVj0/3JGPvmSwAQCNbS8wd2BrPd6w791QQEdWEY+evY962BKRkFwCovP92dj8/BDV3kjgy/ZhqfwWYZuxCCOxJyMKne5Jw/lrlie3OTRzwn0Ft0dbDXuLoiIiMh7pCg+/DL+HzvWdRUKqGTAYM69wY00NbwdPE5rpg0W0gpnYgkJ5ThC/2nsWvpy9DCMBMLsP4kKaYHuoDWyvTHTZJRGRIZWoN1h67gC/3VV6tBIAerRrhrT6+aN/YNAooU+uv7mRqsf917jo+3ZOkvV/RqYECcwa0xpAAT5McJUFEVBuy80qw4Pck7QgzhZkco7s1wRtPtYCzidzvzaLbQEzlQODC9UKsOpyKzVHpKK+o/PX1b+eGmX1aoaWLrcTRERGZhqy8Eiw9cA5hJ9KhvrXGaN+2rni9V0v4G/m9uabSX92PKcQuhMCRc9ex4tB5hKdWLjlnozDDK080w8QezWFvzRPbRERVEXXpJhbtTkLEhcrJnW0UZhgb3BSvPNEULkY+wTOLbgMx9gOB6LSbWPlnKvacycTt39qTPs54q6+vyd4rQUQktUs3CvHlvnPYemvUEAAENXPEqz2bo1crF6O8mmns/dXDGHPs5RUa7Iy9ipWHU5F4NQ9A5dWZUd288UavlmhkaxpXZ4iIjIkQAn+lXMene5IRe2vUkMJMjhcCPDGxR3O0dGkocYT3x6LbQIzxQKCoTI0dsVex4UQaotNytduf9nPBqz2am9x9iERExupsVj5W/pmK7TGXtaOIWjRqgJFdvTGkU2M4NlBIHOHfjLG/qipjjP1KbjE2Rqbj55PpuKoqAVB5NWZ4Fy/888nmJncfIhGRMRJCYF9iNlb+eR6Rl25qt/do1Qgju3ghtI0rLMyMZ8lFFt0GYiwHAkIIxGaosDkqA1ujLyO/VA0AsDCTYbC/Jyb1aA4fVw4jJyIyhKuqYqw5ehE/RaSh4Nb3r8JMjn7t3PBioBeCWzjBTOKr38bSX1WHscReqq7AwaRr2BiZjkPJ2bh1hwGcGyrwckhTjO7WBEob4znRQkRUl0RdysHKP1OxNzFLO8rMuaElhgU2xtBOnkZxyyyLbgOR+kAgOTMfv8VcwW+xV3DpRpF2u7ejDUZ09cI/OjeGi61x3/tARFRX5JeUY3vMFfwUkYaEK3na7c4NLTGwvRue6+iBTt4Okgw/l7q/ehxSxq6u0ODo+Rv4LeYK9iRkIr9Erd3XrbkjRnb1Rt+2blz+i4iolqTdKELYyTRsjMzA9YJS7fbW7nZ4rqM7nuvgIdnyxyy6DaS2DwTK1BpEXszB/qRsHEjK1q6vDQBWFnKEtnbFiC7eCGnhZJT3FBIR1RdxGSpsOJmGXXFXkVtUrt3uameJp3xd8LSfC7r7OMNGYV4r8bDorrrcojL8efYaDiRl41DyNaiK//79udlZ4Xl/Dwzv4oUWjYzznkIiovqgvEKD/YlZ2BiZgcNnr2knOAUqC/Defi54urULOjZW1tpoMxbdBmLoAwGNRiApMx/hqTcQfv46IlJztEPHgcrh4z1bueB5fw/09nNBA8vaOXgjIqKqKVNrcDTlOn6LuYI/zmRph58DgMJcji5NHRDc3AnBLZzQobHSYPemseh+sJLyCkRduonw8zcQnnoDp9NzUXHHwZtjAwUGtHfDcx080KWpI09qExEZmZuFZdiTkInfYq8g/PwN3PEVDqcGCnRr4YSQFk4Ibu6EZs4NIJMZ5nucRbeB1PSBQF5JOWLTVYhOu4no9FxEp93EzTuukACV94495euC3q1d8ERLZ66vTURkIkrVFYhIzcGBpGzsT8pCek6xzn4bhRn8vZQI8FbC38sB/l7KGpv9mkV3JSEErqhKcDqtso89nZ6L2AwVyio0Ou18XW3xdGsX9PZzgb+XEuZGNFEPERE92I2CUvx59hr2J2Xj8NlrOrcFAZUjzjp5O2j72vae9rBW1MwtQiy6DaQm39iFvydh5eHzuPvdtVGYoUtTx8qzMy2c0M7DnmfZiYhMnBAC568VIvz8dRw7fwPHU2/cc5IVAEJbu+LbcYGP/Xosuiv1/eIwkrPy79nuZmeF4Fv9bEgLJzR2kOZ+QCIiqjnlFRpEp+XeGsl0Hacu5d5zktVMLsMHz7fF6G5NHvv1arK/qtap3mXLlqFp06awsrJCUFAQTpw48dD2mzZtgp+fH6ysrNC+fXvs2rVLZ78QAvPmzYO7uzusra0RGhqKc+fO6bTJycnBqFGjYGdnB6VSiQkTJqCgoECnTWxsLJ588klYWVnBy8sLixYtqk56NcLTwRpCAF6O1ni+owfmP9cGv74Rgpj5fbDula54tWcLdGisZMFNRFQHyGQytHRpiDHBTbFidGdEzX0Gu2c8iQVD2uPFwMZo5doQMhngqTS+yTBruk+vTW72VjCXy9DO0w5jujXBZ8M64uCsXgif8zS+GO6PFwO9WHATEdURFmZydG3miOmhPgibFIzY9/sgbFI3zO7vh75tXeFia4kKjYC3RBOvPZTQU1hYmFAoFOK7774TCQkJYuLEiUKpVIqsrKz7tj969KgwMzMTixYtEmfOnBFz584VFhYWIi4uTttm4cKFwt7eXmzdulXExMSI559/XjRr1kwUFxdr2/Tr10907NhRHD9+XBw5ckS0bNlSjBw5UrtfpVIJV1dXMWrUKBEfHy82bNggrK2txcqVK6ucm0qlEgCESqXS9225R25RmbiWX/LYz0NERHWDqrhMZOfVTL9QU/2VIfr02opdCCGu5BaJolL1Yz8PERGZPo1GIy7fLBLFZTXTL9Rkf6X38PKgoCB06dIFX3/9NQBAo9HAy8sLU6dOxezZs+9pP3z4cBQWFmLHjh3abd26dYO/vz+++eYbCCHg4eGBmTNnYtasWQAAlUoFV1dXrF27FiNGjEBiYiLatGmDkydPIjCwclje7t27MWDAAGRkZMDDwwMrVqzAu+++i8zMTCgUletqzp49G1u3bkVSUlKVcjPl4XpERFR/1FR/VdN9em3GTkREZEiSDS8vKytDVFQUQkND/34CuRyhoaEIDw+/78+Eh4frtAeAvn37attfuHABmZmZOm3s7e0RFBSkbRMeHg6lUqktuAEgNDQUcrkcERER2jY9evTQFty3Xyc5ORk3b968b2ylpaXIy8vTeRAREdUHhujT74d9LRER1Xd6Fd3Xr19HRUUFXF1ddba7uroiMzPzvj+TmZn50Pa3/31UGxcXF5395ubmcHR01Glzv+e48zXutmDBAtjb22sfXl5e90+ciIiojjFEn34/7GuJiKi+q9drZsyZMwcqlUr7SE9PlzokIiKiOoV9LRER1Xfm+jR2dnaGmZkZsrKydLZnZWXBzc3tvj/j5ub20Pa3/83KyoK7u7tOG39/f22b7OxsnedQq9XIycnReZ77vc6dr3E3S0tLWFrWzJqpREREpsQQffr9sK8lIqL6Tq8r3QqFAp07d8b+/fu12zQaDfbv34/g4OD7/kxwcLBOewDYu3evtn2zZs3g5uam0yYvLw8RERHaNsHBwcjNzUVUVJS2zYEDB6DRaBAUFKRtc/jwYZSXl+u8jq+vLxwcHPRJk4iIqM4zRJ9ORERE96HvdOdhYWHC0tJSrF27Vpw5c0ZMmjRJKJVKkZmZKYQQYsyYMWL27Nna9kePHhXm5uZi8eLFIjExUcyfP/++S4YplUqxbds2ERsbKwYNGnTfJcMCAgJERESE+Ouvv4SPj4/OkmG5ubnC1dVVjBkzRsTHx4uwsDBhY2Mj2ZJhREREhlKTS4bVdJ9eW7ETEREZUk32V3oNLwcqlwu5du0a5s2bh8zMTPj7+2P37t3aiVXS0tIgl/99AT0kJAQ//fQT5s6di3//+9/w8fHB1q1b0a5dO22bt99+G4WFhZg0aRJyc3PRvXt37N69G1ZWVto269evx5QpU9C7d2/I5XIMHToUS5Ys0e63t7fHH3/8gcmTJ6Nz585wdnbGvHnzMGnSJL1PRBAREdUHhujTiYiISJfe63TXZVw7lIiITIEp91emHDsREdUfkq3TTURERERERERVx6KbiIiIiIiIyEBYdBMREREREREZCItuIiIiIiIiIgNh0U1ERERERERkICy6iYiIiIiIiAyERTcRERERERGRgZhLHYAxub1keV5ensSREBERPdjtfup2v2VK2NcSEZEpqMm+lkX3HfLz8wEAXl5eEkdCRET0aPn5+bC3t5c6DL2wryUiIlNSE32tTJjiaXID0Wg0uHLlCmxtbSGTyR7ZPi8vD15eXkhPT4ednV0tRGgYdSUPgLkYo7qSB8BcjFFdyQPQLxchBPLz8+Hh4QG53LTuFNO3rzW0uvQ3dLe6nBvA/ExdXc6vLucG1J/80tLSIJPJaqSv5ZXuO8jlcjRu3Fjvn7Ozs6sTf3B1JQ+AuRijupIHwFyMUV3JA6h6LqZ2hfu26va1hlaX/obuVpdzA5ifqavL+dXl3IC6n5+9vX2N5Wdap8eJiIiIiIiITAiLbiIiIiIiIiIDYdH9GCwtLTF//nxYWlpKHcpjqSt5AMzFGNWVPADmYozqSh5A3crFlNTl970u5wYwP1NXl/Ory7kBzK86OJEaERERERERkYHwSjcRERERERGRgbDoJiIiIiIiIjIQFt1EREREREREBsKim4iIiIiIiMhAWHRXwdmzZzFo0CA4OzvDzs4O3bt3x8GDB3XapKWlYeDAgbCxsYGLiwveeustqNVqnTaHDh1Cp06dYGlpiZYtW2Lt2rW1mMXfdu7ciaCgIFhbW8PBwQGDBw/W2W9KuZSWlsLf3x8ymQynT5/W2RcbG4snn3wSVlZW8PLywqJFi+75+U2bNsHPzw9WVlZo3749du3aVUuRV7p48SImTJiAZs2awdraGi1atMD8+fNRVlam084UcnmQZcuWoWnTprCyskJQUBBOnDghdUg6FixYgC5dusDW1hYuLi4YPHgwkpOTddqUlJRg8uTJcHJyQsOGDTF06FBkZWXptKnK56Y2LVy4EDKZDDNmzNBuM6U8Ll++jNGjR8PJyQnW1tZo3749IiMjtfuFEJg3bx7c3d1hbW2N0NBQnDt3Tuc5cnJyMGrUKNjZ2UGpVGLChAkoKCio1TwqKirw3nvv6XzG//vf/+LOOUxNJZe6Qt/3MicnB1OnToWvry+sra3h7e2NadOmQaVS1WLUVVedv5VVq1ahV69esLOzg0wmQ25ubu0EWwX69iHG2hc+iD75JSQkYOjQoWjatClkMhm+/PLL2gu0GvTJbfXq1XjyySfh4OAABwcHhIaGGt3xwt30yW/Lli0IDAyEUqlEgwYN4O/vjx9++KEWo9VfdY/fwsLCIJPJ7qkvjI0++a1duxYymUznYWVlpd8LCnokHx8fMWDAABETEyPOnj0r3njjDWFjYyOuXr0qhBBCrVaLdu3aidDQUBEdHS127dolnJ2dxZw5c7TPkZqaKmxsbMSbb74pzpw5I5YuXSrMzMzE7t27azWXzZs3CwcHB7FixQqRnJwsEhISxM8//6zdb0q5CCHEtGnTRP/+/QUAER0drd2uUqmEq6urGDVqlIiPjxcbNmwQ1tbWYuXKldo2R48eFWZmZmLRokXizJkzYu7cucLCwkLExcXVWvy///67ePnll8WePXvE+fPnxbZt24SLi4uYOXOmyeVyP2FhYUKhUIjvvvtOJCQkiIkTJwqlUimysrIkjetOffv2FWvWrBHx8fHi9OnTYsCAAcLb21sUFBRo27z22mvCy8tL7N+/X0RGRopu3bqJkJAQ7f6qfG5q04kTJ0TTpk1Fhw4dxPTp07XbTSWPnJwc0aRJE/Hyyy+LiIgIkZqaKvbs2SNSUlK0bRYuXCjs7e3F1q1bRUxMjHj++edFs2bNRHFxsbZNv379RMeOHcXx48fFkSNHRMuWLcXIkSNrNZePPvpIODk5iR07dogLFy6ITZs2iYYNG4qvvvrK5HKpK/R9L+Pi4sSQIUPE9u3bRUpKiti/f7/w8fERQ4cOrcWoq646fytffPGFWLBggViwYIEAIG7evFk7wT6Cvn2IsfaFD6JvfidOnBCzZs0SGzZsEG5ubuKLL76o3YD1oG9uL730kli2bJmIjo4WiYmJ4uWXXxb29vYiIyOjliOvGn3zO3jwoNiyZYs4c+aMSElJEV9++aVkx85VUd3jtwsXLghPT0/x5JNPikGDBtVOsNWgb35r1qwRdnZ24urVq9pHZmamXq/JovsRrl27JgCIw4cPa7fl5eUJAGLv3r1CCCF27dol5HK5zpu/YsUKYWdnJ0pLS4UQQrz99tuibdu2Os89fPhw0bdv31rIolJ5ebnw9PQU33777QPbmEout2P18/MTCQkJ9xTdy5cvFw4ODtqYhRDinXfeEb6+vtr/v/jii2LgwIE6zxkUFCReffVVg8f+MIsWLRLNmjXT/t+Uc+natauYPHmy9v8VFRXCw8NDLFiwQMKoHi47O1sAEH/++acQQojc3FxhYWEhNm3apG2TmJgoAIjw8HAhRNU+N7UlPz9f+Pj4iL1794qePXtqi25TyuOdd94R3bt3f+B+jUYj3NzcxKeffqrdlpubKywtLcWGDRuEEEKcOXNGABAnT57Utvn999+FTCYTly9fNlzwdxk4cKB45ZVXdLYNGTJEjBo1SghhWrnUBTX1Xm7cuFEoFApRXl5uiDCr7XHzO3jwoFEV3fr2IcbaFz7I4/SRTZo0Meqi+3H7f7VaLWxtbcW6desMFeJjqYnjm4CAADF37lxDhPfYqpOfWq0WISEh4ttvvxXjxo0z6qJb3/zWrFkj7O3tH+s1Obz8EZycnODr64vvv/8ehYWFUKvVWLlyJVxcXNC5c2cAQHh4ONq3bw9XV1ftz/Xt2xd5eXlISEjQtgkNDdV57r59+yI8PLzWcjl16hQuX74MuVyOgIAAuLu7o3///oiPj9e2MZVcsrKyMHHiRPzwww+wsbG5Z394eDh69OgBhUKhE2NycjJu3rypbSN1HvejUqng6Oio/b+p5lJWVoaoqCiduORyOUJDQyV/jx/m9pDR27+DqKgolJeX6+Th5+cHb29vbR5V+dzUlsmTJ2PgwIH3/D2YUh7bt29HYGAghg0bBhcXFwQEBGD16tXa/RcuXEBmZqZOLvb29ggKCtLJRalUIjAwUNsmNDQUcrkcERERtZZLSEgI9u/fj7NnzwIAYmJi8Ndff6F///4ml0tdUFPvpUqlgp2dHczNzQ0RZrXVpb+V6vQhxtgXPoip9pFVURO5FRUVoby8XOd4yFg8bn5CCOzfvx/Jycno0aOHIUOtlurm95///AcuLi6YMGFCbYRZbdXNr6CgAE2aNIGXlxcGDRqk93ERi+5HkMlk2LdvH6Kjo2FrawsrKyt8/vnn2L17NxwcHAAAmZmZOgepALT/z8zMfGibvLw8FBcX10ImQGpqKgDg/fffx9y5c7Fjxw44ODigV69eyMnJeWict/c9rE1t5SKEwMsvv4zXXntN58DiTo+Tx+39UkhJScHSpUvx6quvareZai7Xr19HRUWF0cX1MBqNBjNmzMATTzyBdu3aAah8bxUKBZRKpU7bO/Ooyu+oNoSFheHUqVNYsGDBPftMKY/U1FSsWLECPj4+2LNnD15//XVMmzYN69at04nlYX9bmZmZcHFx0dlvbm4OR0fHWs1l9uzZGDFiBPz8/GBhYYGAgADMmDEDo0aN0sZ5O/Y7GWMudUFNvJfXr1/Hf//7X0yaNMkQIT6WuvS3Up0+xBj7wgcxxT6yqmoit3feeQceHh73nEQxBtXNT6VSoWHDhlAoFBg4cCCWLl2KZ555xtDh6q06+f3111/43//+p3OC3FhVJz9fX19899132LZtG3788UdoNBqEhIQgIyOjyq9bb4vu2bNn33ND/N2PpKQkCCEwefJkuLi44MiRIzhx4gQGDx6M5557DlevXpU6DQBVz0Wj0QAA3n33XQwdOhSdO3fGmjVrIJPJsGnTJomzqHoeS5cuRX5+PubMmSN1yA9U1VzudPnyZfTr1w/Dhg3DxIkTJYq8fps8eTLi4+MRFhYmdSh6S09Px/Tp07F+/Xr9J/cwMhqNBp06dcLHH3+MgIAATJo0CRMnTsQ333wjdWh627hxI9avX4+ffvoJp06dwrp167B48WLtCQSqGdX5zq2OvLw8DBw4EG3atMH777//+IFXUW3lR2QMFi5ciLCwMPz6668m35/dydbWFqdPn8bJkyfx0Ucf4c0338ShQ4ekDuux5efnY8yYMVi9ejWcnZ2lDscggoODMXbsWPj7+6Nnz57YsmULGjVqhJUrV1b5OYxrXFQtmjlzJl5++eWHtmnevDkOHDiAHTt24ObNm7CzswMALF++HHv37sW6deswe/ZsuLm53TPj3e0Zgd3c3LT/3j1LcFZWFuzs7GBtbV0rudw+SdCmTRvtdktLSzRv3hxpaWnaOKXKRZ/fSXh4OCwtLXX2BQYGYtSoUVi3bt0DY6xKHrf3P46q5nLblStX8NRTTyEkJASrVq3SaSd1LtXl7OwMMzMzo4vrQaZMmYIdO3bg8OHDaNy4sXa7m5sbysrKkJubq3OV+M48qvK5MbSoqChkZ2ejU6dO2m0VFRU4fPgwvv76a+zZs8ck8gAAd3d3ne8pAGjdujV++eUXnViysrLg7u6ubZOVlQV/f39tm+zsbJ3nUKvVyMnJqdVc3nrrLe3VbgBo3749Ll26hAULFmDcuHEmlYsxq+p37uO8l/n5+ejXrx9sbW3x66+/wsLC4nHDrrLayM/YVKcPMca+8EFMrY/Ux+PktnjxYixcuBD79u1Dhw4dDBlmtVU3P7lcjpYtWwIA/P39kZiYiAULFqBXr16GDFdv+uZ3/vx5XLx4Ec8995x22+0Lfebm5khOTkaLFi0MG7QeauKzd3vkWkpKStVf+LHuCK8Htm/fLuRyucjPz9fZ3qpVK/HRRx8JIf6efOjOGe9Wrlwp7OzsRElJiRCicvKxdu3a6TzHyJEja3XyMZVKJSwtLXUmUisrKxMuLi7ambBNIZdLly6JuLg47WPPnj0CgNi8ebNIT08XQvw9+VhZWZn25+bMmXPP5GPPPvusznMHBwfX+oQrGRkZwsfHR4wYMUKo1ep79ptSLnfr2rWrmDJlivb/FRUVwtPT06gmUtNoNGLy5MnCw8NDnD179p79tycg27x5s3ZbUlLSfScge9jnxtDy8vJ0PhdxcXEiMDBQjB49WsTFxZlMHkJUfp/cPZHajBkzRHBwsBDi78nHFi9erN1/+/vt7snHIiMjtW327NlT65OPOTo6iuXLl+ts+/jjj4WPj48QwrRyqQuq+16qVCrRrVs30bNnT1FYWFgboVbL4/6tGONEavr0IcbaFz7I4/SRpjCRmr65ffLJJ8LOzk7bJxmzmji+GT9+vOjZs6cBont8+uRXXFx8z/HHoEGDxNNPPy3i4uJqfULZqnjc359arRa+vr7iX//6V5Vfk0X3I1y7dk04OTmJIUOGiNOnT4vk5GQxa9YsYWFhIU6fPi2E+HuZnT59+ojTp0+L3bt3i0aNGt13ma233npLJCYmimXLlkmyVMD06dOFp6en2LNnj0hKShITJkwQLi4uIicnx+Ryue3ChQv3zF6em5srXF1dxZgxY0R8fLwICwsTNjY29yyzZW5uLhYvXiwSExPF/Pnza31pkYyMDNGyZUvRu3dvkZGRobMUganlcj9hYWHC0tJSrF27Vpw5c0ZMmjRJKJVKvZdZMKTXX39d2Nvbi0OHDum8/0VFRdo2r732mvD29hYHDhwQkZGRIjg4WFsAClG1z40U7py9XAjTyePEiRPC3NxcfPTRR+LcuXNi/fr1wsbGRvz444/aNgsXLhRKpVJs27ZNxMbGikGDBt13ma2AgAAREREh/vrrL+Hj41Pry2yNGzdOeHp6apcM27Jli3B2dhZvv/22yeVSVzzqvczIyBC+vr4iIiJCCFFZcAcFBYn27duLlJQUne+J+50olZq++QkhxNWrV0V0dLRYvXq1dsWW6OhocePGDSlS0HpUHzJmzBgxe/ZsbXtj7QsfRN/8SktLRXR0tIiOjhbu7u5i1qxZIjo6Wpw7d06qFB5I39wWLlwoFAqF2Lx5s85n7O6LXsZC3/w+/vhj8ccff4jz58+LM2fOiMWLFwtzc3OxevVqqVJ4KH3zu5uxz16ub34ffPCBdnnfqKgoMWLECGFlZSUSEhKq/Josuqvg5MmTok+fPsLR0VHY2tqKbt26iV27dum0uXjxoujfv7+wtrYWzs7OYubMmfcsJXLw4EHh7+8vFAqFaN68uVizZk0tZlGprKxMzJw5U7i4uAhbW1sRGhoq4uPjddqYSi633a/oFkKImJgY0b17d2FpaSk8PT3FwoUL7/nZjRs3ilatWgmFQiHatm0rdu7cWUtRV1qzZo0AcN/HnUwhlwdZunSp8Pb2FgqFQnTt2lUcP35c6pB0POj9v/Nvuri4WLzxxhvCwcFB2NjYiBdeeEHnxIgQVfvc1La7i25TyuO3334T7dq1E5aWlsLPz0+sWrVKZ79GoxHvvfeecHV1FZaWlqJ3794iOTlZp82NGzfEyJEjRcOGDYWdnZ0YP358rR/A5eXlienTpwtvb29hZWUlmjdvLt59912dM/+mkktd8aj38nafcvDgQSHE31d/7/e4cOGCNEk8hL75CSHE/PnzH/k9KJWH9SE9e/YU48aN02lvrH3hg+iT3+3f3d0PY71aqk9uTZo0uW9u8+fPr/3Aq0if/N59913RsmVLYWVlJRwcHERwcLAICwuTIOqq0/ezdydjL7qF0C+/GTNmaNu6urqKAQMGiFOnTun1ejIhhKj6YHQiIiIiIiIiqqp6O3s5ERERERERkaGx6CYiIiIiIiIyEBbdRERERERERAbCopuIiIiIiIjIQFh0ExERERERERkIi24iIiIiIiIiA2HRTURERERERGQgLLqJiIiIiIiIDIRFNxEREREREZGBsOgmIiIiIiIiMhAW3UREREREREQGwqKbiIiIiIiIyED+D/Jz+R5J6H7vAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generalized Method of Moments"
      ],
      "metadata": {
        "id": "lcFiF37l_B4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionGMM:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Generalized Method of Moments (GMM).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using GMM.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def moment_conditions(self, beta):\n",
        "        \"\"\"Defines the moment conditions for GMM.\"\"\"\n",
        "        probabilities = self.sigmoid(np.dot(self.X, beta))\n",
        "        moments = np.dot(self.X.T, self.y - probabilities)\n",
        "        return moments\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using GMM.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = root(self.moment_conditions, initial_beta, method='hybr')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "XFlx48LWCSRH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized Method of Moments\n",
        "gmm_model = LogisticRegressionGMM(X_train, y_train)\n",
        "gmm_model.fit()\n",
        "accuracy_gmm, log_loss_gmm = gmm_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (GMM): {accuracy_gmm:.4f}\")\n",
        "print(f\"Log Loss (GMM): {log_loss_gmm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYWcsnGCTK1",
        "outputId": "66b5c12c-a2b0-4c9d-f0ed-bc5b532576e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (GMM): 0.8400\n",
            "Log Loss (GMM): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quasi-Likelihood Estimation"
      ],
      "metadata": {
        "id": "W79u0EnhCTIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionQLE:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Quasi-Likelihood Estimation (QLE).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using QLE.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def quasi_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Defines the quasi-likelihood function.\n",
        "        This can be modified based on the specific quasi-likelihood being used.\n",
        "        \"\"\"\n",
        "        probabilities = self.sigmoid(np.dot(self.X, beta))\n",
        "\n",
        "        # Quasi-likelihood function\n",
        "        return -np.sum(self.y * np.log(probabilities) + (1 - self.y) * np.log(1 - probabilities))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using QLE.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = minimize(self.quasi_likelihood, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "wJyN12XGCTFo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quasi-Likelihood Estimation\n",
        "qle_model = LogisticRegressionQLE(X_train, y_train)\n",
        "qle_model.fit()\n",
        "accuracy_qle, log_loss_qle = qle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (QLE): {accuracy_qle:.4f}\")\n",
        "print(f\"Log Loss (QLE): {log_loss_qle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDnkPT5CTDB",
        "outputId": "baec4e5e-a8e9-498d-c41e-4742fce78502"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (QLE): 0.8400\n",
            "Log Loss (QLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LogitBoost"
      ],
      "metadata": {
        "id": "xv9sMSCQCTAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionLogitBoost:\n",
        "    \"\"\"\n",
        "    Logistic Regression using LogitBoost.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    model : AdaBoostClassifier\n",
        "        The LogitBoost model (AdaBoost with decision stumps as weak learners).\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the LogitBoost model to the training data.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.model = AdaBoostClassifier(\n",
        "            estimator=DecisionTreeClassifier(max_depth=1),\n",
        "            n_estimators=n_estimators,\n",
        "            algorithm='SAMME.R'\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the LogitBoost model to the training data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        return self.model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return self.model.predict(X_new)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "m3sbDjo2CS90"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LogitBoost\n",
        "logitboost_model = LogisticRegressionLogitBoost(n_estimators=50)\n",
        "logitboost_model.fit(X_train, y_train)\n",
        "accuracy_logitboost, log_loss_logitboost = logitboost_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (LogitBoost): {accuracy_logitboost:.4f}\")\n",
        "print(f\"Log Loss (LogitBoost): {log_loss_logitboost:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXZVIyBmCS7N",
        "outputId": "39010c83-adcc-4c49-808a-cea1f70375c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (LogitBoost): 0.8633\n",
            "Log Loss (LogitBoost): 0.6291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Empirical Likelihood"
      ],
      "metadata": {
        "id": "jCXGa50ICS4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionEL:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Empirical Likelihood (EL).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using EL.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def el_objective(self, beta):\n",
        "        \"\"\"\n",
        "        Objective function for Empirical Likelihood.\n",
        "        It maximizes the empirical likelihood subject to constraints.\n",
        "        \"\"\"\n",
        "        probabilities = self.sigmoid(np.dot(self.X, beta))\n",
        "        log_weights = np.log(probabilities * self.y + (1 - probabilities) * (1 - self.y))\n",
        "        return -np.sum(log_weights)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Empirical Likelihood.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = minimize(self.el_objective, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "Aohqktm4CS1v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empirical Likelihood\n",
        "el_model = LogisticRegressionEL(X_train, y_train)\n",
        "el_model.fit()\n",
        "accuracy_el, log_loss_el = el_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (EL): {accuracy_el:.4f}\")\n",
        "print(f\"Log Loss (EL): {log_loss_el:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWVt2QEaCSy5",
        "outputId": "b12579f1-74f0-4320-d0ce-6c481c92a2f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (EL): 0.8400\n",
            "Log Loss (EL): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudo-Likelihood"
      ],
      "metadata": {
        "id": "MYfpa48PDS4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionPseudolikelihood:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Pseudolikelihood.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using the pseudolikelihood approach.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def pseudolikelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the pseudolikelihood function.\n",
        "        Approximates the full likelihood by considering individual conditional probabilities.\n",
        "        \"\"\"\n",
        "        probabilities = self.sigmoid(np.dot(self.X, beta))\n",
        "        pseudo_log_likelihood = np.sum(self.y * np.log(probabilities) + (1 - self.y) * np.log(1 - probabilities))\n",
        "        return -pseudo_log_likelihood\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using the pseudolikelihood approach.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = minimize(self.pseudolikelihood, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "_v8ewkj7DgzC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo-Likelihood\n",
        "pl_model = LogisticRegressionPseudolikelihood(X_train, y_train)\n",
        "pl_model.fit()\n",
        "accuracy_pl, log_loss_pl = pl_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Pseudo-likelihood): {accuracy_pl:.4f}\")\n",
        "print(f\"Log Loss (Pseudo-likelihood): {log_loss_pl:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2mTr9cBDk4p",
        "outputId": "b6b3d765-cc9b-45c7-95a1-eed718776360"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Pseudo-likelihood): 0.8400\n",
            "Log Loss (Pseudo-likelihood): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penalized Likelihood"
      ],
      "metadata": {
        "id": "t1XC0ddYDr-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionPenalized:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Penalized Likelihood (L1, L2, or Elastic Net).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    penalty : str\n",
        "        The type of penalty ('l1', 'l2', or 'elasticnet').\n",
        "    alpha : float\n",
        "        The regularization strength.\n",
        "    l1_ratio : float\n",
        "        The mixing parameter for Elastic Net (used only if penalty is 'elasticnet').\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using the specified penalized likelihood.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, penalty='l2', alpha=1.0, l1_ratio=0.5):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.penalty = penalty\n",
        "        self.alpha = alpha\n",
        "        self.l1_ratio = l1_ratio\n",
        "        self.beta = None\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using the specified penalized likelihood.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        model = SklearnPenalizedLogisticRegression(\n",
        "            penalty=self.penalty,\n",
        "            C=1.0/self.alpha,\n",
        "            solver='saga',\n",
        "            l1_ratio=self.l1_ratio if self.penalty == 'elasticnet' else None,\n",
        "            max_iter=10000\n",
        "        )\n",
        "        model.fit(self.X, self.y)\n",
        "        self.beta = model.coef_\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        model = SklearnPenalizedLogisticRegression(\n",
        "            penalty=self.penalty,\n",
        "            C=1.0/self.alpha,\n",
        "            solver='saga',\n",
        "            l1_ratio=self.l1_ratio if self.penalty == 'elasticnet' else None,\n",
        "            max_iter=10000\n",
        "        )\n",
        "        model.fit(self.X, self.y)\n",
        "        return model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "HfzyB9HPD5Gp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penalized Likelihood\n",
        "penalized_model = LogisticRegressionPenalized(X_train, y_train, penalty='elasticnet', alpha=0.1, l1_ratio=0.5)\n",
        "penalized_model.fit()\n",
        "accuracy_penalized, log_loss_penalized = penalized_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Penalized Likelihood): {accuracy_penalized:.4f}\")\n",
        "print(f\"Log Loss (Penalized Likelihood): {log_loss_penalized:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gr4ALGQD-KN",
        "outputId": "bed50184-f21b-4df2-b24f-9e0d2d3024d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Penalized Likelihood): 0.8433\n",
            "Log Loss (Penalized Likelihood): 0.3575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Chain Monte Carlo"
      ],
      "metadata": {
        "id": "UjD8lu9MED-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymc"
      ],
      "metadata": {
        "id": "SDGUrhAaEI3E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "from scipy.special import expit\n",
        "\n",
        "class LogisticRegressionMCMC:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Markov Chain Monte Carlo (MCMC) for Bayesian inference.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    trace : pm.backends.base.MultiTrace or None\n",
        "        The trace of the sampled posterior distribution.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using MCMC.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data based on posterior samples.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        Initializes the LogisticRegressionMCMC class with the feature matrix and target vector.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.trace = None\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MCMC.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        trace : pm.backends.base.MultiTrace\n",
        "            The trace of the sampled posterior distribution.\n",
        "        \"\"\"\n",
        "        with pm.Model() as logistic_model:\n",
        "            # Priors for the coefficients\n",
        "            beta = pm.Normal('beta', mu=0, sigma=10, shape=self.X.shape[1])\n",
        "            intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
        "\n",
        "            # Logistic regression model\n",
        "            p = pm.math.sigmoid(pm.math.dot(self.X, beta) + intercept)\n",
        "            likelihood = pm.Bernoulli('y', p=p, observed=self.y)\n",
        "\n",
        "            # Sampling from the posterior\n",
        "            self.trace = pm.sample(1000, tune=1000, return_inferencedata=False, cores=1)\n",
        "\n",
        "        return self.trace\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for new data based on posterior samples.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            New data for which to predict probabilities.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        p_mean : np.ndarray\n",
        "            Predicted probabilities for each sample in X_new.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            beta_samples = self.trace['beta']\n",
        "            intercept_samples = self.trace['intercept']\n",
        "\n",
        "            print(\"Debugging shapes:\")\n",
        "            print(f\"X_new shape: {X_new.shape}\")\n",
        "            print(f\"beta_samples shape: {beta_samples.shape}\")\n",
        "            print(f\"intercept_samples shape: {intercept_samples.shape}\")\n",
        "\n",
        "            # Compute the mean of the predicted probabilities across samples\n",
        "            p_samples = expit(np.dot(X_new, beta_samples.T) + intercept_samples)\n",
        "\n",
        "            print(\"p_samples shape:\", p_samples.shape)\n",
        "            p_mean = np.mean(p_samples, axis=1)  # Compute the mean across the samples (axis=1)\n",
        "\n",
        "            return p_mean\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during probability prediction: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts binary class labels for new data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            New data for which to predict binary class labels.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            Predicted binary class labels (0 or 1) for each sample in X_new.\n",
        "        \"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            Test data for which to evaluate the model.\n",
        "        y_test : np.ndarray\n",
        "            True labels for the test data.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        accuracy : float\n",
        "            Accuracy of the model on the test data.\n",
        "        log_loss_val : float\n",
        "            Log loss of the model on the test data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            y_pred_proba = self.predict_proba(X_test)\n",
        "            y_pred = self.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "            return accuracy, log_loss_val\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during evaluation: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "06j4Ixp_ENso"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Markov Chain Monte Carlo\n",
        "mcmc_model = LogisticRegressionMCMC(X_train, y_train)\n",
        "mcmc_model.fit()\n",
        "accuracy_mcmc, log_loss_mcmc = mcmc_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy (MCMC): {accuracy_mcmc:.4f}\")\n",
        "print(f\"Log Loss (MCMC): {log_loss_mcmc:.4f}\")"
      ],
      "metadata": {
        "id": "r2fI93wuEUoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "721b3f4c-9ca1-453f-9fa1-81a13eda2d86"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [2000/2000 01:03&lt;00:00 Sampling chain 0, 0 divergences]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [2000/2000 01:03&lt;00:00 Sampling chain 1, 0 divergences]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debugging shapes:\n",
            "X_new shape: (300, 10)\n",
            "beta_samples shape: (2000, 10)\n",
            "intercept_samples shape: (2000,)\n",
            "p_samples shape: (300, 2000)\n",
            "Debugging shapes:\n",
            "X_new shape: (300, 10)\n",
            "beta_samples shape: (2000, 10)\n",
            "intercept_samples shape: (2000,)\n",
            "p_samples shape: (300, 2000)\n",
            "\n",
            "Accuracy (MCMC): 0.8400\n",
            "Log Loss (MCMC): 0.3577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Inference"
      ],
      "metadata": {
        "id": "IKEM6PbIEaox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "from scipy.special import expit\n",
        "\n",
        "class LogisticRegressionVariationalInference:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Variational Inference (VI) for Bayesian inference.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    approx : pm.Approximation or None\n",
        "        The approximation of the posterior distribution using variational inference.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using VI.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data based on the VI approximation.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        Initializes the LogisticRegressionVariationalInference class with the feature matrix and target vector.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.approx = None\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Variational Inference.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        approx : pm.Approximation\n",
        "            The approximation of the posterior distribution using VI.\n",
        "        \"\"\"\n",
        "        with pm.Model() as logistic_model:\n",
        "            # Priors for the coefficients\n",
        "            beta = pm.Normal('beta', mu=0, sigma=10, shape=self.X.shape[1])\n",
        "            intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
        "\n",
        "            # Logistic regression model\n",
        "            p = pm.math.sigmoid(pm.math.dot(self.X, beta) + intercept)\n",
        "            likelihood = pm.Bernoulli('y', p=p, observed=self.y)\n",
        "\n",
        "            # Variational inference approximation\n",
        "            self.approx = pm.fit(10000, method='advi')\n",
        "\n",
        "        return self.approx\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for new data based on the VI approximation.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            New data for which to predict probabilities.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        p_mean : np.ndarray or None\n",
        "            Predicted probabilities for each sample in X_new. Returns None if an error occurs.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            posterior = self.approx.sample(1000)\n",
        "\n",
        "            # Access the posterior samples from the InferenceData object\n",
        "            beta_samples = np.squeeze(posterior.posterior['beta'].values)\n",
        "            intercept_samples = np.squeeze(posterior.posterior['intercept'].values)\n",
        "\n",
        "            print(f\"X_new shape: {X_new.shape}\")\n",
        "            print(f\"beta_samples shape: {beta_samples.shape}\")\n",
        "            print(f\"intercept_samples shape: {intercept_samples.shape}\")\n",
        "\n",
        "            # Compute the mean of the predicted probabilities across samples\n",
        "            p_samples = expit(np.dot(X_new, beta_samples.T) + intercept_samples)\n",
        "\n",
        "            print(f\"p_samples shape: {p_samples.shape}\")\n",
        "\n",
        "            p_mean = np.mean(p_samples, axis=1)\n",
        "            return p_mean\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during probability prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts binary class labels for new data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            New data for which to predict binary class labels.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray or None\n",
        "            Predicted binary class labels (0 or 1) for each sample in X_new. Returns None if an error occurs.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X_new)\n",
        "        if proba is not None:\n",
        "            return (proba >= 0.5).astype(int)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            Test data for which to evaluate the model.\n",
        "        y_test : np.ndarray\n",
        "            True labels for the test data.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        accuracy : float or None\n",
        "            Accuracy of the model on the test data. Returns None if an error occurs.\n",
        "        log_loss_val : float or None\n",
        "            Log loss of the model on the test data. Returns None if an error occurs.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            y_pred_proba = self.predict_proba(X_test)\n",
        "            if y_pred_proba is None:\n",
        "                raise ValueError(\"Probability prediction failed.\")\n",
        "\n",
        "            y_pred = self.predict(X_test)\n",
        "            if y_pred is None:\n",
        "                raise ValueError(\"Label prediction failed.\")\n",
        "\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "            return accuracy, log_loss_val\n",
        "        except Exception as e:\n",
        "            print(f\"Error during evaluation: {e}\")\n",
        "            return None, None"
      ],
      "metadata": {
        "id": "ZGR6k5QDEc4L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variational Inference\n",
        "vi_model = LogisticRegressionVariationalInference(X_train, y_train)\n",
        "vi_model.fit()\n",
        "accuracy_vi, log_loss_vi = vi_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy (VI): {accuracy_vi:.4f}\")\n",
        "print(f\"Log Loss (VI): {log_loss_vi:.4f}\")"
      ],
      "metadata": {
        "id": "gLy5k6LYEgyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "9c62d164-9e67-41cc-d591-3cb9144d72c4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10000/10000 00:01&lt;00:00 Average Loss = 275.64]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_new shape: (300, 10)\n",
            "beta_samples shape: (1000, 10)\n",
            "intercept_samples shape: (1000,)\n",
            "p_samples shape: (300, 1000)\n",
            "X_new shape: (300, 10)\n",
            "beta_samples shape: (1000, 10)\n",
            "intercept_samples shape: (1000,)\n",
            "p_samples shape: (300, 1000)\n",
            "\n",
            "Accuracy (VI): 0.8467\n",
            "Log Loss (VI): 0.3577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteratively Re-Weighted"
      ],
      "metadata": {
        "id": "mRZ0QDbLElfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionIRLS:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Iteratively Reweighted Least Squares (IRLS).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using IRLS.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, tol=1e-6, max_iter=100, lambda_=0.01):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.lambda_ = lambda_  # Regularization parameter\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        z = np.clip(z, -500, 500)  # Clip z to avoid overflow in exp\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using IRLS.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = self.X.shape\n",
        "        X = np.c_[np.ones(n_samples), self.X]  # Add intercept term\n",
        "        self.beta = np.zeros(X.shape[1])  # Initialize beta with the correct size\n",
        "\n",
        "        I = np.eye(X.shape[1])  # Identity matrix for regularization\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            beta_old = self.beta.copy()\n",
        "            p = self.sigmoid(np.dot(X, self.beta))\n",
        "            W = np.diag(p * (1 - p))  # Weight matrix\n",
        "\n",
        "            # Add ridge regularization to avoid singular matrix\n",
        "            XTWX = np.dot(X.T, np.dot(W, X)) + self.lambda_ * I\n",
        "            XTWy = np.dot(X.T, np.dot(W, np.dot(X, self.beta) + np.linalg.inv(W).dot(self.y - p)))\n",
        "            self.beta = np.linalg.solve(XTWX, XTWy)\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(self.beta - beta_old, ord=1) < self.tol:\n",
        "                break\n",
        "\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        X_new = np.c_[np.ones(X_new.shape[0]), X_new]  # Add intercept term\n",
        "        return self.sigmoid(np.dot(X_new, self.beta))\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "LbPIsjH1Eo4d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteratively Re-weighted algorithm\n",
        "irls_model = LogisticRegressionIRLS(X_train, y_train)\n",
        "irls_model.fit()\n",
        "accuracy_irls, log_loss_irls = irls_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (IRLS): {accuracy_irls:.4f}\")\n",
        "print(f\"Log Loss (IRLS): {log_loss_irls:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3m5JulcEtIf",
        "outputId": "7edafdab-7a3f-47c3-c7d9-cfdbfe19c4f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (IRLS): 0.8400\n",
            "Log Loss (IRLS): 0.3576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Maximum Likelihood"
      ],
      "metadata": {
        "id": "BOEPi2iLFBwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionTikuMML:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Tiku's Modified Maximum Likelihood (MML).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using Tiku's MML.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, tol=1e-6, max_iter=100):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def modified_log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the modified log-likelihood function.\n",
        "        This includes a robustness modification based on Tiku's method.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "\n",
        "        # Adding a small constant to prevent overflow/underflow\n",
        "        epsilon = 1e-8\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "\n",
        "        # Modified log-likelihood function\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p)) + np.sum(np.abs(beta))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Tiku's MML.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        initial_beta = np.zeros(self.X.shape[1])\n",
        "        result = minimize(self.modified_log_likelihood, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "tTqOsut2E1xa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiku's Modified Maximum Likelihood\n",
        "tiku_mml_model = LogisticRegressionTikuMML(X_train, y_train)\n",
        "tiku_mml_model.fit()\n",
        "accuracy_tiku_mml, log_loss_tiku_mml = tiku_mml_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Tiku's MML): {accuracy_tiku_mml:.4f}\")\n",
        "print(f\"Log Loss (Tiku's MML): {log_loss_tiku_mml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRtfvL3NFIQ7",
        "outputId": "b2c4b733-ec1e-4faf-c2a4-343b240c7a95"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Tiku's MML): 0.8433\n",
            "Log Loss (Tiku's MML): 0.3545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nested Pseudo-MLE"
      ],
      "metadata": {
        "id": "iSuUNtGCFPB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNPML:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Nested Pseudo Maximum Likelihood (NPML).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using Nested Pseudo Maximum Likelihood.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, tol=1e-6, max_iter=100):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def nested_log_likelihood(self, beta_subset, subset):\n",
        "        \"\"\"\n",
        "        Computes the nested log-likelihood function for a subset of parameters.\n",
        "        \"\"\"\n",
        "        # Create a full beta vector with the current subset\n",
        "        beta = self.beta.copy()\n",
        "        beta[subset] = beta_subset\n",
        "\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Nested Pseudo Maximum Likelihood.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        self.beta = np.zeros(n_features)\n",
        "\n",
        "        # Split the feature indices into two subsets\n",
        "        subset1 = np.arange(0, n_features // 2)\n",
        "        subset2 = np.arange(n_features // 2, n_features)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # Optimize over the first subset of parameters\n",
        "            result1 = minimize(self.nested_log_likelihood, self.beta[subset1], args=(subset1,), method='BFGS')\n",
        "            self.beta[subset1] = result1.x\n",
        "\n",
        "            # Optimize over the second subset of parameters\n",
        "            result2 = minimize(self.nested_log_likelihood, self.beta[subset2], args=(subset2,), method='BFGS')\n",
        "            self.beta[subset2] = result2.x\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(result1.x - self.beta[subset1], ord=1) < self.tol and \\\n",
        "               np.linalg.norm(result2.x - self.beta[subset2], ord=1) < self.tol:\n",
        "                break\n",
        "\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "117zEvI_FbOc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nested Pseudo Maximum Likelihood (NPML)\n",
        "npml_model = LogisticRegressionNPML(X_train, y_train)\n",
        "npml_model.fit()\n",
        "accuracy_npml, log_loss_npml = npml_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (NPML): {accuracy_npml:.4f}\")\n",
        "print(f\"Log Loss (NPML): {log_loss_npml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH2ROmwnFbx5",
        "outputId": "45e42fac-8cf3-4f00-e10c-ce4fb58fb6d1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (NPML): 0.8400\n",
            "Log Loss (NPML): 0.3551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Satterthwaite"
      ],
      "metadata": {
        "id": "iWl14Jl1FuPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionSatterthwaiteMLE:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Satterthwaite-Adjusted Maximum Likelihood Estimation.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using Satterthwaite-Adjusted MLE.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    compute_confidence_intervals():\n",
        "        Computes Satterthwaite-adjusted confidence intervals for the estimated coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_satterthwaite_adjustment(self, beta):\n",
        "        \"\"\"Computes Satterthwaite adjustment to standard errors.\"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        # Add regularization to ensure the Hessian is positive definite\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If inversion fails, return NaNs to signal a problem\n",
        "            se = np.full(self.X.shape[1], np.nan)\n",
        "\n",
        "        # Satterthwaite adjustment\n",
        "        adjustment = 1 + (np.trace(hessian_inv @ hessian) / (2 * len(self.y)))\n",
        "        adjusted_se = se * adjustment\n",
        "\n",
        "        return adjusted_se\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"Computes Satterthwaite-adjusted confidence intervals for the estimated coefficients.\"\"\"\n",
        "        adjusted_se = self.compute_satterthwaite_adjustment(beta)\n",
        "        z_score = norm.ppf(0.975)  # For 95% confidence interval\n",
        "        lower_bound = beta - z_score * adjusted_se\n",
        "        upper_bound = beta + z_score * adjusted_se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using Satterthwaite-Adjusted MLE.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        initial_beta = np.zeros(n_features)\n",
        "\n",
        "        result = minimize(self.log_likelihood, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "RJ8qyT6YFhe1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Satterthwaite-Adjusted MLE\n",
        "satterthwaite_mle_model = LogisticRegressionSatterthwaiteMLE(X_train, y_train)\n",
        "satterthwaite_mle_model.fit()\n",
        "accuracy_satterthwaite_mle, log_loss_satterthwaite_mle = satterthwaite_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Satterthwaite-MLE): {accuracy_satterthwaite_mle:.4f}\")\n",
        "print(f\"Log Loss (Satterthwaite-MLE): {log_loss_satterthwaite_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDUKwmFeFs_a",
        "outputId": "5f629dc2-c907-488b-d145-df12d0890e52"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Satterthwaite-MLE): 0.8400\n",
            "Log Loss (Satterthwaite-MLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genetic Algorithm"
      ],
      "metadata": {
        "id": "Cc7I6LuCF24t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q deap"
      ],
      "metadata": {
        "id": "SiI2f97KGSxM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "class LogisticRegressionGeneticAlgorithm:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation optimized via Genetic Algorithm.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using MLE optimized via Genetic Algorithm.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    compute_confidence_intervals():\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, population_size=50, generations=100, cxpb=0.5, mutpb=0.2, elitism=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.cxpb = cxpb  # Crossover probability\n",
        "        self.mutpb = mutpb  # Mutation probability\n",
        "        self.elitism = elitism  # Whether to apply elitism\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"Computes confidence intervals for the estimated coefficients.\"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        # Regularize Hessian to ensure it's invertible\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            se = np.full(self.X.shape[1], np.nan)  # Handle singular matrix\n",
        "\n",
        "        z_score = norm.ppf(0.975)  # For 95% confidence interval\n",
        "        lower_bound = beta - z_score * se\n",
        "        upper_bound = beta + z_score * se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MLE optimized via Genetic Algorithm.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "\n",
        "        # Check if classes already exist and delete them if they do\n",
        "        if hasattr(creator, \"FitnessMin\"):\n",
        "            del creator.FitnessMin\n",
        "        if hasattr(creator, \"Individual\"):\n",
        "            del creator.Individual\n",
        "\n",
        "        # Define the fitness function\n",
        "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "        creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin)\n",
        "\n",
        "        toolbox = base.Toolbox()\n",
        "        toolbox.register(\"attr_float\", np.random.uniform, -10, 10)\n",
        "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=n_features)\n",
        "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "        def eval_fitness(individual):\n",
        "            log_lik = self.log_likelihood(np.array(individual))\n",
        "            ci = self.compute_confidence_intervals(np.array(individual))\n",
        "            ci_width = np.mean(ci[:, 1] - ci[:, 0])\n",
        "            return log_lik + 0.01 * ci_width,  # Penalize wide confidence intervals\n",
        "\n",
        "        toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "        toolbox.register(\"evaluate\", eval_fitness)\n",
        "\n",
        "        pop = toolbox.population(n=self.population_size)\n",
        "\n",
        "        if self.elitism:\n",
        "            hof = tools.HallOfFame(1, similar=np.array_equal)  # Use np.array_equal to compare individuals\n",
        "            algorithms.eaSimple(pop, toolbox, cxpb=self.cxpb, mutpb=self.mutpb, ngen=self.generations,\n",
        "                                halloffame=hof, verbose=False)\n",
        "            best_individual = hof[0]\n",
        "        else:\n",
        "            algorithms.eaSimple(pop, toolbox, cxpb=self.cxpb, mutpb=self.mutpb, ngen=self.generations, verbose=False)\n",
        "            best_individual = tools.selBest(pop, 1)[0]\n",
        "\n",
        "        self.beta = np.array(best_individual)\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "DlymPJ-rGHq8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genetic Algorithm MLE\n",
        "ga_mle_model = LogisticRegressionGeneticAlgorithm(X_train, y_train)\n",
        "ga_mle_model.fit()\n",
        "accuracy_ga_mle, log_loss_ga_mle = ga_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (GA-MLE): {accuracy_ga_mle:.4f}\")\n",
        "print(f\"Log Loss (GA-MLE): {log_loss_ga_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfL7cd16GIK4",
        "outputId": "338a2c4c-298c-428d-dbf0-6ee288a1ef27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (GA-MLE): 0.8367\n",
            "Log Loss (GA-MLE): 0.3556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nelder Mead"
      ],
      "metadata": {
        "id": "-WBtyigpGNBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNelderMead:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation optimized via Nelder-Mead Algorithm.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using MLE optimized via Nelder-Mead Algorithm.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    compute_confidence_intervals():\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"Computes confidence intervals for the estimated coefficients.\"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        # Regularize Hessian to ensure it's invertible\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            se = np.full(self.X.shape[1], np.nan)  # Handle singular matrix\n",
        "\n",
        "        z_score = norm.ppf(0.975)  # For 95% confidence interval\n",
        "        lower_bound = beta - z_score * se\n",
        "        upper_bound = beta + z_score * se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MLE optimized via Nelder-Mead Algorithm.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        initial_beta = np.zeros(n_features)\n",
        "\n",
        "        def objective_function(beta):\n",
        "            return self.log_likelihood(beta)\n",
        "\n",
        "        # Use Nelder-Mead algorithm for optimization\n",
        "        result = minimize(objective_function, initial_beta, method='Nelder-Mead')\n",
        "        self.beta = result.x\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "tDGfGlaoGZTI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nelder-Mead MLE\n",
        "nm_mle_model = LogisticRegressionNelderMead(X_train, y_train)\n",
        "nm_mle_model.fit()\n",
        "accuracy_nm_mle, log_loss_nm_mle = nm_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (NM-MLE): {accuracy_nm_mle:.4f}\")\n",
        "print(f\"Log Loss (NM-MLE): {log_loss_nm_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwFxzaJOGeSD",
        "outputId": "6ba0b87a-65a1-44f1-bd29-6607ce5fb2bf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (NM-MLE): 0.8333\n",
            "Log Loss (NM-MLE): 0.3563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulated Annealing"
      ],
      "metadata": {
        "id": "hd8XTAvHGngz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "class LogisticRegressionSimulatedAnnealing:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation optimized via Simulated Annealing.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using MLE optimized via Simulated Annealing.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    compute_confidence_intervals():\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"Computes confidence intervals for the estimated coefficients.\"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        # Regularize Hessian to ensure it's invertible\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            se = np.full(self.X.shape[1], np.nan)  # Handle singular matrix\n",
        "\n",
        "        z_score = norm.ppf(0.975)  # For 95% confidence interval\n",
        "        lower_bound = beta - z_score * se\n",
        "        upper_bound = beta + z_score * se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MLE optimized via Simulated Annealing.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        bounds = [(-10, 10)] * n_features\n",
        "\n",
        "        def objective_function(beta):\n",
        "            return self.log_likelihood(beta)\n",
        "\n",
        "        # Use Simulated Annealing for optimization\n",
        "        result = dual_annealing(objective_function, bounds)\n",
        "        self.beta = result.x\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "DnHzxJW1GjDu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated Annealing MLE\n",
        "sa_mle_model = LogisticRegressionSimulatedAnnealing(X_train, y_train)\n",
        "sa_mle_model.fit()\n",
        "accuracy_sa_mle, log_loss_sa_mle = sa_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (SA-MLE): {accuracy_sa_mle:.4f}\")\n",
        "print(f\"Log Loss (SA-MLE): {log_loss_sa_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1oUDBEfGmns",
        "outputId": "195b04dd-0241-41c7-e9b4-7fcf49c7224b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (SA-MLE): 0.8400\n",
            "Log Loss (SA-MLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Particle Swarm"
      ],
      "metadata": {
        "id": "O1A9O9xEGt28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyswarm"
      ],
      "metadata": {
        "id": "y4_ujUHCG39D"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyswarm import pso\n",
        "\n",
        "class LogisticRegressionPSO:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation optimized via Particle Swarm Optimization (PSO).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "    maxiter : int\n",
        "        The maximum number of iterations for the PSO algorithm.\n",
        "    minstep : float\n",
        "        The minimum change in the best position for convergence.\n",
        "    minfunc : float\n",
        "        The minimum change in the objective function for convergence.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using MLE optimized via PSO.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    compute_confidence_intervals():\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, maxiter=100, minstep=1e-8, minfunc=1e-8):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "        self.maxiter = maxiter  # Maximum number of iterations for PSO\n",
        "        self.minstep = minstep  # Minimum change in the best position for convergence\n",
        "        self.minfunc = minfunc  # Minimum change in the objective function for convergence\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"Computes confidence intervals for the estimated coefficients.\"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        # Regularize Hessian to ensure it's invertible\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            se = np.full(self.X.shape[1], np.nan)  # Handle singular matrix\n",
        "\n",
        "        z_score = norm.ppf(0.975)  # For 95% confidence interval\n",
        "        lower_bound = beta - z_score * se\n",
        "        upper_bound = beta + z_score * se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MLE optimized via PSO.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        lb = [-10] * n_features  # Lower bounds for each coefficient\n",
        "        ub = [10] * n_features  # Upper bounds for each coefficient\n",
        "\n",
        "        def objective_function(beta):\n",
        "            return self.log_likelihood(beta)\n",
        "\n",
        "        # Use Particle Swarm Optimization for optimization with user-defined maxiter, minstep, and minfunc\n",
        "        self.beta, _ = pso(objective_function, lb, ub, maxiter=self.maxiter, minstep=self.minstep, minfunc=self.minfunc)\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "pRNtRU1UG6JF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PSO MLE\n",
        "pso_mle_model = LogisticRegressionPSO(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    maxiter=1000,\n",
        "    minstep=1e-9,\n",
        "    minfunc=1e-9\n",
        ")\n",
        "pso_mle_model.fit()\n",
        "accuracy_pso_mle, log_loss_pso_mle = pso_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy (PSO-MLE): {accuracy_pso_mle:.4f}\")\n",
        "print(f\"Log Loss (PSO-MLE): {log_loss_pso_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTZYxLDiG-i7",
        "outputId": "367a6b9e-96a8-4b74-ac05-952b163ad6a5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: Swarm best position change less than 1e-09\n",
            "\n",
            "Accuracy (PSO-MLE): 0.8467\n",
            "Log Loss (PSO-MLE): 0.3602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Game Theory"
      ],
      "metadata": {
        "id": "rxtQzK4eHDNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deap import base, creator, tools\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the fitness function outside of the class to make it picklable\n",
        "def eval_fitness(individual, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the fitness of an individual.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    individual : np.ndarray\n",
        "        The individual (solution) to evaluate.\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        The fitness score, where lower is better.\n",
        "    \"\"\"\n",
        "    model = LogisticRegressionGameTheoryOptimization(X, y)\n",
        "    log_lik = model.log_likelihood(np.array(individual))\n",
        "    ci = model.compute_confidence_intervals(np.array(individual))\n",
        "    ci_width = np.mean(ci[:, 1] - ci[:, 0])\n",
        "    return log_lik + 0.01 * ci_width  # Penalize wide confidence intervals\n",
        "\n",
        "class LogisticRegressionGameTheoryOptimization:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation optimized via Game Theory Optimization.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    confidence_intervals : np.ndarray\n",
        "        Confidence intervals for the estimated coefficients.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    sigmoid(z):\n",
        "        Computes the sigmoid function.\n",
        "    log_likelihood(beta):\n",
        "        Computes the negative log-likelihood function.\n",
        "    compute_confidence_intervals(beta):\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "    fit():\n",
        "        Fits the logistic regression model using MLE optimized via Game Theory Optimization.\n",
        "    payoff_matrix(population):\n",
        "        Creates the payoff matrix for the population.\n",
        "    nash_equilibrium_strategy(population, payoff):\n",
        "        Identifies the best strategy using the Nash equilibrium concept.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, population_size=50, generations=100, cxpb=0.5, mutpb=0.2, elitism=True):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model with game theory optimization.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        population_size : int, optional\n",
        "            The size of the population (default is 50).\n",
        "        generations : int, optional\n",
        "            The number of generations (default is 100).\n",
        "        cxpb : float, optional\n",
        "            Crossover probability (default is 0.5).\n",
        "        mutpb : float, optional\n",
        "            Mutation probability (default is 0.2).\n",
        "        elitism : bool, optional\n",
        "            Whether to apply elitism to preserve the best individual (default is True).\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.cxpb = cxpb  # Crossover probability\n",
        "        self.mutpb = mutpb  # Mutation probability\n",
        "        self.elitism = elitism\n",
        "        self.beta = None\n",
        "        self.confidence_intervals = None\n",
        "\n",
        "        # Remove existing DEAP classes if they exist\n",
        "        if hasattr(creator, \"FitnessMin\"):\n",
        "            del creator.FitnessMin\n",
        "        if hasattr(creator, \"Individual\"):\n",
        "            del creator.Individual\n",
        "\n",
        "        # Create new DEAP classes\n",
        "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "        creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Computes the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            The input value(s).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The sigmoid of the input.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        beta : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The negative log-likelihood value.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.y * np.log(p) + (1 - self.y) * np.log(1 - p))\n",
        "\n",
        "    def compute_confidence_intervals(self, beta):\n",
        "        \"\"\"\n",
        "        Computes confidence intervals for the estimated coefficients.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        beta : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The lower and upper bounds of the confidence intervals.\n",
        "        \"\"\"\n",
        "        p = self.sigmoid(np.dot(self.X, beta))\n",
        "        W = np.diag(p * (1 - p))\n",
        "        hessian = np.dot(self.X.T, np.dot(W, self.X))\n",
        "\n",
        "        epsilon = 1e-5\n",
        "        hessian += np.eye(hessian.shape[0]) * epsilon\n",
        "\n",
        "        try:\n",
        "            hessian_inv = np.linalg.inv(hessian)\n",
        "            se = np.sqrt(np.diag(hessian_inv))\n",
        "        except np.linalg.LinAlgError:\n",
        "            se = np.full(self.X.shape[1], np.nan)\n",
        "\n",
        "        z_score = norm.ppf(0.975)\n",
        "        lower_bound = beta - z_score * se\n",
        "        upper_bound = beta + z_score * se\n",
        "        return np.vstack((lower_bound, upper_bound)).T\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using MLE optimized via Game Theory Optimization.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "\n",
        "        toolbox = base.Toolbox()\n",
        "        toolbox.register(\"attr_float\", np.random.uniform, -10, 10)\n",
        "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=n_features)\n",
        "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "        # Register the fitness function to use with parallel processing\n",
        "        toolbox.register(\"evaluate\", eval_fitness, X=self.X, y=self.y)\n",
        "\n",
        "        pop = toolbox.population(n=self.population_size)\n",
        "\n",
        "        pool = Pool()\n",
        "        toolbox.register(\"map\", pool.map)\n",
        "\n",
        "        # Initialize best fitness and individual\n",
        "        best_fitness_so_far = float('inf')\n",
        "        best_individual_so_far = None\n",
        "\n",
        "        # Create a progress bar with tqdm\n",
        "        progress_bar = tqdm(range(self.generations), desc=\"Generations\", unit=\"gen\")\n",
        "\n",
        "        for gen in progress_bar:\n",
        "            # Mutate and evaluate the population\n",
        "            for i in range(len(pop)):\n",
        "                if np.random.rand() < self.mutpb:\n",
        "                    pop[i] = toolbox.clone(pop[i])\n",
        "                    for j in range(len(pop[i])):\n",
        "                        pop[i][j] += np.random.uniform(-1, 1)\n",
        "\n",
        "            fitnesses = list(toolbox.map(toolbox.evaluate, pop))\n",
        "\n",
        "            for ind, fit in zip(pop, fitnesses):\n",
        "                ind.fitness.values = (fit,)\n",
        "                # Update best individual so far if current one is better\n",
        "                if fit < best_fitness_so_far:\n",
        "                    best_fitness_so_far = fit\n",
        "                    best_individual_so_far = toolbox.clone(ind)\n",
        "\n",
        "            payoff = self.payoff_matrix(pop)\n",
        "            best_individual = self.nash_equilibrium_strategy(pop, payoff)\n",
        "            self.beta = np.array(best_individual)\n",
        "\n",
        "            # Apply elitism: replace the worst individual with the best one from the previous generation\n",
        "            if self.elitism and best_individual_so_far is not None:\n",
        "                worst_idx = np.argmax([ind.fitness.values[0] for ind in pop])\n",
        "                pop[worst_idx] = toolbox.clone(best_individual_so_far)\n",
        "\n",
        "            # Update progress bar and print a progress report\n",
        "            progress_bar.set_postfix(best_fitness=best_individual.fitness.values[0])\n",
        "\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        # Set the final best individual as the model parameters\n",
        "        self.beta = np.array(best_individual_so_far)\n",
        "        self.confidence_intervals = self.compute_confidence_intervals(self.beta)\n",
        "        return self.beta\n",
        "\n",
        "    def payoff_matrix(self, population):\n",
        "        \"\"\"\n",
        "        Creates the payoff matrix for the population.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        population : list\n",
        "            The population of individuals (solutions).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The payoff matrix where each entry represents the outcome of one individual compared to another.\n",
        "        \"\"\"\n",
        "        n = len(population)\n",
        "        payoff = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                fi = eval_fitness(population[i], self.X, self.y)\n",
        "                fj = eval_fitness(population[j], self.X, self.y)\n",
        "                payoff[i, j] = 1 if fi < fj else 0\n",
        "                payoff[j, i] = 1 if fj < fi else 0\n",
        "        return payoff\n",
        "\n",
        "    def nash_equilibrium_strategy(self, population, payoff):\n",
        "        \"\"\"\n",
        "        Identify the best strategy using Nash equilibrium concept.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        population : list\n",
        "            The population of individuals (solutions).\n",
        "        payoff : np.ndarray\n",
        "            The payoff matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The individual (solution) that represents the Nash equilibrium.\n",
        "        \"\"\"\n",
        "        n = len(population)\n",
        "        strategy_stability = np.zeros(n)\n",
        "\n",
        "        # Check if each individual is at Nash equilibrium\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if i != j:\n",
        "                    if payoff[i, j] > payoff[j, i]:  # i performs better than j\n",
        "                        strategy_stability[i] += 1\n",
        "                    elif payoff[i, j] < payoff[j, i]:  # j performs better than i\n",
        "                        strategy_stability[j] += 1\n",
        "\n",
        "        # The individual with the highest stability (closest to Nash equilibrium) is selected\n",
        "        best_idx = np.argmax(strategy_stability)\n",
        "        return population[best_idx]\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for new data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            The new data for which to predict probabilities.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X_new.\n",
        "        \"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"\n",
        "        Predicts binary class labels for new data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_new : np.ndarray\n",
        "            The new data for which to predict class labels.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on test data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "C6TgUsvDHI39"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Game Theory Optimization MLE\n",
        "gto_mle_model = LogisticRegressionGameTheoryOptimization(X_train, y_train)\n",
        "gto_mle_model.fit()\n",
        "accuracy_gto_mle, log_loss_gto_mle = gto_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"\\n\\nAccuracy (GTO-MLE): {accuracy_gto_mle:.4f}\")\n",
        "print(f\"Log Loss (GTO-MLE): {log_loss_gto_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOgPAq_yNb_f",
        "outputId": "39d56cfd-15f2-44ac-dd09-31c43fed9ea3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generations: 100%|██████████| 100/100 [08:30<00:00,  5.10s/gen, best_fitness=328]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy (GTO-MLE): 0.8233\n",
            "Log Loss (GTO-MLE): 0.6041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weighted MLE"
      ],
      "metadata": {
        "id": "q-TbBT3qHQos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionWMLE:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Weighted Maximum Likelihood Estimation (WMLE).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    weights : np.ndarray\n",
        "        The weights assigned to each observation.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using WMLE.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, weights=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weights = weights if weights is not None else np.ones_like(y)\n",
        "        self.beta = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def weighted_log_likelihood(self, beta):\n",
        "        \"\"\"\n",
        "        Computes the weighted negative log-likelihood function.\n",
        "        This is the objective function to be minimized.\n",
        "        \"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        p = self.sigmoid(z)\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "        return -np.sum(self.weights * (self.y * np.log(p) + (1 - self.y) * np.log(1 - p)))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using WMLE.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        beta : np.ndarray\n",
        "            The estimated coefficients.\n",
        "        \"\"\"\n",
        "        n_features = self.X.shape[1]\n",
        "        initial_beta = np.zeros(n_features)\n",
        "\n",
        "        # Minimize the weighted log-likelihood\n",
        "        result = minimize(self.weighted_log_likelihood, initial_beta, method='BFGS')\n",
        "        self.beta = result.x\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "z_AIxIqaHWUT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific weights\n",
        "weights_train = np.array([0.5] * len(y_train))\n",
        "\n",
        "# Weighted MLE\n",
        "wmle_model = LogisticRegressionWMLE(X_train, y_train, weights_train)\n",
        "wmle_model.fit()\n",
        "accuracy_wmle, log_loss_wmle = wmle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (WMLE): {accuracy_wmle:.4f}\")\n",
        "print(f\"Log Loss (WMLE): {log_loss_wmle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT3QustXHXI3",
        "outputId": "2a9b3948-05d9-4df9-b1ea-8ee41766ad4f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (WMLE): 0.8400\n",
            "Log Loss (WMLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted MLE with default equal weights\n",
        "wmle_model = LogisticRegressionWMLE(X_train, y_train)\n",
        "wmle_model.fit()\n",
        "accuracy_wmle, log_loss_wmle = wmle_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (WMLE): {accuracy_wmle:.4f}\")\n",
        "print(f\"Log Loss (WMLE): {log_loss_wmle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAo233N0R7vu",
        "outputId": "96690c7d-fc08-42f1-a2b2-fecf429be0f1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (WMLE): 0.8400\n",
            "Log Loss (WMLE): 0.3557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Entropy Loss"
      ],
      "metadata": {
        "id": "ZqKIiAkgHtUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionCrossEntropy:\n",
        "    \"\"\"\n",
        "    Logistic Regression with Cross-Entropy Loss.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using gradient descent.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and cross-entropy loss.\n",
        "    cross_entropy_loss(y_true, y_pred_proba):\n",
        "        Computes the cross-entropy loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, learning_rate=0.01, iterations=1000):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.beta = np.zeros(X.shape[1])\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using gradient descent.\n",
        "        \"\"\"\n",
        "        for _ in range(self.iterations):\n",
        "            z = np.dot(self.X, self.beta)\n",
        "            h = self.sigmoid(z)\n",
        "            gradient = np.dot(self.X.T, (h - self.y)) / self.y.size\n",
        "            self.beta -= self.learning_rate * gradient\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred_proba):\n",
        "        \"\"\"Computes the cross-entropy loss.\"\"\"\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
        "        loss = -np.mean(y_true * np.log(y_pred_proba) + (1 - y_true) * np.log(1 - y_pred_proba))\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and cross-entropy loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        cross_entropy = self.cross_entropy_loss(y_test, y_pred_proba)\n",
        "        return accuracy, cross_entropy"
      ],
      "metadata": {
        "id": "1zY2jdl_HzFv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Entropy Loss\n",
        "cross_entropy_model = LogisticRegressionCrossEntropy(X_train, y_train)\n",
        "cross_entropy_model.fit()\n",
        "accuracy, cross_entropy_loss = cross_entropy_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Cross-Entropy): {accuracy:.4f}\")\n",
        "print(f\"Cross-Entropy Loss: {cross_entropy_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BVwdN4KHzdi",
        "outputId": "7773ac00-db5f-4477-a93b-7b8a3c51f711"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Cross-Entropy): 0.8467\n",
            "Cross-Entropy Loss: 0.3661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KL Regularization"
      ],
      "metadata": {
        "id": "ALLht6l6H4sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionKLRegularization:\n",
        "    \"\"\"\n",
        "    Logistic Regression with KL Divergence Regularization.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    lambda_kl : float\n",
        "        The regularization strength for the KL divergence term.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit():\n",
        "        Fits the logistic regression model using gradient descent with KL divergence regularization.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and the combined loss.\n",
        "    combined_loss(y_true, y_pred_proba):\n",
        "        Computes the combined loss of cross-entropy and KL divergence.\n",
        "    kl_divergence(y_true, y_pred_proba):\n",
        "        Computes the KL divergence between true and predicted probability distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, learning_rate=0.01, iterations=1000, lambda_kl=0.1):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.lambda_kl = lambda_kl\n",
        "        self.beta = np.zeros(X.shape[1])\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using gradient descent with KL divergence regularization.\n",
        "        \"\"\"\n",
        "        for _ in range(self.iterations):\n",
        "            z = np.dot(self.X, self.beta)\n",
        "            h = self.sigmoid(z)\n",
        "            gradient = np.dot(self.X.T, (h - self.y)) / self.y.size\n",
        "            kl_grad = self.kl_divergence_gradient(self.X, self.y, h)\n",
        "            self.beta -= self.learning_rate * (gradient + self.lambda_kl * kl_grad)\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        return self.sigmoid(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary class labels for new data.\"\"\"\n",
        "        return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "\n",
        "    def kl_divergence(self, y_true, y_pred_proba):\n",
        "        \"\"\"Computes the KL divergence between the true and predicted probability distributions.\"\"\"\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
        "        q = y_true * y_pred_proba + (1 - y_true) * (1 - y_pred_proba)\n",
        "        p = y_true * y_true + (1 - y_true) * (1 - y_true)\n",
        "        kl_div = np.sum(p * np.log(p / q))\n",
        "        return kl_div\n",
        "\n",
        "    def kl_divergence_gradient(self, X, y_true, y_pred_proba):\n",
        "        \"\"\"Computes the gradient of the KL divergence term with respect to beta.\"\"\"\n",
        "        epsilon = 1e-8  # To prevent division by zero\n",
        "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
        "        kl_grad = np.dot(X.T, y_true - y_pred_proba) / y_true.size\n",
        "        return kl_grad\n",
        "\n",
        "    def combined_loss(self, y_true, y_pred_proba):\n",
        "        \"\"\"Computes the combined loss of cross-entropy and KL divergence.\"\"\"\n",
        "        # Cross-entropy loss\n",
        "        epsilon = 1e-8  # To prevent log(0)\n",
        "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
        "        cross_entropy = -np.mean(y_true * np.log(y_pred_proba) + (1 - y_true) * np.log(1 - y_pred_proba))\n",
        "\n",
        "        # KL divergence\n",
        "        kl_div = self.kl_divergence(y_true, y_pred_proba)\n",
        "\n",
        "        # Combined loss\n",
        "        return cross_entropy + self.lambda_kl * kl_div\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and the combined loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        combined_loss_val = self.combined_loss(y_test, y_pred_proba)\n",
        "        return accuracy, combined_loss_val"
      ],
      "metadata": {
        "id": "JhmJJQLfIAUO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KL Divergence Regularization\n",
        "kl_reg_model = LogisticRegressionKLRegularization(X_train, y_train)\n",
        "kl_reg_model.fit()\n",
        "accuracy_kl_reg, combined_loss_kl_reg = kl_reg_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (KL Regularization): {accuracy_kl_reg:.4f}\")\n",
        "print(f\"Combined Loss (KL Regularization): {combined_loss_kl_reg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXchxUuIIAra",
        "outputId": "54fd62d3-2788-4f45-e8e7-6a6b467cd1fd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (KL Regularization): 0.8467\n",
            "Combined Loss (KL Regularization): 11.4445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual MLE"
      ],
      "metadata": {
        "id": "6j_hY5BiHd_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionMLE:\n",
        "    \"\"\"\n",
        "    Logistic Regression model using Maximum Likelihood Estimation (MLE).\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    learning_rate : float\n",
        "        The step size for gradient descent.\n",
        "    n_iter : int\n",
        "        The number of iterations for gradient descent.\n",
        "    coef_ : np.ndarray\n",
        "        Coefficients for the features after fitting the model.\n",
        "    intercept_ : float\n",
        "        Intercept term after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the logistic regression model to the training data.\n",
        "    predict_proba(X):\n",
        "        Predicts probabilities for the input features.\n",
        "    predict(X):\n",
        "        Predicts binary class labels for the input features.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
        "        \"\"\"\n",
        "        Initializes the LogisticRegressionMLE model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        learning_rate : float, optional (default=0.01)\n",
        "            The step size for gradient descent.\n",
        "        n_iter : int, optional (default=1000)\n",
        "            The number of iterations for gradient descent.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Computes the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            The linear combination of input features and coefficients.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The sigmoid of the input z.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def log_likelihood(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Computes the log-likelihood of the model given the data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The log-likelihood value.\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        return np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model to the training data using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.coef_ = np.zeros(n_features)\n",
        "        self.intercept_ = 0\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            linear_model = X @ self.coef_ + self.intercept_\n",
        "            y_pred = self.sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients\n",
        "            gradient_w = (1 / n_samples) * (X.T @ (y_pred - y))\n",
        "            gradient_b = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.coef_ -= self.learning_rate * gradient_w\n",
        "            self.intercept_ -= self.learning_rate * gradient_b\n",
        "\n",
        "            # Optionally print log likelihood every 100 iterations\n",
        "            if _ % 100 == 0:\n",
        "                log_likelihood_value = self.log_likelihood(X, y, self.coef_, self.intercept_)\n",
        "                print(f\"Iteration {_}, Log-Likelihood: {log_likelihood_value}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X.\n",
        "        \"\"\"\n",
        "        linear_model = X @ self.coef_ + self.intercept_\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts binary class labels for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "sP92xRDdHgIp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual MLE\n",
        "manual_mle_model = LogisticRegressionMLE(learning_rate=0.01, n_iter=1000)\n",
        "manual_mle_model.fit(X_train, y_train)\n",
        "accuracy_manual_mle, log_loss_manual_mle = manual_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy: {accuracy_manual_mle:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_manual_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OntMr3maJDFw",
        "outputId": "19f27189-fb35-4ad9-89b5-d43a0b104abb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Log-Likelihood: -483.10714197323347\n",
            "Iteration 100, Log-Likelihood: -363.8669969722753\n",
            "Iteration 200, Log-Likelihood: -317.9575059297764\n",
            "Iteration 300, Log-Likelihood: -293.50890576235355\n",
            "Iteration 400, Log-Likelihood: -278.21788669371415\n",
            "Iteration 500, Log-Likelihood: -267.75167786265564\n",
            "Iteration 600, Log-Likelihood: -260.1642748812542\n",
            "Iteration 700, Log-Likelihood: -254.43974933001158\n",
            "Iteration 800, Log-Likelihood: -249.99151698616618\n",
            "Iteration 900, Log-Likelihood: -246.45562057855784\n",
            "\n",
            "Accuracy: 0.8500\n",
            "Log Loss: 0.3690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Newton-Raphson"
      ],
      "metadata": {
        "id": "c12UWbnwHgzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cholesky_decomposition(A, epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    Perform Cholesky decomposition on a positive-definite matrix A.\n",
        "    Returns the lower triangular matrix L such that A = L * L.T.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    A : np.ndarray\n",
        "        The positive-definite matrix to decompose.\n",
        "    epsilon : float, optional\n",
        "        A small value added to the diagonal elements for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    L : np.ndarray\n",
        "        The lower triangular matrix.\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    L = np.zeros_like(A)\n",
        "\n",
        "    # Add small value to diagonal for numerical stability\n",
        "    A += np.eye(n) * epsilon\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1):\n",
        "            sum_k = np.sum(L[i, :j] ** 2)\n",
        "            if i == j:\n",
        "                L[i, j] = np.sqrt(A[i, i] - sum_k)\n",
        "            else:\n",
        "                L[i, j] = (A[i, j] - np.sum(L[i, :j] * L[j, :j])) / L[j, j]\n",
        "\n",
        "            # Handle cases where the result may be negative or zero due to numerical issues\n",
        "            if np.isnan(L[i, j]) or np.isinf(L[i, j]):\n",
        "                raise ValueError(f\"Cholesky decomposition failed at index ({i}, {j}).\")\n",
        "\n",
        "    return L\n",
        "\n",
        "def cholesky_solve(L, b):\n",
        "    \"\"\"\n",
        "    Solve the system of linear equations A * x = b using Cholesky decomposition.\n",
        "    Given L (from cholesky_decomposition), solve L * L.T * x = b.\n",
        "    \"\"\"\n",
        "    # Forward substitution to solve L * y = b\n",
        "    y = np.zeros_like(b)\n",
        "    for i in range(len(y)):\n",
        "        y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]\n",
        "\n",
        "    # Backward substitution to solve L.T * x = y\n",
        "    x = np.zeros_like(y)\n",
        "    for i in range(len(x) - 1, -1, -1):\n",
        "        x[i] = (y[i] - np.dot(L[i + 1:, i], x[i + 1:])) / L[i, i]\n",
        "\n",
        "    return x\n",
        "\n",
        "def pseudo_inverse(A, tol=1e-10):\n",
        "    \"\"\"\n",
        "    Compute the pseudo-inverse of a matrix A using SVD.\n",
        "    \"\"\"\n",
        "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "    s_inv = np.array([1/x if x > tol else 0 for x in s])\n",
        "    return Vt.T @ np.diag(s_inv) @ U.T"
      ],
      "metadata": {
        "id": "-MpgmuEgHi-5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNewtonLineSearch:\n",
        "    \"\"\"\n",
        "    Logistic Regression model using Newton's method with line search.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for Newton's method.\n",
        "    tol : float\n",
        "        Tolerance for convergence.\n",
        "    solver : str\n",
        "        Method to solve the linear system. Options are 'cholesky' or 'svd'.\n",
        "    coef_ : np.ndarray\n",
        "        Coefficients for the features after fitting the model.\n",
        "    intercept_ : float\n",
        "        Intercept term after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the logistic regression model to the training data.\n",
        "    predict_proba(X):\n",
        "        Predicts probabilities for the input features.\n",
        "    predict(X):\n",
        "        Predicts binary class labels for the input features.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_iter=10, tol=1e-5, solver='cholesky'):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_iter : int, optional (default=10)\n",
        "            Maximum number of iterations for Newton's method.\n",
        "        tol : float, optional (default=1e-5)\n",
        "            Tolerance for convergence.\n",
        "        solver : str, optional (default='cholesky')\n",
        "            Method to solve the linear system. Options are 'cholesky' or 'svd'.\n",
        "        \"\"\"\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.solver = solver\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Computes the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            The linear combination of input features and coefficients.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The sigmoid of the input z.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def negative_log_likelihood(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Computes the negative log-likelihood of the model given the data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The negative log-likelihood value.\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        return -np.sum(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
        "\n",
        "    def gradient(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The gradient vector (including gradient w.r.t. coefficients and intercept).\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        grad_w = X.T @ (y_pred - y)\n",
        "        grad_b = np.sum(y_pred - y)\n",
        "        return np.append(grad_w, grad_b)\n",
        "\n",
        "    def hessian(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Computes the Hessian matrix of the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The Hessian matrix.\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        diag_gradient = y_pred * (1 - y_pred)\n",
        "        H_w = (X.T * diag_gradient) @ X\n",
        "        H_b = np.sum(diag_gradient)\n",
        "        H_wb = np.sum(X.T * diag_gradient, axis=1)\n",
        "        H = np.vstack((np.column_stack((H_w, H_wb)), np.append(H_wb, H_b)))\n",
        "        return H\n",
        "\n",
        "    def solve_hessian_system(self, H, grad):\n",
        "        \"\"\"\n",
        "        Solves the linear system H * delta = grad using the specified solver.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        H : np.ndarray\n",
        "            The Hessian matrix.\n",
        "        grad : np.ndarray\n",
        "            The gradient vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The Newton direction vector.\n",
        "        \"\"\"\n",
        "        if self.solver == 'cholesky':\n",
        "            try:\n",
        "                L = cholesky_decomposition(H)\n",
        "                direction = cholesky_solve(L, grad)\n",
        "            except np.linalg.LinAlgError:\n",
        "                print(\"Hessian is not positive-definite, switching to SVD.\")\n",
        "                self.solver = 'svd'\n",
        "                direction = self.solve_hessian_system(H, grad)\n",
        "            except ValueError as e:\n",
        "                print(str(e) + \" Switching to SVD.\")\n",
        "                self.solver = 'svd'\n",
        "                direction = self.solve_hessian_system(H, grad)\n",
        "        elif self.solver == 'svd':\n",
        "            H_inv = pseudo_inverse(H)\n",
        "            direction = H_inv @ grad\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported solver. Choose 'cholesky' or 'svd'.\")\n",
        "        return direction\n",
        "\n",
        "    def line_search(self, X, y, params, grad, direction):\n",
        "        \"\"\"\n",
        "        Performs line search to find the optimal step size in the direction of the Newton step.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (including coefficients and intercept).\n",
        "        grad : np.ndarray\n",
        "            The gradient vector.\n",
        "        direction : np.ndarray\n",
        "            The Newton direction vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The optimal step size.\n",
        "        \"\"\"\n",
        "        alpha = 1.0\n",
        "        rho = 0.8\n",
        "        c = 1e-4\n",
        "        current_nll = self.negative_log_likelihood(X, y, params[:-1], params[-1])\n",
        "\n",
        "        while True:\n",
        "            new_params = params - alpha * direction\n",
        "            new_nll = self.negative_log_likelihood(X, y, new_params[:-1], new_params[-1])\n",
        "\n",
        "            if new_nll <= current_nll - c * alpha * np.dot(grad, direction):\n",
        "                break\n",
        "            alpha *= rho\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model to the training data using Newton's method with line search.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters (coefficients + intercept)\n",
        "        params = np.zeros(n_features + 1)\n",
        "        coef, intercept = params[:-1], params[-1]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            grad = self.gradient(X, y, coef, intercept)\n",
        "            H = self.hessian(X, y, coef, intercept)\n",
        "\n",
        "            # Compute the Newton direction using the chosen solver\n",
        "            direction = self.solve_hessian_system(H, grad)\n",
        "\n",
        "            # Perform line search to find the optimal step size\n",
        "            alpha = self.line_search(X, y, params, grad, direction)\n",
        "\n",
        "            # Update the parameters\n",
        "            params -= alpha * direction\n",
        "            coef, intercept = params[:-1], params[-1]\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(direction) < self.tol:\n",
        "                print(f\"Converged after {i + 1} iterations.\")\n",
        "                break\n",
        "\n",
        "            log_likelihood_value = self.negative_log_likelihood(X, y, coef, intercept)\n",
        "            print(f\"Iteration {i + 1}, Log-Likelihood: {-log_likelihood_value}\")\n",
        "\n",
        "        self.coef_ = coef\n",
        "        self.intercept_ = intercept\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X.\n",
        "        \"\"\"\n",
        "        linear_model = X @ self.coef_ + self.intercept_\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts binary class labels for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "61NVEPiBJWWB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Newton-Raphson with Line Search\n",
        "nr_mle_model = LogisticRegressionNewtonLineSearch(max_iter=10, tol=1e-5, solver='cholesky')\n",
        "nr_mle_model.fit(X_train, y_train)\n",
        "accuracy_nr_mle, log_loss_nr_mle = nr_mle_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy: {accuracy_nr_mle:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_nr_mle:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LhiHPSFJWTb",
        "outputId": "c8911ce8-dc0b-4f30-9234-980ac4e6812d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Log-Likelihood: -269.9122134684517\n",
            "Iteration 2, Log-Likelihood: -232.67119041398956\n",
            "Iteration 3, Log-Likelihood: -226.31846419447578\n",
            "Iteration 4, Log-Likelihood: -225.9960537181275\n",
            "Iteration 5, Log-Likelihood: -225.99485053589905\n",
            "Iteration 6, Log-Likelihood: -225.9948505157966\n",
            "Iteration 7, Log-Likelihood: -225.9948505157966\n",
            "Iteration 8, Log-Likelihood: -225.99485051579654\n",
            "Iteration 9, Log-Likelihood: -225.99485051579654\n",
            "Iteration 10, Log-Likelihood: -225.99485051579654\n",
            "\n",
            "Accuracy: 0.8400\n",
            "Log Loss: 0.3576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual BFGS"
      ],
      "metadata": {
        "id": "envSnH7JHjeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionBFGS:\n",
        "    \"\"\"\n",
        "    Logistic Regression model using BFGS optimization.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    coef_ : np.ndarray\n",
        "        Coefficients for the features after fitting the model.\n",
        "    intercept_ : float\n",
        "        Intercept term after fitting the model.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for BFGS optimization.\n",
        "    tol : float\n",
        "        Tolerance for convergence.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the logistic regression model to the training data.\n",
        "    predict_proba(X):\n",
        "        Predicts probabilities for the input features.\n",
        "    predict(X):\n",
        "        Predicts binary class labels for the input features.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_iter=100, tol=1e-5):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model with BFGS optimization.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_iter : int, optional (default=100)\n",
        "            Maximum number of iterations for BFGS optimization.\n",
        "        tol : float, optional (default=1e-5)\n",
        "            Tolerance for convergence.\n",
        "        \"\"\"\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid function with clipping to avoid overflow.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            Linear combination of input features and coefficients.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            Sigmoid of the input z.\n",
        "        \"\"\"\n",
        "        # Clip z to avoid overflow in exp\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def negative_log_likelihood(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Compute the negative log-likelihood of the model given the data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The negative log-likelihood value.\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        nll = -np.sum(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
        "        return nll\n",
        "\n",
        "    def gradient(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The gradient vector (including gradient w.r.t. coefficients and intercept).\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        grad_w = X.T @ (y_pred - y)\n",
        "        grad_b = np.sum(y_pred - y)\n",
        "        return np.append(grad_w, grad_b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model to the training data using BFGS optimization.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters (coefficients + intercept)\n",
        "        params = np.zeros(n_features + 1)\n",
        "        coef, intercept = params[:-1], params[-1]\n",
        "\n",
        "        # Initialize the inverse Hessian approximation to identity\n",
        "        H_inv = np.eye(n_features + 1)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Calculate the gradient\n",
        "            grad = self.gradient(X, y, coef, intercept)\n",
        "\n",
        "            # Update step\n",
        "            direction = -H_inv @ grad\n",
        "            alpha = self.line_search(X, y, params, grad, direction)\n",
        "\n",
        "            # Update parameters\n",
        "            params_new = params + alpha * direction\n",
        "            coef_new, intercept_new = params_new[:-1], params_new[-1]\n",
        "\n",
        "            # Convergence check\n",
        "            if np.linalg.norm(params_new - params) < self.tol:\n",
        "                break\n",
        "\n",
        "            # Update the inverse Hessian approximation\n",
        "            s = params_new - params\n",
        "            y_vec = self.gradient(X, y, coef_new, intercept_new) - grad\n",
        "\n",
        "            rho = 1.0 / (y_vec.T @ s)\n",
        "            V = np.eye(n_features + 1) - rho * np.outer(s, y_vec)\n",
        "            H_inv = V @ H_inv @ V.T + rho * np.outer(s, s)\n",
        "\n",
        "            # Move to new parameters\n",
        "            params = params_new\n",
        "            coef, intercept = coef_new, intercept_new\n",
        "\n",
        "        self.coef_ = coef\n",
        "        self.intercept_ = intercept\n",
        "\n",
        "    def line_search(self, X, y, params, grad, direction):\n",
        "        \"\"\"\n",
        "        Perform backtracking line search to find the optimal step size.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (including coefficients and intercept).\n",
        "        grad : np.ndarray\n",
        "            The gradient vector.\n",
        "        direction : np.ndarray\n",
        "            The search direction vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The optimal step size.\n",
        "        \"\"\"\n",
        "        alpha = 1.0\n",
        "        rho = 0.8\n",
        "        c = 1e-4\n",
        "        while True:\n",
        "            new_params = params + alpha * direction\n",
        "            coef_new, intercept_new = new_params[:-1], new_params[-1]\n",
        "            if self.negative_log_likelihood(X, y, coef_new, intercept_new) <= \\\n",
        "                    self.negative_log_likelihood(X, y, params[:-1], params[-1]) + \\\n",
        "                    c * alpha * np.dot(grad, direction):\n",
        "                break\n",
        "            alpha *= rho\n",
        "        return alpha\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X.\n",
        "        \"\"\"\n",
        "        linear_model = X @ self.coef_ + self.intercept_\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict binary class labels for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "iufiSUeSHjaY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual BFGS\n",
        "bfgs_model = LogisticRegressionBFGS(max_iter=100, tol=1e-5)\n",
        "bfgs_model.fit(X_train, y_train)\n",
        "accuracy_bfgs, log_loss_bfgs = bfgs_model.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy_bfgs:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_bfgs:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLGU0vsyJa80",
        "outputId": "3a7ad0d4-dac2-41f3-85bd-69f2d8d0d06f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8400\n",
            "Log Loss: 0.3576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual L-BFGS"
      ],
      "metadata": {
        "id": "l4mHTLVGHjVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionLBFGS:\n",
        "    \"\"\"\n",
        "    Logistic Regression model using L-BFGS optimization algorithm.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    coef_ : np.ndarray\n",
        "        Coefficients for the features after fitting the model.\n",
        "    intercept_ : float\n",
        "        Intercept term after fitting the model.\n",
        "    m : int\n",
        "        Number of corrections to store for L-BFGS.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for the optimization algorithm.\n",
        "    tol : float\n",
        "        Tolerance for convergence.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the logistic regression model to the training data using L-BFGS.\n",
        "    predict_proba(X):\n",
        "        Predicts probabilities for the input features.\n",
        "    predict(X):\n",
        "        Predicts binary class labels for the input features.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "    line_search(X, y, params, grad, direction):\n",
        "        Performs a simple backtracking line search to find the optimal step size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, m=10, max_iter=100, tol=1e-5):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        m : int, optional (default=10)\n",
        "            Number of corrections to store for L-BFGS.\n",
        "        max_iter : int, optional (default=100)\n",
        "            Maximum number of iterations for the optimization algorithm.\n",
        "        tol : float, optional (default=1e-5)\n",
        "            Tolerance for convergence.\n",
        "        \"\"\"\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.m = m\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            The linear combination of input features and coefficients.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The sigmoid of the input z.\n",
        "        \"\"\"\n",
        "        z = np.clip(z, -500, 500)  # Clip to avoid overflow\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def negative_log_likelihood(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Compute the negative log-likelihood of the model given the data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The negative log-likelihood value.\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        nll = -np.sum(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
        "        return nll\n",
        "\n",
        "    def gradient(self, X, y, coef, intercept):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        coef : np.ndarray\n",
        "            The coefficients of the logistic regression model.\n",
        "        intercept : float\n",
        "            The intercept term of the logistic regression model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The gradient vector (including gradient w.r.t. coefficients and intercept).\n",
        "        \"\"\"\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "\n",
        "        # Ensure y_pred and y have the same shape for subtraction\n",
        "        y_pred = y_pred.reshape(-1)  # Flatten y_pred to ensure it's 1D\n",
        "        y = y.reshape(-1)  # Flatten y to ensure it's 1D\n",
        "\n",
        "        grad_w = X.T @ (y_pred - y)  # Gradient with respect to coefficients\n",
        "        grad_b = np.sum(y_pred - y)  # Gradient with respect to intercept\n",
        "        return np.append(grad_w, grad_b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model to the training data using L-BFGS.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters (coefficients + intercept)\n",
        "        params = np.zeros(n_features + 1)\n",
        "        coef, intercept = params[:-1], params[-1]\n",
        "\n",
        "        # Initialize lists for L-BFGS corrections\n",
        "        s_list = []\n",
        "        y_list = []\n",
        "        rho_list = []\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Calculate the gradient\n",
        "            grad = self.gradient(X, y, coef, intercept)\n",
        "\n",
        "            # L-BFGS two-loop recursion\n",
        "            q = grad\n",
        "            alpha = []\n",
        "            for s, y_corr, rho in reversed(list(zip(s_list, y_list, rho_list))):\n",
        "                a = rho * np.dot(s, q)\n",
        "                alpha.append(a)\n",
        "                q -= a * y_corr\n",
        "            if len(s_list) > 0:\n",
        "                gamma = np.dot(s_list[-1], y_list[-1]) / np.dot(y_list[-1], y_list[-1])\n",
        "            else:\n",
        "                gamma = 1.0\n",
        "            r = gamma * q\n",
        "            for s, y_corr, rho, a in zip(s_list, y_list, rho_list, reversed(alpha)):\n",
        "                b = rho * np.dot(y_corr, r)\n",
        "                r += s * (a - b)\n",
        "\n",
        "            direction = -r\n",
        "\n",
        "            # Line search\n",
        "            alpha_step = self.line_search(X, y, params, grad, direction)\n",
        "\n",
        "            # Update parameters\n",
        "            params_new = params + alpha_step * direction\n",
        "            coef_new, intercept_new = params_new[:-1], params_new[-1]\n",
        "\n",
        "            # Convergence check\n",
        "            if np.linalg.norm(params_new - params) < self.tol:\n",
        "                print(f\"Converged after {i + 1} iterations.\")\n",
        "                break\n",
        "\n",
        "            # Update L-BFGS correction history\n",
        "            s = params_new - params\n",
        "            y_corr = self.gradient(X, y, coef_new, intercept_new) - grad\n",
        "            rho = 1.0 / np.dot(y_corr, s)\n",
        "            if len(s_list) == self.m:\n",
        "                s_list.pop(0)\n",
        "                y_list.pop(0)\n",
        "                rho_list.pop(0)\n",
        "            s_list.append(s)\n",
        "            y_list.append(y_corr)\n",
        "            rho_list.append(rho)\n",
        "\n",
        "            # Move to new parameters\n",
        "            params = params_new\n",
        "            coef, intercept = coef_new, intercept_new\n",
        "\n",
        "        self.coef_ = coef\n",
        "        self.intercept_ = intercept\n",
        "\n",
        "    def line_search(self, X, y, params, grad, direction):\n",
        "        \"\"\"\n",
        "        Perform a simple backtracking line search to find the optimal step size.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (including coefficients and intercept).\n",
        "        grad : np.ndarray\n",
        "            The gradient vector.\n",
        "        direction : np.ndarray\n",
        "            The L-BFGS direction vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The optimal step size.\n",
        "        \"\"\"\n",
        "        alpha = 1.0\n",
        "        rho = 0.8\n",
        "        c = 1e-4\n",
        "        while True:\n",
        "            new_params = params + alpha * direction\n",
        "            coef_new, intercept_new = new_params[:-1], new_params[-1]\n",
        "            if self.negative_log_likelihood(X, y, coef_new, intercept_new) <= \\\n",
        "                    self.negative_log_likelihood(X, y, params[:-1], params[-1]) + \\\n",
        "                    c * alpha * np.dot(grad, direction):\n",
        "                break\n",
        "            alpha *= rho\n",
        "        return alpha\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X.\n",
        "        \"\"\"\n",
        "        linear_model = X @ self.coef_ + self.intercept_\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict binary class labels for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "mN1MxjUkHmVY"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual LBFGS\n",
        "lbfgs_model = LogisticRegressionLBFGS(m=10, max_iter=1000, tol=1e-8)\n",
        "lbfgs_model.fit(X_train, y_train)\n",
        "accuracy_lbfgs, log_loss_lbfgs = lbfgs_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy: {accuracy_lbfgs:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_lbfgs:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETgU9Q2UJLDY",
        "outputId": "4c7852c3-0815-4a22-9cc5-f520575f3d33"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 3 iterations.\n",
            "\n",
            "Accuracy: 0.8367\n",
            "Log Loss: 0.5972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Newton-CG"
      ],
      "metadata": {
        "id": "H42pI8gGHnJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNewtonCG:\n",
        "    \"\"\"\n",
        "    Logistic Regression model using Newton's method with Conjugate Gradient (CG) for Hessian inversion.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for Newton's method.\n",
        "    tol : float\n",
        "        Tolerance for convergence.\n",
        "    cg_tol : float\n",
        "        Tolerance for the conjugate gradient solver.\n",
        "    cg_max_iter : int or None\n",
        "        Maximum iterations for the conjugate gradient solver. If None, defaults to the number of features.\n",
        "    coef_ : np.ndarray\n",
        "        Coefficients for the features after fitting the model.\n",
        "    intercept_ : float\n",
        "        Intercept term after fitting the model.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y):\n",
        "        Fits the logistic regression model to the training data.\n",
        "    predict_proba(X):\n",
        "        Predicts probabilities for the input features.\n",
        "    predict(X):\n",
        "        Predicts binary class labels for the input features.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on the test data, returning accuracy and log loss.\n",
        "    conjugate_gradient(b, X, y, params):\n",
        "        Solves the linear system H * delta = b using the conjugate gradient method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_iter=10, tol=1e-5, cg_tol=1e-5, cg_max_iter=None):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_iter : int, optional (default=10)\n",
        "            Maximum number of iterations for Newton's method.\n",
        "        tol : float, optional (default=1e-5)\n",
        "            Tolerance for convergence.\n",
        "        cg_tol : float, optional (default=1e-5)\n",
        "            Tolerance for the conjugate gradient solver.\n",
        "        cg_max_iter : int or None, optional (default=None)\n",
        "            Maximum iterations for the conjugate gradient solver. If None, defaults to the number of features.\n",
        "        \"\"\"\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.cg_tol = cg_tol\n",
        "        self.cg_max_iter = cg_max_iter\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        z : np.ndarray\n",
        "            The linear combination of input features and coefficients.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The sigmoid of the input z.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def negative_log_likelihood(self, params, X, y):\n",
        "        \"\"\"\n",
        "        Compute the negative log-likelihood of the model given the data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (coefficients and intercept).\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float\n",
        "            The negative log-likelihood value.\n",
        "        \"\"\"\n",
        "        coef = params[:-1]\n",
        "        intercept = params[-1]\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        nll = -np.sum(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
        "        return nll\n",
        "\n",
        "    def gradient(self, params, X, y):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the negative log-likelihood function.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (coefficients and intercept).\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The gradient vector (including gradient w.r.t. coefficients and intercept).\n",
        "        \"\"\"\n",
        "        coef = params[:-1]\n",
        "        intercept = params[-1]\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        grad_w = X.T @ (y_pred - y)\n",
        "        grad_b = np.sum(y_pred - y)\n",
        "        return np.append(grad_w, grad_b)\n",
        "\n",
        "    def hessian_vector_product(self, params, X, y, vector):\n",
        "        \"\"\"\n",
        "        Compute the Hessian-vector product for the conjugate gradient method.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        params : np.ndarray\n",
        "            The current parameter vector (coefficients and intercept).\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        vector : np.ndarray\n",
        "            The vector to multiply with the Hessian.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The result of the Hessian-vector product.\n",
        "        \"\"\"\n",
        "        coef = params[:-1]\n",
        "        intercept = params[-1]\n",
        "        linear_model = X @ coef + intercept\n",
        "        y_pred = self.sigmoid(linear_model)\n",
        "        diag_gradient = y_pred * (1 - y_pred)\n",
        "        H_w = (X.T * diag_gradient) @ (X @ vector[:-1])\n",
        "        H_b = np.sum(diag_gradient) * vector[-1]\n",
        "        H_wb = np.sum(X.T * diag_gradient, axis=1) * vector[-1]\n",
        "        hvp = np.append(H_w + H_wb, H_b)\n",
        "        return hvp\n",
        "\n",
        "    def conjugate_gradient(self, b, X, y, params):\n",
        "        \"\"\"\n",
        "        Conjugate gradient method for solving H * x = b, where H is the Hessian.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        b : np.ndarray\n",
        "            Right-hand side vector of the linear system.\n",
        "        X : np.ndarray\n",
        "            Feature matrix.\n",
        "        y : np.ndarray\n",
        "            Target vector.\n",
        "        params : np.ndarray\n",
        "            Current parameter vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The solution vector x.\n",
        "        \"\"\"\n",
        "        n = len(b)\n",
        "        x = np.zeros_like(b)\n",
        "        r = b.copy()  # residual vector\n",
        "        p = r.copy()  # search direction\n",
        "        rsold = np.dot(r, r)\n",
        "\n",
        "        if self.cg_max_iter is None:\n",
        "            self.cg_max_iter = n\n",
        "\n",
        "        for i in range(self.cg_max_iter):\n",
        "            Ap = self.hessian_vector_product(params, X, y, p)\n",
        "            alpha = rsold / np.dot(p, Ap)\n",
        "            x += alpha * p\n",
        "            r -= alpha * Ap\n",
        "            rsnew = np.dot(r, r)\n",
        "\n",
        "            if np.sqrt(rsnew) < self.cg_tol:\n",
        "                break\n",
        "\n",
        "            p = r + (rsnew / rsold) * p\n",
        "            rsold = rsnew\n",
        "\n",
        "        return x\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the logistic regression model to the training data using Newton's method with CG.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "        y : np.ndarray\n",
        "            The target vector.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters (coefficients + intercept)\n",
        "        params = np.zeros(n_features + 1)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            grad = self.gradient(params, X, y)\n",
        "            b = -grad\n",
        "\n",
        "            # Use conjugate gradient to solve the linear system H * delta = -grad\n",
        "            delta = self.conjugate_gradient(b, X, y, params)\n",
        "\n",
        "            # Update parameters\n",
        "            params += delta\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(delta) < self.tol:\n",
        "                print(f\"Converged after {i+1} iterations.\")\n",
        "                break\n",
        "\n",
        "            nll = self.negative_log_likelihood(params, X, y)\n",
        "            print(f\"Iteration {i+1}, Negative Log-Likelihood: {nll}\")\n",
        "\n",
        "        self.coef_ = params[:-1]\n",
        "        self.intercept_ = params[-1]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted probabilities for each instance in X.\n",
        "        \"\"\"\n",
        "        linear_model = X @ self.coef_ + self.intercept_\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict binary class labels for the input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            The feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            The predicted binary class labels.\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test data, returning accuracy and log loss.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : np.ndarray\n",
        "            The test feature matrix.\n",
        "        y_test : np.ndarray\n",
        "            The test target vector.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and log loss on the test data.\n",
        "        \"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "DKcZN17FHohU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Newton-CG\n",
        "newton_cg_model = LogisticRegressionNewtonCG(max_iter=10, tol=1e-5, cg_tol=1e-5, cg_max_iter=100)\n",
        "newton_cg_model.fit(X_train, y_train)\n",
        "accuracy_newton_cg, log_loss_newton_cg = newton_cg_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nAccuracy: {accuracy_newton_cg:.4f}\")\n",
        "print(f\"Log Loss: {log_loss_newton_cg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_bCOdWwJgNy",
        "outputId": "d2db936f-771d-4f38-ebf4-8ad148c3d16f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Negative Log-Likelihood: 269.91221349544537\n",
            "Iteration 2, Negative Log-Likelihood: 232.68950528625325\n",
            "Iteration 3, Negative Log-Likelihood: 226.3300758334056\n",
            "Iteration 4, Negative Log-Likelihood: 225.99710174188453\n",
            "Iteration 5, Negative Log-Likelihood: 225.99485662227278\n",
            "Iteration 6, Negative Log-Likelihood: 225.99485051746527\n",
            "Converged after 7 iterations.\n",
            "\n",
            "Accuracy: 0.8400\n",
            "Log Loss: 0.3576\n"
          ]
        }
      ]
    }
  ]
}