{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "wZBlm6MVKVvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize"
      ],
      "metadata": {
        "id": "Jegt0tZqIlM3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "wwPADm7NKY6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target"
      ],
      "metadata": {
        "id": "-h7cnwjoJ83D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Data"
      ],
      "metadata": {
        "id": "7H2axHWuFj_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Jmoydb-2J80t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert target to float32\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Convert data to TensorFlow tensors\n",
        "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "3J07DahKJ8yG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Optimizer"
      ],
      "metadata": {
        "id": "ie93SlXMFnFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to calculate MSE\n",
        "def calculate_mse(model, X, y_true):\n",
        "    # Ensure input shapes are consistent\n",
        "    if len(X.shape) == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "    mse = tf.keras.losses.MeanSquaredError(\n",
        "        reduction=\"sum_over_batch_size\", name=\"mean_squared_error\"\n",
        "    )\n",
        "    return mse(y_true, y_pred).numpy()"
      ],
      "metadata": {
        "id": "bsoEpij1J8vw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to build a Keras model\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(X_train.shape[1],)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_DB_Q2xXJ8s7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = build_model()\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_adam = calculate_mse(model, X_test, y_test)\n",
        "print(f\"Adam Optimizer Test MSE: {mse_adam:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx6fPwgI5apR",
        "outputId": "0fda4370-8df6-487d-e27b-b565c5198521"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Adam Optimizer Test MSE: 2933.6528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SciPy Implementation 1"
      ],
      "metadata": {
        "id": "z0qiyC6cFqn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to flatten the model weights into a single vector\n",
        "def flatten_weights(weights):\n",
        "    return np.concatenate([tf.reshape(w, [-1]).numpy() for w in weights])\n",
        "\n",
        "# Function to reshape the flat vector back into the original weights\n",
        "def reshape_weights(flat_weights, weight_shapes):\n",
        "    weights = []\n",
        "    offset = 0\n",
        "    for shape in weight_shapes:\n",
        "        size = np.prod(shape)\n",
        "        weights.append(tf.reshape(flat_weights[offset:offset + size], shape))\n",
        "        offset += size\n",
        "    return weights\n",
        "\n",
        "# Function to compute the loss and gradients\n",
        "def scipy_loss_and_grads(flat_vars, model, X, y):\n",
        "    var_shapes = [v.shape for v in model.trainable_variables]\n",
        "    vars = tf.split(tf.convert_to_tensor(flat_vars, dtype=tf.float32), [tf.reduce_prod(v.shape) for v in model.trainable_variables])\n",
        "    vars = [tf.reshape(v, shape) for v, shape in zip(vars, var_shapes)]\n",
        "    model.set_weights(vars)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X)\n",
        "        loss = tf.reduce_mean(tf.square(y - y_pred))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    flat_grads = flatten_weights(grads)\n",
        "    return loss.numpy().astype(np.float64), flat_grads.astype(np.float64)\n",
        "\n",
        "# Function to perform SciPy optimization\n",
        "def fit_scipy_minimize(model, X, y, method='L-BFGS-B', max_iter=100):\n",
        "    initial_weights = model.get_weights()\n",
        "    weight_shapes = [w.shape for w in initial_weights]\n",
        "    initial_weights_flat = flatten_weights(initial_weights)\n",
        "\n",
        "    results = minimize(\n",
        "        fun=lambda w: scipy_loss_and_grads(w, model, X, y)[0],\n",
        "        x0=initial_weights_flat,\n",
        "        jac=lambda w: scipy_loss_and_grads(w, model, X, y)[1],\n",
        "        method=method,\n",
        "        options={'maxiter': max_iter}\n",
        "    )\n",
        "\n",
        "    optimized_weights = reshape_weights(results.x, weight_shapes)\n",
        "    model.set_weights(optimized_weights)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7J14M_CC8ZjW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model_scipy = build_model()\n",
        "\n",
        "# Train the model\n",
        "fitted_model_scipy = fit_scipy_minimize(\n",
        "    model_scipy,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    method='L-BFGS-B',\n",
        "    max_iter=500\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "mse_scipy = calculate_mse(fitted_model_scipy, X_test, y_test)\n",
        "print(f\"SciPy Test MSE: {mse_scipy:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TItedwtRBQrL",
        "outputId": "b014632e-2dd9-4b24-ebb0-59503a42f31d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "SciPy Test MSE: 5368.9180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SciPy Implementation 2"
      ],
      "metadata": {
        "id": "3YJO21HQFuTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Enable eager execution for tf.data functions\n",
        "tf.data.experimental.enable_debug_mode()\n",
        "\n",
        "# Function to get the model's weights and biases as a single vector\n",
        "def get_weights_vector(model):\n",
        "    weights = []\n",
        "    for layer in model.layers:\n",
        "        for param in layer.get_weights():\n",
        "            weights.append(param.flatten())\n",
        "    return np.concatenate(weights)\n",
        "\n",
        "# Function to set the model's weights and biases from a vector\n",
        "def set_weights_vector(model, vector):\n",
        "    index = 0\n",
        "    new_weights = []\n",
        "    for layer in model.layers:\n",
        "        layer_weights = []\n",
        "        for param in layer.get_weights():\n",
        "            shape = param.shape\n",
        "            size = np.prod(shape)\n",
        "            layer_weights.append(vector[index:index + size].reshape(shape))\n",
        "            index += size\n",
        "        new_weights.append(layer_weights)\n",
        "    for layer, layer_weights in zip(model.layers, new_weights):\n",
        "        layer.set_weights(layer_weights)\n",
        "\n",
        "# Function to compute the loss\n",
        "@tf.function(reduce_retracing=True)\n",
        "def loss_function(weights_vector, model, X, y_true):\n",
        "    set_weights_vector(model, weights_vector)\n",
        "    y_pred = model(X, training=False)\n",
        "    loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
        "    return loss.numpy()\n",
        "\n",
        "# Function to perform SciPy optimization\n",
        "def fit_scipy_minimize(model, X_train, y_train, method='L-BFGS-B', jac='2-point', maxiter=100, **kwargs):\n",
        "    # Convert inputs to tensors if they are not already\n",
        "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "\n",
        "    # Get the initial weights vector\n",
        "    initial_weights_vector = get_weights_vector(model)\n",
        "\n",
        "    # Perform the optimization using SciPy's minimize function\n",
        "    result = minimize(fun=loss_function,\n",
        "                      x0=initial_weights_vector,\n",
        "                      args=(model, X_train_tensor, y_train_tensor),\n",
        "                      method=method,\n",
        "                      jac=jac,\n",
        "                      options={'maxiter': maxiter},\n",
        "                      **kwargs)\n",
        "\n",
        "    # Set the optimized weights back to the model\n",
        "    set_weights_vector(model, result.x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "By4hn76uC0_B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model_scipy2 = build_model()\n",
        "\n",
        "# Train the model\n",
        "fitted_model_scipy2 = fit_scipy_minimize(\n",
        "    model_scipy2,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=500\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "mse_scipy2 = calculate_mse(fitted_model_scipy2, X_test, y_test)\n",
        "print(f\"SciPy Test MSE: {mse_scipy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7U6oaOQC088",
        "outputId": "60bf9768-0fe4-4e51-a479-16a482bd36f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "SciPy Test MSE: 4446.0269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SciPy Implementation 3"
      ],
      "metadata": {
        "id": "e810v2mnUDUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class SciPyModelOptimizer:\n",
        "    def __init__(self, model, enable_eager_execution=True):\n",
        "        self.model = model\n",
        "        if enable_eager_execution:\n",
        "            self.enable_eager_execution()\n",
        "\n",
        "    def enable_eager_execution(self):\n",
        "        # Enable eager execution\n",
        "        tf.config.run_functions_eagerly(True)\n",
        "        # Enable eager execution for tf.data functions\n",
        "        tf.data.experimental.enable_debug_mode()\n",
        "\n",
        "    def get_weights_vector(self):\n",
        "        weights = []\n",
        "        for layer in self.model.layers:\n",
        "            for param in layer.get_weights():\n",
        "                weights.append(param.flatten())\n",
        "        return np.concatenate(weights)\n",
        "\n",
        "    def set_weights_vector(self, vector):\n",
        "        index = 0\n",
        "        new_weights = []\n",
        "        for layer in self.model.layers:\n",
        "            layer_weights = []\n",
        "            for param in layer.get_weights():\n",
        "                shape = param.shape\n",
        "                size = np.prod(shape)\n",
        "                layer_weights.append(vector[index:index + size].reshape(shape))\n",
        "                index += size\n",
        "            new_weights.append(layer_weights)\n",
        "        for layer, layer_weights in zip(self.model.layers, new_weights):\n",
        "            layer.set_weights(layer_weights)\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def loss_function(self, weights_vector, X, y_true):\n",
        "        self.set_weights_vector(weights_vector)\n",
        "        y_pred = self.model(X, training=False)\n",
        "        loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
        "        return loss.numpy()\n",
        "\n",
        "    def fit(self, X_train, y_train, method='L-BFGS-B', jac='2-point', maxiter=100, **kwargs):\n",
        "        # Convert inputs to tensors if they are not already\n",
        "        X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "\n",
        "        # Get the initial weights vector\n",
        "        initial_weights_vector = self.get_weights_vector()\n",
        "\n",
        "        # Perform the optimization using SciPy's minimize function\n",
        "        result = minimize(fun=self.loss_function,\n",
        "                          x0=initial_weights_vector,\n",
        "                          args=(X_train_tensor, y_train_tensor),\n",
        "                          method=method,\n",
        "                          jac=jac,\n",
        "                          options={'maxiter': maxiter},\n",
        "                          **kwargs)\n",
        "\n",
        "        # Set the optimized weights back to the model\n",
        "        self.set_weights_vector(result.x)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        # Convert test inputs to tensors\n",
        "        X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "        y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "        # Predict and calculate the Mean Squared Error (MSE)\n",
        "        y_pred = self.model(X_test_tensor, training=False)\n",
        "        mse = tf.keras.losses.MeanSquaredError()(y_test_tensor, y_pred).numpy()\n",
        "        return mse"
      ],
      "metadata": {
        "id": "HW63Vt3oUDRv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model_scipy3 = build_model()\n",
        "\n",
        "# Instantiate the optimizer\n",
        "model_with_optimizer = SciPyModelOptimizer(model_scipy3)\n",
        "\n",
        "# Train the model\n",
        "fitted_model_scipy3 = model_with_optimizer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=500\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "mse_scipy3 = fitted_model_scipy3.evaluate(X_test, y_test)\n",
        "print(f\"SciPy Test MSE: {mse_scipy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBdsg5beUPgk",
        "outputId": "4dbc0e38-36ab-48cb-e019-7c2953093a38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SciPy Test MSE: 4378.7764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Comparison"
      ],
      "metadata": {
        "id": "Bm2CUC_4Fyef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of results\n",
        "print(\"Summary of MSE Results:\")\n",
        "print(f\"Adam Optimizer Test MSE: {mse_adam:.4f}\")\n",
        "print(f\"SciPy Test MSE (1): {mse_scipy:.4f}\")\n",
        "print(f\"SciPy Test MSE (2): {mse_scipy2:.4f}\")\n",
        "print(f\"SciPy Test MSE (3): {mse_scipy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZfcDBN7KMjw",
        "outputId": "4eda348c-f907-4897-e933-24c401ec84c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of MSE Results:\n",
            "Adam Optimizer Test MSE: 2933.6528\n",
            "SciPy Test MSE (1): 5368.9180\n",
            "SciPy Test MSE (2): 4446.0269\n",
            "SciPy Test MSE (3): 4378.7764\n"
          ]
        }
      ]
    }
  ]
}